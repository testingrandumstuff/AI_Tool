CIT 354 Virtualized Enterprise Sys

W1

Student Guide

Cloud computing
	0.	Cloud computing is the on-demand delivery of compute power, database, storage, applications, and other IT resources via the internet with pay-as-you-go pricing
	0.	These resources run on server  computers that are located in large data centers in different locations around the world. When  you use a cloud service provider like AWS, that service provider owns the computers that you are  using. These resources can be used together like building blocks to build solutions that help meet  business goals and satisfy technology requirements
	0.	Enables you to stop thinking of your infrastructure as hardware and instead think of and use it as software
Traditional computing model
	0.	Infrastructure thought as hardware (hardware solutions are physical which means they require space, staff, physical security, planning and capital expenditures
	0.	another prohibitive aspect of traditional computing 
	0.	is the long hardware procurement cycle that involves acquiring, provisioning, and maintaining on-premises infrastructure
	0.	With a hardware solution, you must ask if there is enough resource capacity or sufficient storage to meet your needs, and you provision capacity by guessing theoretical maximum peaks. If you don’t meet your projected maximum peak, then you pay for expensive resources that stay idle. If you exceed your projected maximum peak, then you don’t have sufficient capacity to meet your needs. And if your needs change, then you must spend the time, effort, and money required to implement a new solution
Cloud computing model
	0.	By contrast, cloud computing enables you to think of your infrastructure as software. Software solutions are flexible. You can select the cloud services that best match your needs, provision and terminate those resources on-demand, and pay for what you use. You can elastically scale resources up and down in an automated fashion. With the cloud computing model, you can treat resources as temporary and disposable. The flexibility that cloud computing offers enables businesses to implement new solutions quickly and with low upfront costs.
	0.	Compared to hardware solutions, software solutions can change much more quickly, easily, and cost-effectively.
	0.	Cloud computing helps developers and IT departments avoid undifferentiated work like procurement, maintenance, and capacity planning, thus enabling them to focus on what matters most.
	0.	As cloud computing has grown in popularity, several different service models and deployment strategies have emerged to help meet the specific needs of different users. Each type of cloud service model and deployment strategy provides you with a different level of control, flexibility, and management. Understanding the differences between these cloud service models and deployment strategies can help you decide what set of services is right for your needs.
Cloud service models
There are three main cloud service models. Each model represents a different part of the cloud computing stack and gives you a different level of control over your IT resources:
	•	 Infrastructure as a service (laaS): Services in this category are the basic building blocks for cloud IT and typically provide you with access to networking features, computers (virtual or on dedicated hardware), and data storage space. laaS provides you with the highest level of flexibility and management control over your IT resources. It is the most similar to existing IT resources that many IT departments and developers are familiar with today.
	•	Platform as a service (PaaS): Services in this category reduce the need for you to manage the underlying infrastructure (usually hardware and operating systems) and enable you to focus on the deployment and management of your applications.
	•	Software as a service (SaaS): Services in this category provide you with a completed product that the service provider runs and manages. In most cases, software as a service refers to end-user applications. With a SaaS offering, you do not have to think about how the service is maintained or how the underlying infrastructure is managed. You need to think only about how you plan to use that particular piece of software. A common example of a Saas application is web-based email, where you can send and receive email without managing feature additions to the email product or maintaining the servers and operating systems that the email program runs on.
Cloud computing deployment models
There are three main cloud computing deployment models, which represent the cloud environments that your applications can be deployed in:
	•	  Cloud: A cloud-based application is fully deployed in the cloud, and all parts of the application run in the cloud. Applications in the cloud have either been created in the cloud or have been migrated from an existing infrastructure to take advantage of the benefits of cloud computing (see https://aws.amazon.com/what-is-cloud-computing ). Cloud-based applications can be built on low-level infrastructure pieces or they can use higher-level services that provide abstraction from the management, architecting, and scaling requirements of core infrastructure.
	•	  Hybrid: A hybrid deployment is a way to connect infrastructure and applications between cloud-based resources and existing resources that are not located in the cloud. The most common method of hybrid deployment is between the cloud and existing on-premises infrastructure. This model enables an organization to extend and grow their infrastructure into the cloud while connecting cloud resources to internal systems.
	•	  On-premises: Deploying resources on-premises, using virtualization and resource management tools, is sometimes called private cloud. While on-premises deployment does not provide many of the benefits of cloud computing, it is sometimes sought for its ability to provide dedicated resources. In most cases, this deployment model is the same as legacy IT infrastructure, but it might also use application management and virtualization technologies to increase resource utilization.
Similarities between AWS and traditional IT
There are many similarities between AWS and the traditional, on-premises IT space:
	•	  AWS security groups, network access control lists (network ACLs), and AWS Identity and Access Management (IAM) are similar to firewalls, access control lists (ACLs), and administrators.
	•	  Elastic Load Balancing and Amazon Virtual Private Cloud (Amazon VPC) are similar to routers, network pipelines, and switches.
	•	  Amazon Machine Images (AMls) and Amazon Elastic Compute Cloud (Amazon EC2) instances are similar to on-premises servers.
	•	  Amazon Elastic Block Store (Amazon EBS), Amazon Elastic File System (Amazon EFS), AmazonSimple Storage Service (Amazon S3), and Amazon Relational Database Service (Amazon RDS) are similar to direct attached storage (DAS), storage area networks (SAN), network attached storage (NAS), and a relational database management service (RDBMS).
With AWS services and features, you can do almost everything that you would want to do with a traditional data center.

Section 2

Trade capital expense for variable expense
	0.	Advantage #1-Trade capital expense for variable expense: Capital expenses (capex) are funds that a company uses to acquire, upgrade, and maintain physical assets such as property, industrial buildings, or equipment. Do you remember the data center example in the traditional computing model where you needed to rack and stack the hardware, and then manage it all? You must pay for everything in the data center whether you use it or not.
	0.	By contrast, a variable expense is an expense that the person who bears the cost can easily alter or avoid. Instead of investing heavily in data centers and servers before you know how you will use them, you can pay only when you consume resources and pay only for the amount you consume. Thus, you save money on technology. It also enables you to adapt to new applications with as much space as you need in minutes, instead of weeks or days. Maintenance is reduced, so you can spend focus more on the core goals of your business.
Massive economies of scale
	0.	Advantage #2-Benefit from massive economies of scale: By using cloud computing, you can achieve a lower variable cost than you can get on your own. Because usage from hundreds of thousands of customers is aggregated in the cloud, providers such as AWS can achieve higher economies of scale, which translates into lower pay-as-you-go prices.
Stop guessing Capacity
	0.	Advantage #3-Stop guessing capacity: Eliminate guessing about your infrastructure capacity needs. When you make a capacity decision before you deploy an application, you often either have expensive idle resources or deal with limited capacity. With cloud computing, these problems go away. You can access as much or as little as you need, and scale up and down as required with only a few minutes' notice.
Increase speed and agility
	0.	Advantage #4-Increase speed and agility: In a cloud computing environment, new IT resources are only a click away, which means that you reduce the time it takes to make those resources available to your developers from weeks to just minutes. The result is a dramatic increase in agility for the organization because the cost and time that it takes to experiment and develop are significantly lower.
Stop spending money on running and maintaining data centers
	0.	Advantage #5-Stop spending money on running and maintaining data centers: Focus on projects that differentiate your business instead of focusing on the infrastructure. Cloud computing enables you to focus on your own customers instead of the heavy lifting of racking, stacking, and powering servers.
Go global in minutes
	0.	Advantage #6-Go global in minutes: You can deploy your application in multiple AWS Regions around the world with just a few clicks. As a result, you can provide a lower latency and better experience for your customers simply and at minimal cost.

Section 3

What are web services
	0.	In general, a web service is any piece of software that makes itself available over the internet or on private (intranet) networks. A web service uses a standardized format -such as Extensible Markup Language (XML) or JavaScript Object Notation (JSON)-for the request and the response of an application programming interface (API) interaction. It is not tied to any one operating system or programming language. It's self-describing via an interface definition file and it is discoverable.
What is AWS
	0.	Amazon Web Services (AWS) is a secure cloud platform that offers a broad set of global cloud-based products. Because these products are delivered over the internet, you have on-demand access to the compute, storage, network, database, and other IT resources that you might need for your projects-and the tools to manage them. You can immediately provision and launch AWS resources. The resources are ready for you to use in minutes.
	0.	AWS offers flexibility. Your AWS environment can be reconfigured and updated on demand, scaled up or down automatically to meet usage patterns and optimize spending, or shut down temporarily or permanently. The billing for AWS services becomes an operational expense instead of a capital expense.
	0.	AWS services are designed to work together to support virtually any type of application or workload. Think of these services like building blocks, which you can assemble quickly to build sophisticated, scalable solutions, and then adjust them as your needs change.
Categories of AWS services
	0.	AWS services fall under different categories, and each category contains one or more services.
	0.	You can select the services that you want from these different categories to build your solutions.
Choosing service (Compute)
	0.	Which service you choose to use will depend on your business goals and technology requirements. In the example you just looked at, the solution made use of Amazon EC2 as the compute service. However, that is only one of many compute services that AWS offers. Here are some other AWS compute offerings that you might choose to use for the following example use cases:
	0.	Amazon EC2 (https://aws.amazon.com/ec2/): You want complete control over your AWS computing resources.
	•	  AWS Lambda (https://aws.amazon.com/lambda/): You want to run your code and not manage or provision servers.
	•	  AWS Elastic Beanstalk (https://aws.amazon.com/elasticbeanstalk/): You want a service that deploys, manages, and scales your web applications for you.
	•	  Amazon Lightsail (https://aws.amazon.com/lightsail/): You need a lightweight cloud platform for a simple web application.
	•	  AWS Batch https://aws.amazon.com/batch/): You need to run hundreds of thousands of batch workloads.
	•	  AWS Outposts (https://aws.amazon.com/outposts/): You want to run AWS infrastructure in your on-premises data center.Amazon Elastic Container Service (Amazon ECS) (https://aws.amazon.com/ecs/)Amazon Elastic Kubernetes Service (Amazon EKS) (https://aws.amazon.com/eks/)
	•	  AWS Fargate (https:// aws.amazon.com/fargate/): You want to implement a containers or microservices architecture.
	•	  VMware Cloud on AWS (https://aws.amazon.com/vmware/): You have an on-premises server virtualization platform that you want to migrate to AWS.
Services covered in this course

Three ways to interact with AWS
	0.	You might wonder how to access the broad array of services that are offered by AWS. There are three ways to create and manage resources on the AWS Cloud:
	•	AWS Management Console: The console provides a rich graphical interface to a majority of the features offered by AWS. (Note: From time to time, new features might not have all of their capabilities included in the console when the feature initially launches.)
	•	AWS Command Line Interface (AWS CLI): The AWS CLI provides a suite of utilities that can be launched from a command script in Linux, macOS, or Microsoft Windows.
	•	Software development kits (SDKs): AWS provides packages that enable accessing AWS in a variety of popular programming languages. This makes it easy to use AWS in your existing applications and it also enables you to create applications that deploy and monitor complex systems entirely through code.
	0.	All three options are built on a common REST-like API that serves as the foundation of AWS.

Section 4

AWS cloud adoption framework (AWS CAF)
	0.	Each organization's cloud adoption journey is unique. However, in order for any organization to successfully migrate its IT portfolio to the cloud, three elements (that is, people, process, and technology) must be in alignment. Business and technology leaders in an organization must understand the organization's current state, target state, and the transition that is needed to achieve the target state so they can set goals and create processes for staff.
	0.	The AWS Cloud Adoption Framework (AWS CAF) provides guidance and best practices to help organizations identify gaps in skills and processes. It also helps organizations build a comprehensive approach to cloud computing -both across the organization and throughout the IT lifecycle -to accelerate successful cloud adoption.
	0.	At the highest level, the AWS CAF organizes guidance into six areas of focus, called perspectives.
	0.	Perspectives span people, processes, and technology. Each perspective consists of a set of capabilities, which covers distinct responsibilities that are owned or managed by functionally related stakeholders.
	0.	Capabilities within each perspective are used to identify which areas of an organization require attention. By identifying gaps, prescriptive work streams can be created that support a successful cloud journey.
Six core perspectives
	0.	In general, the Business, People, and Governance perspectives focus on business capabilities, while the Platform, Security, and Operations perspectives focus on technical capabilities.
Business perspective
	0.	Stakeholders from the Business perspective (for example, business managers, finance managers, budget owners, and strategy stakeholders) can use the AWS CAF to create a strong business case for cloud adoption and prioritize cloud adoption initiatives. Stakeholders should ensure that an organization's business strategies and goals align with its IT strategies and goals.
People perspective
	0.	Stakeholders from the People perspective (for example, human resources, staffing, and people managers) can use the AWS CAF to evaluate organizational structures and roles, new skill and process requirements, and identify gaps. Performing an analysis of needs and gaps can help prioritize training, staffing, and organizational changes to build an agile organization.
Governance perspective
	0.	Stakeholders from the Governance perspective (for example, the Chief Information Officer or CIO, program managers, enterprise architects, business analysts, and portfolio managers) can use the AWS CAF to focus on the skills and processes that are needed to align IT strategy and goals with business strategy and goals. This focus helps the organization maximize the business value of its IT investment and minimize the business risks.
Platform perspectives
	0.	Stakeholders from the Platform perspective (for example, Chief Technology Officer or CTO, IT managers, and solutions architects) use a variety of architectural dimensions and models to understand and communicate the nature of IT systems and their relationships. They must be able to describe the architecture of the target state environment in detail. The AWS CAF includes principles and patterns for implementing new solutions on the cloud, and for migrating on-premises workloads to the cloud.
Security perspective
	0.	Stakeholders from the Security perspective (for example, Chief Information Security Officer or CISO, IT security managers, and IT security analysts) must ensure that the organization meets security objectives for visibility, auditability, control, and agility. Security perspective stakeholders can use the AWS CAF to structure the selection and implementation of security controls that meet the organization's needs.
Operations perspective
	0.	Stakeholders from the Operations perspective (for example, IT operations managers and IT support managers) define how day-to-day, quarter-to-quarter, and year-to-year business is conducted. Stakeholders from the Operations perspective align with and support the operations of the business. The AWS CAF helps these stakeholders define current operating procedures. It also helps them identify the process changes and training that are needed to implement successful cloud adoption.

W2

Module 2 Cloud Economics and Biling
	0.	This module will address the following topics:
	•	  Fundamentals of Pricing
	•	  Total Cost of Ownership
	•	  AWS Organizations
	•	  AWS Billing and Cost Management
	•	  Technical Support
	0.	The module also includes an instructor-led demonstration that will show you how to interact with the billing dashboard.
	0.	The module also includes an activity that challenges you to estimate the costs for a company by using the AWS Pricing Calculator.
	0.	Objectives
	•	 Explain the AWS pricing philosophy
	•	  Recognize fundamental pricing characteristics
	•	  Indicate the elements of total cost of ownership
	•	  Discuss the results of the AWS Pricing Calculator
	•	  Identify how to set up an organizational structure that simplifies billing and account visibility to review cost data.
	•	  Identify the functionality in the AWS Billing DashboardDescribe how to use AWS Bills, AWS Cost Explorer, AWS Budgets, and AWS Cost and Usage Reports
	•	  Identify the various AWS technical support plans and features
Section 1: Fundamentals of Pricing

AWS Pricing Model
	0.	There are three fundamental drivers of cost with AWS: compute, storage, and outbound data transfer. These characteristics vary somewhat, depending on the AWS product and pricing model you choose.
	0.	In most cases, there is no charge for inbound data transfer or for data transfer between other AWS services within the same AWS Region. There are some exceptions, so be sure to verify data transfer rates before you begin to use the AWS service.
	0.	Outbound data transfer is aggregated across services and then charged at the outbound data transfer rate. This charge appears on the monthly statement as AWS Data Transfer Out.
How do you pay for AWS
	0.	This philosophy is what underlies AWS pricing. While the number and types of services offered by AWS have increased dramatically, our philosophy on pricing has not changed. At the end of each month, you pay for what you use. You can start or stop using a product at any time. No long-term contracts are required.
	0.	AWS offers a range of cloud computing services. For each service, you pay for exactly the amount of resources that you actually need. This utility-style pricing model includes:
	•	  Pay for what you use
	•	  Pay less when you reserve
	•	  Pay less when you use more
	•	  Pay even less as AWS grows
	0.	You will now take a closer look at these core concepts of pricing.
	0.	To learn more about AWS pricing, see the AWS pricing overview at https://dO.awsstatic.com/whitepapers/aws pricing overview.pdf.
Pay for what you use
	0.	Unless you build data centers for a living, you might have spent too much time and money building them. With AWS, you pay only for the services that you consume with no large upfront expenses. You can lower variable costs, so you no longer need to dedicate valuable resources to building costly infrastructure, including purchasing servers, software licenses, or leasing facilities.
	0.	Quickly adapt to changing business needs and redirect your focus on innovation and invention by paying only for what you use and for as long as you need it. All AWS services are available on demand, require no long-term contracts, and have no complex licensing dependencies.
Pay less when you reserve
	0.	For certain services like Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Relational Database Service (Amazon RDS), you can invest in reserved capacity. With Reserved Instances, you can save up to 75 percent over equivalent on-demand capacity. Reserved Instances are available in three options:
	•	All Upfront Reserved Instance (or AURI)
	•	Partial Upfront Reserved Instance (or PURI)
	•	No Upfront Payments Reserved Instance (or NURI)
	0.	When you buy Reserved Instances, you receive a greater discount when you make a larger upfront payment. To maximize your savings, you can pay all upfront and receive the largest discount. Partial Upfront Rls offer lower discounts, but they give you the option to spend less upfront. Lastly, you can choose to spend nothing upfront and receive a smaller discount, which enables you to free capital to spend on other projects.
	0.	By using reserved capacity, your organization can minimize risks, more predictably manage budgets, and comply with policies that require longer-term commitments.
Pay less by using more
	0.	With AWS, you can get volume-based discounts and realize important savings as your usage increases. For services like Amazon Simple Storage Service (Amazon S3), pricing is tiered, which means that you pay less per GB when you use more. In addition, data transfer in is always free.
	0.	Multiple storage services deliver lower storage costs based on your needs. As a result, as your AWS usage needs increase, you benefit from the economies of scale that enable you to increase adoption and keep costs under control.
	0.	As your organization evolves, AWS also gives you options to acquire services that help you address your business needs. For example, the AWS storage services portfolio offers options to help you lower pricing based on how frequently you access data and the performance that you need to retrieve it. To optimize your savings, you can choose the right combination of storage solutions that help you reduce costs while preserving performance, security, and durability.
Pay even less as AWS grows
	0.	AWS constantly focuses on reducing data center hardware costs, improving operational efficiencies, lowering power consumption, and generally lowering the cost of doing business.
	0.	These optimizations and the substantial and growing economies of scale of AWS result in passing savings back to you as lower pricing. Since 2006, AWS has lowered pricing 75 times (as of September 2019).
	0.	Another benefit of AWS growth is that future, higher-performing resources replace current ones for no extra charge.
Custom pricing
	0.	AWS realizes that every customer has different needs. If none of the AWS pricing models work for your project, custom pricing is available for high-volume projects with unique requirements.
AWS free tier
	0.	To help new AWS customers get started in the cloud, AWS offers a free usage tier (the AWS Free Tier) for new customers for up to 1 year. The AWS Free Tier applies to certain services and options. If you are a new AWS customer, you can run a free Amazon Elastic Compute Cloud (Amazon EC2) T2 micro instance for a year, while also using a free usage tier for Amazon S3, Amazon Elastic Block Store (Amazon EBS), Elastic Load Balancing, AWS data transfer, and other AWS services.
	0.	To learn more, see AWS Free Tier at https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsf.Free%20Tier%20Categories=*all.
Services with no charge
	0.	AWS also offers a variety of services for no additional charge.
	•	  Amazon Virtual Private Cloud (Amazon VPC enables you to provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define.
	•	  AWS Identity and Access Management (IAM) controls your users' access to AWS services and resources.
	•	  Consolidated Billing is a billing feature in AWS Organizations to consolidate payment for multiple AWS accounts or multiple Amazon Internet Services Private Limited (AISPL)accounts*. Consolidated billing provides:
	•	One bill for multiple accounts.
	•	The ability to easily track each account's charges.
	•	The opportunity to decrease charges as a result of volume pricing discounts from combined usage.And you can consolidate all of your accounts using Consolidated Billing and get tiered benefits.
	•	AWS Elastic Beanstalk is an even easier way for you to quickly deploy and manage applications in the AWS Cloud.
	•	AWS CloudFormation gives developers and systems administrators an easy way to create a collection of related AWS resources and provision them in an orderly and predictable fashion.
	•	Automatic Scaling automatically adds or removes resources according to conditions you define. The resources you are using increase seamlessly during demand spikes to maintain performance and decrease automatically during demand lulls to minimize costs.
	•	AWS OpsWorks is an application management service that makes it easy to deploy and operate applications of all shapes and sizes.
	0.	Though there is no charge for these services, there might be charges associated with other AWS services used with these services. For example, when you automatically scale additional EC2 instances, there will be charges for those instances.
	0.	* Note: The main difference between AWS accounts and AISPL accounts is the seller of record. AWS accounts are administered by Amazon Web Services, Inc., but AISPL accounts are administered by Amazon Internet Services Private Limited. If you used an Indian address when you created your account, your account's default seller of record is AISPL. By default, AISPL accounts are billed in Indian Rupees (INR). See more on the seller of record here https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-account-payment-aispl.html#determine-seller.

Total cost of ownership

On-premise vsa cloud
	0.	On-premises versus cloud is a question that many businesses ask. The difference between these two options is how they are deployed.
	0.	An on-premises infrastructure is installed locally on a company's own computers and servers. There are several fixed costs, also known as capital expenses, that are associated with the traditional infrastructure. Capital expenses include facilities, hardware, licenses, and maintenance staff. Scaling up can be expensive and time-consuming. Scaling down does not reduce fixed costs.
	0.	A cloud infrastructure is purchased from a service provider who builds and maintains the facilities, hardware, and maintenance staff. A customer pays for what is used. Scaling up or down is simple. Costs are easy to estimate because they depend on service use.
	0.	It is difficult to compare an on-premises IT delivery model with the AWS Cloud. The two are different because they use different concepts and terms.
	0.	Using on-premises IT involves a discussion that is based on capital expenditure, long planning cycles, and multiple components to buy, build, manage, and refresh resources over time. Using the AWS Cloud involves a discussion about flexibility, agility, and consumption-based costs.
What is total cost of ownership
	0.	You can identify the best option by comparing the on-premises solution to a cloud solution. Total Cost of Ownership or TCO) is a financial estimate that is intended to help buyers and owners determine the direct and indirect costs of a product or system. TCO includes the cost of a service, plus all the costs that are associated with owning the service.
	0.	You might want to compare the costs of running an entire infrastructure environment for a specific workload in an on-premises or collocation facility to the same workload running on a cloud-based infrastructure. This comparison is done for budgeting purposes or to build a business case for business decisions about the optimal deployment solution.
TCO considerations
	0.	Some of the costs that are associated with data center management include:
	•	Server costs for both hardware and software, and facilities costs to house the equipment.Storage costs for the hardware, administration, and facilities.
	•	Network costs for hardware, administration, and facilities.
	•	And IT labor costs that are required to administer the entire solution.
	0.	When you compare an on-premises to cloud solution, it is important to accurately assess the true costs of both options. With the cloud, most costs are upfront and readily calculated. For example, cloud providers give transparent pricing based on different usage metrics, such as RAM, storage, and bandwidth, among others. Pricing is frequently fixed per unit of time.
	0.	Customers gain certainty over pricing and are then able to readily calculate costs based on several different usage estimates.
	0.	Compare this process to on-premises technology. Though they are sometimes difficult to determine, calculations of in-house costs must take into account all:
	•	Direct costs that accompany running a server- like power, floor space, storage, and IT operations to manage those resources.
	•	Indirect costs of running a server, like network and storage infrastructure.
	0.	This diagram is conceptual, and it does not include every cost item. For example, depending on the solution you are implementing, software costs can include database, management, and middle-tier costs. Facilities costs can include upgrades, maintenance, building security, taxes, and so on. IT labor costs can include security administration and application administration costs. This diagram includes an abbreviated list to demonstrate the type of costs that are involved in data center maintenance.
On-premise versus all-in-cloud
	0.	Here is a sample cost comparison. This example shows a cost comparison for an on-premises solution and a cloud solution over 3 years. For this comparison, two similar environments were constructed to represent the on-premises and AWS environments. Additional direct and indirect costs that are associated with the on-premises solution were not included. The components of the on-premises solution include:
	•	  1 virtual machine with 4 CPUs, 16 GB of RAM, and a Linux operating system
	•	  Average utilization is 100 percent
	•	  Optimized by RAM
	0.	The components of a comparable AWS environment include:
	•	  1 m4.large instance with 4 CPUs, 16 GB of RAM
	•	  The instance type is a 3-year Partial Upfront Reserved Instance
	0.	The on-premises 3-year total cost is $167,422. The AWS Cloud 3-year total cost is $7,509, which is a 96 percent savings over the on-premises solution. Thus, the 3-year total savings on cloud infrastructure would be $159,913. This comparison helps a business clearly understand the differences between the alternatives.
	0.	What is the difference in cost? Remember, the on-premises solution is predicted. It continues to incur costs whether the capacity is used. In contrast, the AWS solution is commissioned when needed and decommissioned when the resources are no longer in use, which results in lower overall costs.
	0.	For accessibility: Chart comparing three-year total cost of ownership for on-premises and AWS. On-Premises comes out to $167,422 and AWS comes to $7,509. End of accessibility description.
AWS pricing calculator
	0.	AWS offers the AWS Pricing Calculator to help you estimate a monthly AWS bill. You can use this tool to explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs. This enables you to make informed decisions about using AWS. You can plan your AWS costs and usage or price out setting up a new set of instances and services.
	0.	The AWS Pricing Calculator helps you:
	•	  Estimate monthly costs of AWS services
	•	  Identify opportunities for cost reduction
	•	  Model your solutions before building them
	•	  Explore price points and calculations behind your estimate
	•	  Find the available instance types and contract terms that meet your needs
	0.	The AWS Pricing Calculator enables you to name your estimate and create and name groups of services. Groups are containers that you add services to in order to organize and build your estimate. You can organize your groups and services by cost-center, department, product architecture, etc.
	0.	For more information, see the AWS Pricing Calculator website at httos://calculator.aws/#/.
Reading an estimate
	0.	AWS Pricing Calculator estimates are broken into:
	•	  The total for your first 12 months - The total estimate for your current group and all of the services and groups in your current group. It combines the upfront and monthly estimates.
	•	  Your total upfront - How much you are estimated to pay upfront as you set up your AWS stack.
	•	  Your total monthly - How much you're estimated to spend every month while you run yourAWS stack.
	0.	Within a group, you can see how much each service is estimated to cost. If you want to price out different ways to build your AWS setup, you can use different groups for each variation of your setup and compare the estimates for the different setups.
	0.	For more information, see Reading an estimate at https://docs.aws.amazon.com/pricing-calculator/latest/userguide/what-is-pricing-calculator.html.
	0.	For accessibility: Example AWS Pricing Calculator estimate. The first 12 month total is $886.92, the total upfront cost is $O, and the total monthly cost is $73.91. End of accessibility description.
Activity: AWS pricing calculator activity
	0.	Break up into groups of four or five and use the AWS Pricing Calculator and the specifications provided to develop a cost estimate.
	0.	Be prepared to report your findings back to the class.
Additional benefit consideration
	0.	Hard benefits include reduced spending on compute, storage, networking, and security. They also include reductions in hardware and software purchases; reductions in operational costs, backup, and disaster recovery; and a reduction in operations personnel.
	0.	Cloud Total Cost of Ownership defines what will be spent on the technology after adoption-or what it costs to run the solution. Typically, a TCO analysis looks at the as-is on-premises infrastructure and compares it with the cost of the to-be infrastructure state in the cloud. While this difference might be easy to calculate, it might only provide a narrow view of the total financial impact of moving to the cloud.
	0.	A return on investment (ROl) analysis can be used to determine the value that is generated while considering spending and saving. This analysis starts by identifying the hard benefits in terms of direct and visible cost reductions and efficiency improvements.
	0.	Next, soft savings are identified. Soft savings are value points that are challenging to accurately quantify, but they can be more valuable than the hard savings. It is important for you to understand both hard and soft benefits to understand the full value of the cloud. Soft benefits include:
	•	Reusing service and applications that enable you to define (and redefine solutions) by using the same cloud service
	•	Increased developer productivity
	•	  Improved customer satisfaction
	•	  Agile business processes that can quickly respond to new and emerging opportunities
	•	  Increased global reach
	0.	Now, you will review a case study from Delaware North to see an actual TCO example.
Case study: Total Cost Of Ownership (1 of 6)
	0.	Background
	•	Delaware North originated in 1915 as a peanut and popcorn concessions vendor; today, it's a major food and hospitality company. Although the company deliberately keeps a low profile, it is a leader in the food-service and hospitality industry.
	•	Delaware North serves more than 500 million customers annually at more than 200 locations around the world, including venues the Kennedy Space Center in Florida, London Heathrow Airport, Kings Canyon Resort in Australia, and the Green Bay Packers' Lambeau Field in Wisconsin.
	•	This global presence has turned Delaware North into a $3 billion enterprise.
Case study: Total cost of ownership (2 of 6)
	0.	The company's on-premises data center was becoming too expensive and inefficient to support its global business operations.
	0.	Kevin Quinlivan, Delaware North's Chief Information Officer, said, "As the company continued to grow, the demand to rapidly deploy new solutions to meet customer requirements increased as well. This fact, combined with the need to constantly upgrade aging equipment, required an even greater commitment of resources on our part. We had to find a better strategy."
	0.	Delaware North turned to AWS for a solution.
Case study: total cost of ownership (3 of 6)
	0.	After a successful migration of about 50 websites to AWS in 2013, Delaware North evaluated the cost benefit and Total Cost of Ownership to move their IT infrastructure to AWS. Their focus was to answer executive-level business demands for measurable benefits that could convince an executive committee that the AWS Cloud was the right approach.
	0.	The evaluation process centered on three criteria:
	•	  First, a cloud solution needed a broad set of technologies that could handle all of Delaware North's enterprise workloads while delivering support for critical functions.
	•	  From an operational perspective, Delaware North wanted the features and flexibility to modify core IT processes to improve efficiencies and lower costs. This included eliminating redundant or time-consuming tasks like patching software or pushing test and development tasks through outdated systems that, in the past, added months to the deployment of new services.
	•	  Finally, financial requirements needed to demonstrate a return on investment with a solid cost-benefit justification for moving away from their existing data center environment.
Case study: Total cost of ownership (4 of 6)
	0.	A cost comparison completed by Delaware North demonstrated that it could save $3.5 million US dollars based on a 5-year run rate by moving its on-premises data center to AWS and using 3-year Amazon EC2 Reserved Instances and Reserved Instance renewals.
	0.	Quinlivan noted that the deep technology stack available on AWS was more than sufficient to meet the company's technical and operational requirements. The pricing structure of the AWS offerings-which includes paying only for what is used -provided total cost of ownership benefits that were presented to senior leaders.
	0.	Quinlivan stated, "We compared the costs of keeping our on-premises data center versus moving to the AWS Cloud, measuring basic infrastructure items such as hardware cost and maintenance." He also says "We estimate that moving to AWS will save us at least $3.5 million over five years by reducing our server hardware by more than 90 percent. But the cost savings will likely be greater due to additional benefits, like the increased compute capacity we can get using AWS. That lets us continually add more and larger workloads than we could using a traditional data center infrastructure, and achieve savings by only paying for what we use."
	0.	Delaware North moved almost all of its applications to AWS, including enterprise software such as its Fiorano middleware, Crystal Reports and QLIK business intelligence solutions, its Citrix virtual desktop system, and Microsoft System Center Configuration Manager, which is used to manage workstations.
	0.	The most dramatic physical change was the elimination of 205 servers. Everything that ran on that hardware was migrated to AWS. The IT department decided to keep about 20 servers on-premises at the new headquarters building to run communications and file-and-print tasks.
	0.	"We erred on the side of caution to ensure there is no latency with these tasks, but once we reach a certain comfort level, we may move these to the cloud as well," said Scott Mercer, head of the IT department's service-oriented architecture team.
Case study: Total cost of ownership (5 of 6)
	0.	This chart displays the cost comparison done by Delaware North showing the costs of their on-premises environment and the proposed AWS environment. The estimates showed a $3.5 million savings based on a five-year run rate by moving from an on-premises data center to AWS.
Case study: Total cost of ownership (6 of 6)
	0.	About 6 months into its cloud migration, Delaware North realized benefits in addition to its data center consolidation, including cost-effective security compliance, enhanced disaster recovery, and faster deployment times for new services.
	0.	"Robust security in a retail environment is critical for us because of our many retail operations, and AWS is enormously helpful for that," said Brian Mercer, the senior software architect for the project. "By leveraging the security best practices of AWS, we've been able to eliminate a lot of compliance tasks that in the past took up valuable time and money."
	0.	Brian Mercer added that the company also increased its disaster recovery capabilities at a lower cost than what was available in its previous data center deployment. "It significantly improved our business continuity capabilities, including seamless failovers," he said.
	0.	The solution is also helping Delaware North operate with greater speed and agility. For example, it can bring in new businesses-either through contracts or acquisitions-and get them online more quickly than in the past by eliminating the need for traditional IT procurement and provisioning. It used to take between 2 and 3 weeks to provision new business units; now it takes 1 day. The Delaware North IT team is also using AWS to overhaul its operations by eliminating outdated and cumbersome processes, cleaning up documentation, and using the benefits of running test and development tasks in combination with rapid deployment of services through the cloud.
	0.	"Our DevOps team can now spin up the resources to push out a service in just minutes, compared to the weeks it used to take," said Brian Mercer. "With AWS, we can respond much faster to business needs. And we can start repurposing time and resources to deliver more value and services to our internal teams and to our customers."
Section 3 AWS Organizations

Introduction to AWS Organizations
	0.	AWS Organizations is a free account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. AWS
	0.	Organizations include consolidated billing and account management capabilities that help you to better meet the budgetary, security, and compliance needs of your business.
	0.	The main benefits of AWS Organizations are:
	•	  Centrally managed access policies across multiple AWS accounts.
	•	  Controlled access to AWS services.
	•	  Automated AWS account creation and management.
	•	  Consolidated billing across multiple AWS accounts.
AWS Organizations terminology

	0.	Here is some terminology to understand the structure of AWS Organizations.
	0.	The diagram shows a basic organization, or root, that consists of seven accounts that are organized into four organizational units (or OUs). An OU is a container for accounts within a root.
	0.	An OU can also contain other OUs. This structure enables you to create a hierarchy that looks like an upside-down tree with the root at the top. The branches consist of child OUs and they move downward until they end in accounts, which are like the leaves of the tree.
	0.	When you attach a policy to one of the nodes in the hierarchy, it flows down and it affects all the branches and leaves. This example organization has several policies that are attached to some of the OUs or are attached directly to accounts.
	0.	An OU can have only one parent and, currently, each account can be a member of exactly one OU. An account is a standard AWS account that contains your AWS resources. You can attach a policy to an account to apply controls to only that one account.
Key features and benefits
	0.	AWS Organizations enables you to:
	•	  Create service control policies (SCs) that centrally control AWS services across multiple AWS accounts.
	•	  Create groups of accounts and then attach policies to a group to ensure that the correct policies are applied across the accounts.
	•	  Simplify account management by using application programming interfaces (APIs to automate the creation and management of new AWS accounts.
	•	  Simplify the billing process by setting up a single payment method for all the AWS accounts in your organization. With consolidated billing, you can see a combined view of charges that are incurred by all your accounts, and you can take advantage of pricing benefits from aggregated usage. Consolidated billing provides a central location to manage billing across all of your AWS accounts, and the ability to benefit from volume discounts.
Security with AWS Organizations
	0.	AWS Organizations does not replace associating AWS Identity and Access Management (IAM) policies with users, groups, and roles within an AWS account.
	0.	With IAM policies, you can allow or deny access to AWS services (such as Amazon S3), individual AWS resources (such as a specific S3 bucket), or individual API actions (such as s3:CreateBucket).
	0.	An IAM policy can be applied only to IAM users, groups, or roles, and it can never restrict the AWS account root user.
	0.	In contrast, with Organizations, you use service control policies (§Cs) to allow or deny access to particular AWS services for individual AWS accounts or for groups of accounts in an OU. The specified actions from an attached SCP affect all IAM users, groups, and roles for an account, including the AWS account root user.
Organizations setup
	0.	Keep in mind that this process assumes that you have access to two existing AWS accounts, and that you can sign in to each account as an administrator.
	0.	Review these steps for setting up AWS Organizations:
	•	Step 1 is to create your organization with your current AWS account as the primary account.You also invite one AWS account to join your organization and create another account as a member account.
	•	Step 2 is to create two organizational units in your new organization and place the member accounts in those OUs.
	•	Step 3 is to create service control policies, which enable you to apply restrictions to what actions can be delegated to users and roles in the member accounts. A service control policy is a type of organization control policy.
	•	Step 4 is to test your organization's policies. Sign in as a user for each of the roles (such as OU1 or OU2) and see how the service control policies impact account access. Alternatively, you can use the IAM policy simulator to test and troubleshoot IAM and resource-based policies that are attached to IAM users, groups, or roles in your AWS account.
	0.	To learn more about the IAM policy simulator, see the IAM policy simulator at https://docs.aws.amazon.com/IAM/latest/UserGuide/access policies testing-policies.html.
Limits of AWS Organizations

	0.	There are restrictions on names that you can create in AWS Organizations, which includes names of accounts, OUs, roots, and policies.
	0.	Names must be composed of Unicode characters and not exceed 250 characters in length.
	0.	AWS Organizations has several maximum and minimum values for entities.
	0.	For accessibility: List of the AWS Organizations limits, including names, number of accounts (varies), number of roots (1), number of OUs (1,000), number of policies (1,000), max size of control policy document (5,120 bytes), max nesting of BUs (5 levels of BUs under a root), invitations sent per day (20), member accounts created concurrently (5), and entities to which you can attach a policy (unlimited).End of accessibility description.
Accessing AWS Organizations
	0.	AWS Organizations can be managed through different interfaces.
	0.	The AWS Management Console is a browser-based interface that you can use to manage your organization and your AWS resources. You can perform any task in your organization by using the console.
	0.	AWS Command Line Interface (AWS CLI) tools enable you to issue commands at your system's command line to perform AWS Organizations tasks and AWS tasks. This method can be faster and more convenient than using the console.
	0.	You can use also AWS software development kits (SDKs) to handle tasks such as cryptographically signing requests, managing errors, and retrying requests automatically. AWS SDKs consist of libraries and sample code for various programming languages and platforms, such as Java, Python, Ruby, .NET, iOS, and Android.
	0.	The AWS Organizations HTTPS Query API gives you programmatic access to AWS Organizations and AWS. You can use the API to issue HTTPS requests directly to the service. When you use the HTTPS API, you must include code to digitally sign requests by using your credentials.

Section 4 AWS Filing and Cost Management

Introducing AWS Billing and Cost Management
	0.	AWS Billing and Cost Management is the service that you use to pay your AWS bill, monitor your usage, and budget your costs. Billing and Cost Management enables you to forecast and obtain a better idea of what your costs and usage might be in the future so that you can plan ahead.
	0.	You can set a custom time period and determine whether you would like to view your data at a monthly or daily level of granularity.
	0.	With the filtering and grouping functionality, you can further analyze your data using a variety of available dimensions. The AWS Cost and Usage Report Tool enables you to identify opportunities for optimization by understanding your cost and usage data trends and how you are using your
	0.	AWS implementation.
AWS Biling Dashboard
	0.	The AWS Billing Dashboard lets you view the status of your month-to-date AWS expenditure, identify the services that account for the majority of your overall expenditure, and understand at a high level how costs are trending.
	0.	One of the graphs that is located on the dashboard is the Spend Summary. The Spend Summary shows you how much you spent last month, the estimated costs of your AWS usage for the month to date, and a forecast for how much you are likely to spend this month.
	0.	Another graph is Month-to-Date Spend by Service, which shows the top services that you use most and the proportion of costs that are attributed to that service.
Tools
	0.	From the billing dashboard, you can access several other cost management tools that you can use to estimate and plan your AWS costs. These tools include AWS Bills, AWS Cost Explorer, AWS Budgets, and AWS Cost and Usage Reports.
Monthly Bills
	0.	The AWS Bills page lists the costs that you incurred over the past month for each AWS service, with a further breakdown by AWS Region and linked account.
	0.	This tool gives you access to the most up-to-date information on your costs and usage, including your monthly bill and the detailed breakdown of the AWS services that you use.
Cost Explorer
	0.	The AWS Billing and Cost Management console includes the Cost Explorer page for viewing your AWS cost data as a graph.
	0.	With Cost Explorer, you can visualize, understand, and manage your AWS costs and usage over time.
	0.	The Cost Explorer includes a default report that visualizes your costs and usage for your top cost-incurring AWS services. The monthly running costs report gives you an overview of all your costs for the past 3 months. It also provides forecasted numbers for the coming month, with a corresponding confidence interval.
	0.	The Cost Explorer is a free tool that enables you to:
	•	  View charts of your costs.
	•	  View cost data for the past 13 months.
	•	  Forecast how much you are likely to spend over the next 3 months.
	•	  Discover patterns in how much you spend on AWS resources over time and identify cost problem areas.
	•	  Identify the services that you use the most
	•	  View metrics, like which Availability Zones have the most traffic or which linked AWS account is used the most.
Forecast and track costs
	0.	AWS Budgets uses the cost visualization that is provided by Cost Explorer to show you the status of your budgets and to provide forecasts of your estimated costs.
	0.	You can also use AWS Budgets to create notifications for when you go over your budget for the month, or when your estimated costs exceed your budget. Budgets can be tracked at the monthly, quarterly, or yearly level, and you can customize the start and end dates. Budget alerts can be sent via email or via Amazon Simple Notification Service (Amazon SNS).
	0.	For accessibility: The AWS Billing budgets panel showing budget names, current and future costs and usages, and headings for current and forecasted versus budgets. End of accessibility description.
Cost and usage reporting
	0.	The AWS Cost and Usage Report is a single location for accessing comprehensive information about your AWS costs and usage. This tool lists the usage for each service category that is used by an account (and its users) in hourly or daily line items, and any tax that you activated for tax allocation purposes.
	0.	You can choose to have AWS to publish billing reports to an S3 bucket. These reports can be updated once a day.

Section 5 Technical support

AWS Support (1 of 2)
	0.	Whether you are new or continuing to adopt AWS services and applications as your business solutions, AWS want help you do amazing things with AWS. AWS Support can provide you with a unique combination of tools and expertise based on your current or future planned use cases.
	0.	AWS Support was developed to provide complete support and the right resources to aid your success. We want to support all our customers, including customers that might be experimenting with AWS, those that are looking for production uses of AWS, and also customers that use AWS as a business-critical resource. AWS Support can vary the type of support that is provided, depending on the customer's needs and goals.
AWS Support (2 of 2)
	0.	With AWS, customers can plan, deploy, and optimize with confidence.
	0.	If you would like proactive guidance, AWS Support has Technical Account Managers (TAMs) who are designated as that user's primary point of contact. The TAM can provide guidance, architectural review, and continuous ongoing communication to keep you informed and prepared as you plan, deploy, and optimize your solutions.
	0.	If you want to ensure that you follow best practices to increase performance and fault tolerance in the AWS environment, AWS Support has AWS Trusted Advisor. AWS Trusted Advisor is like a customized cloud expert. It is an online resource that checks for opportunities to reduce monthly expenditures and increase productivity.
	0.	For account assistance, the Support Concierge is a billing and account expert who will provide quick and efficient analysis on billing and account issues. The concierge addresses all nontechnical billing and account-level inquiries.
Support plans
	0.	AWS wants you to be able to plan, deploy, and optimize with confidence. We have developed specific plans to support you, including Basic, Developer, Business, and Enterprise support plans.
	•	  The Basic Support Plan offers:
	•	  24/7 access to customer service, documentation, whitepapers and support forums.
	•	  Access to six core Trusted Advisor checks.
	•	  Access to Personal Health Dashboard.
	•	 The Developer Support Plan offers resources for customers that are testing or doing early development on AWS, and any customers who:
	•	  Want access to guidance and technical support.
	•	  Are exploring how to quickly put AWS to work.
	•	  Use AWS for non-production workloads or applications.
	•	  The Business Support Plan offers resources for customers that are running production workloads on AWS, and any customers who:
	•	  Run one or more applications in production environments.
	•	  Have multiple services activated, or use key services extensively.
	•	  Depend on their business solutions to be available, scalable, and secure.
	•	  The Enterprise Support Plan offers resources for customers that are running business and mission-critical workloads on AWS, and any customers who want to:
	•	  Focus on proactive management to increase efficiency and availability.
	•	  Build and operate workloads that follow AWS best practices.
	•	  Use AWS expertise to support launches and migrations.
	•	  Use a Technical Account Manager (TAM), who provides technical expertise for the full range of AWS services and obtains a detailed understanding of your use case and technology architecture. The Technical Account Manager is the primary point of contact for ongoing support needs.
Case severity and response times
	0.	It addition to understanding the costs that are associated with different support plans, it is critical that you understand the service levels that are associated with each plan. In addition to the support plan you select, the case severity will drive the type of response that you receive. There are five different severity levels:
	•	  Critical - Your business is at risk. Critical functions of your application are unavailable.
	•	  Urgent - Your business is significantly impacted. Important functions of your application are unavailable.
	•	  High - Important functions of your application are impaired or degraded.
	•	  Normal - Non-critical functions of your application are behaving abnormally, or you have a time-sensitive development question.
	•	  Low - You have a general development question, or you want to request a feature.
	0.	Note that there is no case support with the Basic Support Plan. These response times should be considered when you determine which support plan is best for your organization.
	0.	To learn more about AWS Support plans, see Compare AWS Support Plans at https://aws.amazon.com/premiumsupport/plans/.
	0.	For accessibility: Case severity response times for the four different support levels (basic, developer plan, business plan, and enterprise plan, each with more support respectively. End of accessibility description.
Activity support plan scavenger hunt
	0.	In this activity, your group will read the description of a business and develop a recommendation for the appropriate support plan. When you report back to the class, describe the support plan that you selected, and the decision-making criteria that you used to develop your recommendation.
Quiz
	0.	Reserved resources are available in AURI, NURI, and PURI
	0.	AWS Cost Explorer can give you more details about amazon EC2 billing activity for the past 3 months
	0.	To receive the discount rate associated with Reserved instances, you do not need to make a full, upfront payment
	0.	Storage is typically charged based on how many gigabytes you use. For services that use tiered pricing, the per gigabyte cost less when you use more.
	0.	The four support plans offered by AWS Support are Basic, Developer, Business, and Enterprise
	0.	The AWS Pricing Calculator lets you model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contact terms that meet your needs
	0.	As AWS grows, the general cost of doing business is reduced and savings are passed back to the customer due to economies of scale.
	0.	AWS offers these services at no charge, though you might be charged for other AWS services that you use in conjunction with these services.
	0.	With AWS Organizations, you can create groups of accounts and then attach policies to a group. You can use the API to automate account creation and management
	0.	The AWS Free Tier applies to certain services and options

W3

Module 3 AWS Global Infrastructure Overview

Module Overview
	0.	This module will address the following topics:
	•	  AWS Global Infrastructure
	•	  AWS service and service category overview
	0.	The module includes an educator-led demonstration that focuses on the details of the AWS Global Infrastructure. The module also includes a hands-on activity where you will explore the AWS Management Console.
	0.	Finally, you will be asked to complete a knowledge check that will test your understanding of the key concepts that are covered in this module.
Section 1 AWS Global Infrastructure
	0.	The AWS Global Infrastructure is designed and built to deliver a flexible, reliable, scalable, and secure cloud computing environment with high-quality global network performance.
	0.	AWS continually updates its global infrastructure footprint. Visit one of the following web pages for current infrastructure information:
	•	AWS Global Infrastructure Map: https://aws.amazon.com/about-aws/global- infrastructure/#AWS Global Infrastructure Map Choose a circle on the map to view summary information about the Region represented by the circle.
	•	Regions and Availability Zones: https://aws.amazon.com/about-aws/global-infrastructure/regions az/ Choose a tab to view a map of the selected geography and a list of Regions, Edge locations, Local zones, and Regional Caches.
Educator-Led Demo: AWS Global Infrastructure Details
	0.	The educator might now choose to conduct a live demonstration of the AWS Global Infrastructure map introduced on the previous slide. This resource provides an interactive way to learn about the AWS Global Infrastructure. The remaining slides in this section cover many of the same topics and go into greater detail on some topics.
AWS Regions
	0.	The AWS Cloud infrastructure is built around Regions. AWS has 22 Regions worldwide. An AWS Region is a physical geographical location with one or more Availability Zones. Availability Zones in turn consist of one or more data centers.
	0.	To achieve fault tolerance and stability, Regions are isolated from one another. Resources in one Region are not automatically replicated to other Regions. When you store data in a specific Region, it is not replicated outside that Region.
	0.	It is your responsibility to replicate data across Regions, if your business needs require it.
	0.	AWS Regions that were introduced before March 20, 2019 are enabled by default. Regions that were introduced after March 20, 2019-such as Asia Pacific (Hong Kong) and Middle East (Bahrain)-are disabled by default. You must enable these Regions before you can use them. You can use the AWS Management Console to enable or disable a Region.
	0.	Some Regions have restricted access. An Amazon AWS (China) account provides access to the Beijing and Ningxia Regions only. To learn more about AWS in China, see: https://www.amazonaws.cn/en/about-aws/china/. The isolated AWS GovCloud (US) Region is designed to allow US government agencies and customers to move sensitive workloads into the cloud by addressing their specific regulatory and compliance requirements.
	0.	For accessibility: Snapshot from the infrastructure.aws website that shows a picture of downtown London including the Tower Bridge and the Shard. It notes that there are three Availability Zones in the London region. End of accessibility description.
Selecting a Region
	0.	There are a few factors that you should consider when you select the optimal Region or Regions where you store data and use AWS services.
	0.	One essential consideration is data governance and legal requirements. Local laws might require that certain information be kept within geographical boundaries. Such laws might restrict the Regions where you can offer content or services. For example, consider the European Union (EU) Data Protection Directive.
	0.	All else being equal, it is generally desirable to run your applications and store your data in a Region that is as close as possible to the user and systems that will access them. This will help you reduce latency. Clouding is one website that you can use to test latency between your location and all AWS Regions. To learn more about Clouding, see: http://www.cloudping.info/
	0.	Keep in mind that not all services are available in all Regions. To learn more, see: https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/?p=tgi&loc=4.
	0.	Finally, there is some variation in the cost of running services, which can depend on which Region you choose. For example, as of this writing, running an On-Demand t3.medium size Amazon Elastic Compute Cloud (Amazon EC2) Linux instance in the US East (Ohio) Region costs $0.0416 per hour, but running the same instance in the Asia Pacific (Tokyo) Region costs $0.0544 per hour.
Availability Zones
	0.	Each AWS Region has multiple, isolated locations that are known as Availability Zones.
	0.	Each Availability Zone provides the ability to operate applications and databases that are more highly available, fault-tolerant, and scalable than would be possible with a single data center. Each Availability Zone can include multiple data centers (typically three), and at full-scale, they can include hundreds of thousands of servers. They are fully isolated partitions of the AWS Global Infrastructure. Availability Zones have their own power infrastructure, and they are physically separated by many kilometers from other Availability Zones-though all Availability Zones are within 100 km of each other.
	0.	All Availability Zones are interconnected with high-bandwidth, low-latency networking over fully redundant, dedicated fiber that provides high-throughput between Availability Zones. The network accomplishes synchronous replication between Availability Zones.
	0.	Availability Zones help build highly available applications. When an application is partitioned across Availability Zones, companies are better isolated and protected from issues such as lightning, tornadoes, earthquakes, and more.
	0.	You are responsible for selecting the Availability Zones where your systems will reside. Systems can span multiple Availability Zones. AWS recommends replicating across Availability Zones for resiliency. You should design your systems to survive the temporary or prolonged failure of an Availability Zone if a disaster occurs.
AWS data centers
	0.	The foundation for the AWS infrastructure is the data centers. Customers do not specify a data center for the deployment of resources. Instead, an Availability Zone is the most granular level of specification that a customer can make. However, a data center is the location where the actual data resides. Amazon operates state-of-the-art, highly available data centers. Although rare, failures can occur that affect the availability of instances in the same location. If you host all your instances in a single location that is affected by such a failure, none of your instances will be available.
	0.	Data centers are securely designed with several factors in mind:
	0.	Each location is carefully evaluated to mitigate environmental risk.
	•	Data centers have a redundant design that anticipates and tolerates failure while maintaining service levels.
	•	To ensure availability, critical system components are backed up across multiple Availability Zones.
	•	To ensure capacity, AWS continuously monitors service usage to deploy infrastructure to support availability commitments and requirements.
	•	Data center locations are not disclosed and all access to them is restricted.
	•	In case of failure, automated processes move data traffic away from the affected area.
	0.	AWS uses custom network equipment sourced from multiple original device manufacturers (ODMs). ODMs design and manufacture products based on specifications from a second company. The second company then rebrands the products for sale.
Points of Presence
	0.	Amazon CloudFront is a content delivery network (CD used to distribute content to end users to reduce latency. Amazon Route 53 is a Domain Name System (DNS) service. Requests going to either one of these services will be routed to the nearest edge location automatically in order to lower latency.
	0.	AWS Points of Presence are located in most of the major cities around the world. By continuously measuring internet connectivity, performance and computing to find the best way to route requests, the Points of Presence deliver a better near real-time user experience. They are used by many AWS services, including Amazon CloudFront, Amazon Route 53, AWS Shield, and AWS Web Application Firewall (AWS WAF) services.
	0.	Regional edge caches are used by default with Amazon CloudFront. Regional edge caches are used when you have content that is not accessed frequently enough to remain in an edge location. Regional edge caches absorb this content and provide an alternative to that content having to be fetched from the origin server.
AWS Infrastructure Features
	0.	Now that you have a good understanding of the major components that comprise the AWS Global Infrastructure, let's consider the benefits provided by this infrastructure.
	0.	Thew AWS Global Infrastructure has several valuable features:
	•	First, it is elastic and scalable. This means resources can dynamically adjust to increases or decreases in capacity requirements. It can also rapidly adjust to accommodate growth.
	•	Second, this infrastructure is fault tolerant, which means it has built-in component redundancy which enables it to continue operations despite a failed component.
	•	Finally, it requires minimal to no human intervention, while providing high availability with minimal down time.

Section 2 AWS services and service category overview

AWS Foundational Services
	0.	As discussed previously, the AWS Global Infrastructure can be broken down into three elements: Regions, Availability Zones, and Points of Presence, which include edge locations. This infrastructure provides the platform for a broad set of services, such as networking, storage, compute services, and databases-and these services are delivered as an on-demand utility that is available in seconds, with pay-as-you-go pricing.
	0.	For accessibility: Marketing diagram showing infrastructure at the bottom, consisting of Regions, Availability Zones, and edge locations. The next level up is labeled Foundational Services and includes graphics for compute, networking, and storage. That level is highlighted. Next level up is platform services that includes databases, analytics, app services, deployment and management, and mobile services. Top layer is labeled applications and includes virtual desktops and collaboration and sharing. End of accessibility description.

AWS Categories of Services
	0.	AWS offers a broad set of cloud-based services. There are 23 different product or service categories, and each category consists of one or more services. This course will not attempt to introduce you to each service. Rather, the focus of this course is on the services that are most widely used and offer the best introduction to the AWS Cloud. This course also focuses on services that are more likely to be covered in the AWS Certified Cloud Practitioner exam.
	0.	The categories that this course will discuss are highlighted on the slide: Compute, Cost Management, Database, Management and Governance, Networking and Content Delivery, Security, Identity, and Compliance, and Storage.
	0.	To learn more about AWS products, see Cloud Products at https://aws.amazon.com/products/. All AWS products are organized into the service categories that are shown here. For example, if you click Compute, you will see that Amazon Elastic Compute Cloud (Amazon EC2) is first on the list. The compute category also lists many other products and services.
	0.	If you click Amazon EC2, it takes you to the Amazon EC2 page. Each product page provides a detailed description of the product and lists some of its benefits
	0.	Explore the different service groups to understand the categories and services within them. Now that you know how to locate information about different services, this module will discuss the highlighted service categories. The next seven slides list the individual services -within each of the categories highlighted above-that this course will discuss.
Storage Service Category

	0.	AWS storage services include the services listed here, and many others.
	0.	Amazon Simple Storage Service (Amazon S3) is an object storage service that offers scalability, data availability, security, and performance. Use it to store and protect any amount of data for websites, mobile apps, backup and restore, archive, enterprise applications, Internet of Things (loT) devices, and big data analytics.
	0.	Amazon Elastic Block Store (Amazon EBS) is high-performance block storage that is designed for use with Amazon EC2 for both throughput and transaction intensive workloads. It is used for a broad range of workloads, such as relational and non-relational databases, enterprise applications, containerized applications, big data analytics engines, file systems, and media workflows.
	0.	Amazon Elastic File System (Amazon EFS) provides a scalable, fully managed elastic Network File System (NFS) file system for use with AWS Cloud services and on-premises resources. It is built to scale on demand to petabytes, growing and shrinking automatically as you add and remove files. It reduces the need to provision and manage capacity to accommodate growth.
	0.	Amazon Simple Storage Service Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 11 9s of durability, and to provide comprehensive security and compliance capabilities to meet stringent regulatory requirements.
Compute Service Category

	0.	AWS compute services include the services listed here, and many others.
	0.	Amazon Elastic Compute Cloud (Amazon EC2) provides resizable compute capacity as virtual machines in the cloud.
	0.	Amazon EC2 Auto Scaling enables you to automatically add or remove EC2 instances according to conditions that you define.
	0.	Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers.
	0.	Amazon Elastic Container Registry (Amazon ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.
	0.	AWS Elastic Beanstalk is a service for deploying and scaling web applications and services on familiar servers such as Apache and Microsoft Internet Information Services (IS).
	0.	AWS Lambda enables you to run code without provisioning or managing servers. You pay only for the compute time that you consume. There is no charge when your code is not running.
	0.	Amazon Elastic Kubernetes Service (Amazon EKS) makes it easy to deploy, manage, and scale containerized applications that use Kubernetes on AWS.
	0.	AWS Fargate is a compute engine for Amazon ECS that allows you to run containers without having to manage servers or clusters.
Database Service Category

	0.	AWS database services include the services listed here, and many others.
	0.	Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups.
	0.	Amazon Aurora is a MySQL and PostgreSQL-compatible relational database. It is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases.
	0.	Amazon Redshift enables you to run analytic queries against petabytes of data that is stored locally in Amazon Redshift, and directly against exabytes of data that are stored in Amazon S3. It delivers fast performance at any scale.
	0.	Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale, with built-in security, backup and restore, and in-memory caching.
Networking and Content Delivery Service Category

	0.	AWS networking and content delivery services include the services listed here, and many others.
	0.	Amazon Virtual Private Cloud (Amazon VPC) enables you to provision logically isolated sections of the AWS Cloud.
	0.	Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.
	0.	Amazon CloudFront is a fast content delivery network (CD) service that securely delivers data, videos, applications, and application programming interfaces (APIs) to customers globally, with low latency and high transfer speeds.
	0.	AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway.
	0.	Amazon Route 53 is a scalable cloud Domain Name System (DNS) web service designed to give you a reliable way to route end users to internet applications. It translates names like www.example.com) into the numeric IP addresses (like 192.0.2.1 that computers use to connect to each other.
	0.	AWS Direct Connect provides a way to establish a dedicated private network connection from your data center or office to AWS, which can reduce network costs and increase bandwidth throughput.
	0.	AWS VPN provides a secure private tunnel from your network or device to the AWS global network.
Security, Identity, and Compliance Service Category

	0.	AWS security, identity, and compliance services include the services listed here, and many others.
	0.	AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. By using IAM, you can create and manage AWS users and groups. You can use IAM permissions to allow and deny user and group access to AWS resources.
	0.	AWS Organizations allows you to restrict what services and actions are allowed in your accounts.
	0.	Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps.
	0.	AWS Artifact provides on-demand access to AWS security and compliance reports and select online agreements.
	0.	AWS Key Management Service (AWS KMS) enables you to create and manage keys. You can use AWS KMS to control the use of encryption across a wide range of AWS services and in your applications.
	0.	AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.
AWS Cost Management Service Category

	0.	AWS cost management services include the services listed here, and others.
	0.	The AWS Cost and Usage Report contains the most comprehensive set of AWS cost and usage data available, including additional metadata about AWS services, pricing, and reservations.
	0.	AWS Budgets enables you to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount.
	0.	AWS Cost Explorer has an easy-to-use interface that enables you to visualize, understand, and manage your AWS costs and usage over time.
Management and Governance Service Category

	0.	AWS management and governance services include the services listed here, and others.
	0.	The AWS Management Console provides a web-based user interface for accessing your AWS account.
	0.	AWS Config provides a service that helps you track resource inventory and changes.
	0.	Amazon CloudWatch allows you to monitor resources and applications.
	0.	AWS Auto Scaling provides features that allow you to scale multiple resources to meet demand.
	0.	AWS Command Line Interface provides a unified tool to manage AWS services.
	0.	AWS Trusted Advisor helps you optimize performance and security.
	0.	AWS Well-Architected Tool provides help in reviewing and improving your workloads.
	0.	AWS CloudTrail tracks user activity and API usage.
Quiz
	0.	Amazon CloudFront uses AWS edge locations to ensure low-latency delivery
	0.	You can run applications and workloads from a Region closer to the end users to decrease latency
	0.	These are examples of service categories that AWS offers
	0.	AWS Regions host two or more availability zones.
	0.	Fault tolerant means the infrastructure has built-in component redundancy and elastic and scalable means that resources dynamically adjust to increase or decrease in capacity requirements
	0.	It’s true that Availability Zones within a Region are connected through low-latency links
	0.	A data center cannot be used for more than one Availability Zone
	0.	It’s true that Availability Zones within a Region are connected through low-latency links
	0.	AWS highly recommends provisioning your compute resources across multiple Availability Zones.
	0.	Edge locations do not need to be located in the same general area as Regions

W4

Module 4 AWS Cloud Security

Section 1 AWS shared responsibility model

AWS shared responsibility model

	0.	Security and compliance are a shared responsibility between AWS and the customer. This shared responsibility model is designed to help relieve the customer's operational burden. At the same time, to provide the flexibility and customer control that enables the deployment of customer solutions on AWS, the customer remains responsible for some aspects of the overall security. The differentiation of who is responsible for what is commonly referred to as security "of" the cloud versus security "in" the cloud.
	0.	AWS operates, manages, and controls the components from the software virtualization layer down to the physical security of the facilities where AWS services operate. AWS is responsible for protecting the infrastructure that runs all the services that are offered in the AWS Cloud. This infrastructure is composed of the hardware, software, networking, and facilities that run the AWS Cloud services.
	0.	The customer is responsible for the encryption of data at rest and data in transit. The customer should also ensure that the network is configured for security and that security credentials and logins are managed safely. Additionally, the customer is responsible for the configuration of security groups and the configuration of the operating system that run on compute instances that they launch (including updates and security patches).
AWS responsibility security of the cloud
	0.	AWS is responsible for security of the cloud. But what does that mean?
	0.	Under the AWS shared responsibility model, AWS operates, manages, and controls the components from the bare metal host operating system and hypervisor virtualization layer down to the physical security of the facilities where the services operate. It means that AWS is responsible for protecting the global infrastructure that runs all the services that are offered in the AWS Cloud. The global infrastructure includes AWS Regions, Availability Zones, and edge locations.
	0.	AWS is responsible for the physical infrastructure that hosts your resources, including:
	•	  Physical security of data centers with controlled, need-based access; located in nondescript facilities, with 24/7 security guards; two-factor authentication; access logging and review; video surveillance; and disk degaussing and destruction.
	•	  Hardware infrastructure, such as servers, storage devices, and other appliances that AWS relies on.
	•	  Software infrastructure, which hosts operating systems, service applications, and virtualization software.
	•	  Network infrastructure, such as routers, switches, load balancers, firewalls, and cabling. AWS also continuously monitors the network at external boundaries, secures access points, and provides redundant infrastructure with intrusion detection.
	0.	Protecting this infrastructure is the top priority for AWS. While you cannot visit AWS data centers or offices to see this protection firsthand, Amazon provides several reports from third-party auditors who have verified our compliance with a variety of computer security standards and regulations.
Customer responsibility security in the cloud
	0.	While the cloud infrastructure is secured and maintained by AWS, customers are responsible for security of everything they put in the cloud.
	0.	The customer is responsible for what is implemented by using AWS services and for the applications that are connected to AWS. The security steps that you must take depend on the services that you use and the complexity of your system.
	0.	Customer responsibilities include selecting and securing any instance operating systems, securing the applications that are launched on AWS resources, security group configurations, firewall configurations, network configurations, and secure account management.
	0.	When customers use AWS services, they maintain complete control over their content.
	0.	Customers are responsible for managing critical content security requirements, including:
	•	What content they choose to store on AWS
	•	Which AWS services are used with the content
	•	In what country that content is storedThe format and structure of that content and whether it is masked, anonymized, or encrypted
	•	Who has access to that content and how those access rights are granted, managed, and revoked
	0.	Customers retain control of what security they choose to implement to protect their own data, environment, applications, AM configurations, and operating systems.
Service  characteristics and security responsibilities (1 of 2)
	0.	Infrastructure as a service (laS) refers to services that provide basic building blocks for cloud IT, typically including access to configure networking, computers (virtual or on dedicated hardware), and data storage space. Cloud services that can be characterized as laS provide the customer with the highest level of flexibility and management control over IT resources. laaS services are most similar to existing on-premises computing resources that many IT departments are familiar with today.
	0.	AWS services-such as Amazon EC2-can be categorized as laaS and thus require the customer to perform all necessary security configuration and management tasks. Customers who deploy EC2 instances are responsible for managing the guest operating system (including updates and security patches), any application software that is installed on the instances, and the configuration of the security groups that were provided by AWS.
	0.	Platform as a service (Paas) refers to services that remove the need for the customer to manage the underlying infrastructure (hardware, operating systems, etc.). PaaS services enable the customer to focus entirely on deploying and managing applications. Customers don't need to worry about resource procurement, capacity planning, software maintenance, or patching.
	0.	AWS services such as AWS Lambda and Amazon RDS can be categorized as Paas because AWS operates the infrastructure layer, the operating system, and platforms. Customers only need to access the endpoints to store and retrieve data. With PaaS services, customers are responsible for managing their data, classifying their assets, and applying the appropriate permissions. However, these service act more like managed services, with AWS handling a larger portion of the security requirements. For these services, AWS handles basic security tasks-such as operating system and database patching, firewall configuration, and disaster recovery.
Service characteristics and security responsibilities (2 of 2)
	0.	Software as a service SaaS) refers to services that provide centrally hosted software that is typically accessible via a web browser, mobile app, or application programming interface (API). The licensing model for SaaS offerings is typically subscription or pay as you go. With Saas offerings, customers do not need to manage the infrastructure that supports the service. Some AWS services-such as AWS Trusted Advisor, AWS Shield, and Amazon Chime-could be categorized as SaaS offerings, given their characteristics.
	0.	AWS Trusted Advisor is an online tool that analyzes your AWS environment and provides realtime guidance and recommendations to help you provision your resources by following AWS best practices. The Trusted Advisor service is offered as part of your AWS Support plan. Some of the Trusted Advisor features are free to all accounts, but Business Support and Enterprise Support customers have access to the full set of Trusted Advisor checks and recommendations.
	0.	AWS Shield is a managed distributed denial of service (DDoS) protection service that safeguards applications running on AWS. It provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. AWS Shield Advanced is available to all customers. However, to contact the DoS Response Team, customers must have either Enterprise Support or Business Support from AWS Support.
	0.	Amazon Chime is a communications service that enables you to meet, chat, and place business calls inside and outside your organization, all using a single application. It is a pay-as-you-go communications service with no upfront fees, commitments, or long-term contracts.
Scenario 1 of 2 Answers

Scenario 2 of 2

Section 2 AWS Identity and Access Management

AWS Identity and Access Management (IAM)
	0.	AWS Identity and Access Management (IAM) allows you to control access to compute, storage, database, and application services in the AWS Cloud. IAM can be used to handle authentication, and to specify and enforce authorization policies so that you can specify which users can access which services.
	0.	IAM is a tool that centrally manages access to launching, configuring, managing, and terminating resources in your AWS account. It provides granular control over access to resources, including the ability to specify exactly which API calls the user is authorized to make to each service. Whether you use the AWS Management Console, the AWS CLI, or the AWS software development kits (SDKs), every call to an AWS service is an API call.
	0.	With IAM, you can manage which resources can be accessed by who, and how these resources can be accessed. You can grant different permissions to different people for different resources. For example, you might allow some users full access to Amazon EC2, Amazon S3, Amazon DynamoDB, Amazon Redshift, and other AWS services. However, for other users, you might allow read-only access to only a few S3 buckets. Similarly, you might grant permission to other users to administer only specific EC2 instances. You could also allow a few users to access only the account billing information, but nothing else.
	0.	IAM is a feature of your AWS account, and it is offered at no additional charge.
IAM: Essential Components
	0.	To understand how to use IAM to secure your AWS account, it is important to understand the role and function of each of the four IAM components.
	0.	An IAM user is a person or application that is defined in an AWS account, and that must make API calls to AWS products. Each user must have a unique name with no spaces in the name within the AWS account, and a set of security credentials that is not shared with other users. These credentials are different from the AWS account root user security credentials. Each user is defined in one and only one AWS account.
	0.	An lAM group is a collection of IAM users. You can use IAM groups to simplify specifying and managing permissions for multiple users.
	0.	An lAM policy is a document that defines permissions to determine what users can do in the AWS account. A policy typically grants access to specific resources and specifies what the user can do with those resources. Policies can also explicitly deny access.
	0.	An IAM role is a tool for granting temporary access to specific AWS resources in an AWS account.
Authentication as am IAM user to gain access
	0.	Authentication is a basic computer security concept: a user or system must first prove their identity. Consider how you authenticate yourself when you go to the airport and you want to get through airport security so that you can catch your flight. In this situation, you must present some form of identification to the security official to prove who you are before you can enter a restricted area. A similar concept applies for gaining access to AWS resources in the cloud.
	0.	When you define an IAM user, you select what type of access the user is permitted to use to access AWS resources. You can assign two different types of access to users: programmatic access and AWS Management Console access. You can assign programmatic access only, console access only, or you can assign both types of access.
	0.	If you grant programmatic access, the lAM user will be required to present an access key ID and a secret access key when they make an AWS API call by using the AWS CLI, the AWS SDK, or some other development tool.
	0.	If you grant AWS Management Console access, the IAM user will be required to fill in the fields that appear in the browser login window. The user is prompted to provide either the 12-digit account ID or the corresponding account alias. The user must also enter their IAM user name and password. If multi-factor authentication (MA) is enabled for the user, they will also be prompted for an authentication code.
IAM MFA
	0.	AWS services and resources can be accessed by using the AWS Management Console, the AWS CLI, or through SDKs and APIs. For increased security, we recommend enabling MA.
	0.	With MFA, users and systems must provide an MA token -in addition to the regular sign-in credentials-before they can access AWS services and resources.
	0.	Options for generating the MA authentication token include virtual MFA-compliant applications (such as Google Authenticator or Authy 2-Factor Authentication), U2F security key devices, and hardware MA devices.
Authorization what actions are permitted
	0.	Authorization is the process of determining what permissions a user, service or application should be granted. After a user has been authenticated, they must be authorized to access AWS services.
	0.	By default, lAM users do not have permissions to access any resources or data in an AWS account. Instead, you must explicitly grant permissions to a user, group, or role by creating a policy, which is a document in JavaScript Object Notation (JSON) format. A policy lists permissions that allow or deny access to resources in the AWS account.
IAM Authorization
	0.	To assign permission to a user, group or role, you must create an IAM policy (or find an existing policy in the account). There are no default permissions. All actions in the account are denied to the user by default (implicit deny) unless those actions are explicitly allowed. Any actions that you do not explicitly allow are denied. Any actions that you explicitly deny are always denied.
	0.	The principle of least privilege is an important concept in computer security. It promotes that you grant only the minimal user privileges needed to the user, based on the needs of your users. When you create IAM policies, it is a best practice to follow this security advice of granting least privilege. Determine what users need to be able to do and then craft policies for them that let the users perform only those tasks. Start with a minimum set of permissions and grant additional permissions as necessary. Doing so is more secure than starting with permissions that are too broad and then later trying to lock down the permissions granted.
	0.	Note that the scope of the IAM service configurations is global. The settings are not defined at an AWS Region level. IAM settings apply across all AWS Regions.
IAM Policies
	0.	An IAM policy is a formal statement of permissions that will be granted to an entity. Policies can be attached to any IAM entity. Entities include users, groups, roles, or resources. For example, you can attach a policy to AWS resources that will block all requests that do not come from an approved Internet Protocol (IP) address range. Policies specify what actions are allowed, which resources to allow the actions on, and what the effect will be when the user requests access to the resources.
	0.	The order in which the policies are evaluated has no effect on the outcome of the evaluation. All policies are evaluated, and the result is always that the request is either allowed or denied. When there is a conflict, the most restrictive policy applies.
	0.	There are two types of IAM policies. Identity-based policies are permissions policies that you can attach to a principal (or identity) such as an IAM user, role, or group. These policies control what actions that identity can perform, on which resources, and under what conditions. Identity-based policies can be further categorized as:
	•	Managed policies - Standalone identity-based policies that you can attach to multiple users, groups, and roles in your AWS account
	•	Inline policies - Policies that you create and manage, and that are embedded directly into a single user group or role.
	0.	Resource-based policies are JSON policy documents that you attach to a resource, such as an S3 bucket. These policies control what actions a specified principal can perform on that resource, and under what conditions.

	0.	As mentioned previously, IAM policy documents are written in JSON.
	0.	The example IAM policy grants users access only to the following resources:
	•	The DynamoDB table whose name is represented by table-name.
	•	The AWS account's S3 bucket, whose name is represented by bucket-name and all the objects that it contains.
	0.	The IAM policy also includes an explicit deny ("Effect"."Deny") element. The NotResource element helps to ensure that users cannot use any other DynamoDB or S3 actions or resources except the actions and resources that are specified in the policy-even if permissions have been granted in another policy. An explicit deny statement takes precedence over an allow statement.
Resource based policies
	0.	While identity-based policies are attached to a user, group, or role, resource-based policies are attached to a resource, such as an S3 bucket. These policies specify who can access the resource and what actions they can perform on it.
	0.	Resource-based policies are defined inline only, which means that you define the policy on the resource itself, instead of creating a separate AM policy document that you attach. For example, to create an S3 bucket policy (a type of resource-based policy) on an S3 bucket, navigate to the bucket, click the Permissions tab, click the Bucket Policy button, and define the JSON-formatted policy document there. An Amazon S3 access control list (ACL) is another example of a resource-based policy.
	0.	The diagram shows two different ways that the user MaryMajor could be granted access to objects in the S3 bucket that is named photos. On the left, you see an example of an identity-based policy. An IAM policy that grants access to the S3 bucket is attached to the MaryMajor user. On the right, you see an example of a resource-based policy. The S3 bucket policy for the photos bucket specifies that the user MaryMajor is allowed to list and read the objects in the bucket.
	0.	Note that you could define a deny statement in a bucket policy to restrict access to specific IAM users, even if the users are granted access in a separate identity-based policy. An explicit deny statement will always take precedence over any allow statement.
IAM Permisions
	0.	IAM policies enable you to fine-tune privileges that are granted to IAM users, groups, and roles.
	0.	When IAM determines whether a permission is allowed, IAM first checks for the existence of any applicable explicit denial policy. If no explicit denial exists, it then checks for any applicable explicit allow policy. If neither an explicit deny nor an explicit allow policy exists, AM reverts to the default, which is to deny access. This process is referred to as an implicit deny. The user will be permitted to take the action only if the requested action is not explicitly denied and is explicitly allowed.
	0.	It can be difficult to figure out whether access to a resource will be granted to an IAM entity when you develop AM policies. The IAM Policy Simulator at https://docs.aws.amazon.com/IAM/latest/UserGuide//access policies testing-policies.htmlisa useful tool for testing and troubleshooting IAM policies.
IAM groups
	0.	An IAM group is a collection of IAM users. IAM groups offer a convenient way to specify permissions for a collection of users, which can make it easier to manage the permissions for those users.
	0.	For example, you could create an IAM group that is called Developers and attach an IAM policy or multiple IAM policies to the Developers group that grant the AWS resource access permissions that developers typically need. Any user that you then add to the Developer group will automatically have the permissions that are assigned to the group. In such a case, you do not need to attach the lAM policy or IAM policies directly to the user. If a new user joins your organization and should be granted developer privileges, you can simply add that user to the Developers group. Similarly, if a person changes jobs in your organization, instead of editing that user's permissions, simply remove the user from the group.
	0.	Important characteristics of AM groups:
	•	A group can contain many users, and a user can belong to multiple groups.
	•	Groups cannot be nested. A group can contain only users, and a group cannot contain other groups.
	•	There is no default group that automatically includes all users in the AWS account. If you want to have a group with all account users in it, you need to create the group and add each new user to it.
IAM roles
	0.	An IAM role is an IAM identity you can create in your account that has specific permissions. An AM role is similar to an IAM user because it is also an AWS identity that you can attach permissions policies to, and those permissions determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have standard long-term credentials such as a password or access keys associated with it. Instead, when you assume a role, the role provides you with temporary security credentials for your role session.
	0.	You can use roles to delegate access to users, applications, or services that do not normally have access to your AWS resources. For example, you might want to grant users in your AWS account access to resources they don't usually have, or grant users in one AWS account access to resources in another account. Or you might want to allow a mobile app to use AWS resources, but you do not want to embed AWS keys within the app (where the keys can be difficult to rotate and where users can potentially extract them and misuse them). Also, sometimes you may want to grant AWS access to users who already have identities that are defined outside of AWS, such as in your corporate directory. Or, you might want to grant access to your account to third parties so that they can perform an audit on your resources.
	0.	For all of these example use cases, IAM roles are an essential component to implementing the cloud deployment.
Example use of an IAM role

	0.	In the diagram, a developer runs an application on an EC2 instance that requires access to the S3 bucket that is named photos. An administrator creates the IAM role and attaches the role to the EC2 instance. The role includes a permissions policy that grants read-only access to the specified S3 bucket. It also includes a trust policy that allows the EC2 instance to assume the role and retrieve the temporary credentials. When the application runs on the instance, it can use the role's temporary credentials to access the photos bucket. The administrator does not need to grant the application developer permission to access the photos bucket, and the developer never needs to share or manage credentials.
	0.	To learn more details about this example, see Using an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances at httos://docs.aws.amazon.com/IAM/latest/UserGuide/id roles use switch-role-ec2.html.

Section 3 Securing a new AWS account

AWS account root user access versus IAM access
	0.	When you first create an AWS account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and it is accessed by signing into the AWS Management Console with the email address and password that you used to create the account. AWS account root users have (and retain) full access to all resources in the account. Therefore, AWS strongly recommends that you do not use account root user credentials for day-to-day interactions with the account.
	0.	Instead, AWS recommends that you use IAM to create additional users and assign permissions to these users, following the principle of least privilege. For example, if you require administrator-level permissions, you can create an AM user, grant that user full access, and then use those credentials to interact with the account. Later, if you need to revoke or modify your permissions, you can delete or modify any policies that are associated with that IAM user.
	0.	Additionally, if you have multiple users that require access to the account, you can create unique credentials for each user and define which user will have access to which resources. For example, you can create IAM users with read-only access to resources in your AWS account and distribute those credentials to users that require read access. You should avoid sharing the same credentials with multiple users.
	0.	While the account root user should not be used for routine tasks, there are a few tasks that can only be accomplished by logging in as the account root user. A full list of these tasks is detailed on the Tasks that require root user credentials AWS documentation page at https://docs.aws.amazon.com/general/latest/gr/root-vs-iam.htm|#aws tasks-that-require-root.
Securing a new AWS account account root user
	0.	To stop using the account root user, take the following steps:
	0.	While you are logged into the account root user, create an IAM user for yourself with AWS Management Console access enabled (but do not attach any permissions to the user yet).Save the IAM user access keys if needed.
	0.	Next, create an IAM group, give it a name (such as FullAccess), and attach IAM policies to the group that grant full access to at least a few of the services you will use. Next, add the IAM user to the group.
	0.	Disable and remove your account root user access keys, if they exist.
	0.	Enable a password policy for all users. Copy the IAM users sign-in link from the IAMDashboard page. Then, sign out as the account root user.
	0.	Browse to the IAM users sign-in link that you copied, and sign in to the account by using your new IAM user credentials.
	0.	Store your account root user credentials in a secure place.
	0.	To view detailed instructions for how to set up your first IAM user and IAM group, see Creating Your First IAM Admin User and Group at https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started create-admin-group.html.
Securing a new AWS account MFA
	0.	Another recommended step for securing a new AWS account is to require multi-factor authentication (MFA) for the account root user login and for all other IAM user logins. You can also use MFA to control programmatic access. For details, see Configuring MFA-Protected API Access at https://docs.aws.amazon.com/IAM/latest/UserGuide/id credentials mfa configure-api-require.html.
	0.	You have a few options for retrieving the MA token that is needed to log in when MFA is enabled. Options include virtual MFA-compliant applications (such as Google Authenticator and Authy Authenticator), U2F security key devices, and hardware MFA options that provide a key fob or display card.
Securing a new AWS account: AWS CloudTrail
	0.	AWS CloudTrail is a service that logs all API requests to resources in your account. In this way, it enables operational auditing on your account.
	0.	AWS CloudTrail is enabled on account creation by default on all AWS accounts, and it keeps a record of the last 90 days of account management event activity. You can view and download the last 90 days of your account activity for create, modify, and delete operations of services that are supported by CloudTrail without needing to manually create another trail. See more on the supported services at https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail aws-service-specific-topics.html.
	0.	To enable CloudTrail log retention beyond the last 90 days and to enable alerting whenever specified events occur, create a new trail (which is described at a high level on the slide). For detailed step-by-step instructions about how to create a trail in AWS CloudTrail, see creating a trail in the AWS documentation at https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html.
Securing a new AWS account Billing report
	0.	An additional recommended step for securing a new AWS account is to enable billing reports, such as the AWS Cost and Usage Report. Billing reports provide information about your use of AWS resources and estimated costs for that use. AWS delivers the reports to an Amazon S3 bucket that you specify and AWS updates the reports at least once per day.
	0.	The AWS Cost and Usage Report tracks usage in the AWS account and provides estimated charges, either by the hour or by the day.
	0.	For details about how to create an AWS Cost and Usage Report, see the AWS Documentation at https://docs.aws.amazon.com/cur/latest/usereuide/cur-create.html.

W4

Section 4 Securing accounts

AWS Organizations
	0.	AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. Here, the focus is on the security features that AWS Organizations provides.
	0.	One helpful security feature is that you can group accounts into organizational units (OUs) and attach different access policies to each OU. For example, if you have accounts that should only be allowed to access AWS services that meet certain regulatory requirements, you can put those accounts into one OU. You then can define a policy that blocks OU access to services that do not meet those regulatory requirements, and then attach the policy to the OU.
	0.	Another security feature is that AWS Organizations integrates with and supports IAM. AWS Organizations expands that control to the account level by giving you control over what users and roles in an account or a group of accounts can do. The resulting permissions are the logical intersection of what is allowed by the AWS Organizations policy settings and what permissions are explicitly granted by lAM in the account for that user or role. The user can access only what is allowed by both the AWS Organizations policies and IAM policies.
	0.	Finally, AWS Organizations provides service control policies (SCs) that enable you to specify the maximum permissions that member accounts in the organization can have. In SCs, you can restrict which AWS services, resources, and individual actions the users and roles in each member account can access. These restrictions even override the administrators of member accounts. When AWS Organizations blocks access to a service, resource, or API action, a user or role in that account can't access it, even if an administrator of a member account explicitly grants such permissions.
AWS Organizations: Service control policies
	0.	Here is a closer look at the Service control policies (SCPs) feature of AWS Organizations.
	0.	SCPs offer central control over the maximum available permissions for all accounts in your organization, enabling you to ensure that your accounts stay in your organization's access control guidelines. SCPs are available only in an organization that has all features enabled, including consolidated billing. See more on enabling features at https://docs.aws.amazon.com/organizations/latest/userguide/orgs manage _org_ support-all-features.html. SCs aren't available if your organization has enabled only the consolidated billing features. For instructions about enabling SCs, see Enabling and Disabling a Policy Type on a Root at https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies.html#enable policies on root.
	0.	SCPs are similar to IAM permissions policies and they use almost the same syntax. However, an SCP never grants permissions. Instead, SCs are JSON policies that specify the maximum permissions for an organization or OU. Attaching an SCP to the organization root or an organizational unit (OU) defines a safeguard for the actions that accounts in the organization root or OU can do. However, it is not a substitute for well-managed IAM configurations within each account. You must still attach IAM policies to users and roles in your organization's accounts to actually grant permissions to them. See more on IAM policies at https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html.
AWS Key Management Service (AWS KMS)
	0.	AWS Key Management Service (AWS KMS) is a service that enables you to create and manage encryption keys, and to control the use of encryption across a wide range of AWS services and your applications. AWS KMS is a secure and resilient service that uses hardware security modules (HSMs) that were validated under Federal Information Processing Standards (FIPS) 140-2 (or are in the process of being validated) to protect your keys. AWS KMS also integrates with AWS CloudTrail to provide you with logs of all key usage to help meet your regulatory and compliance needs.
	0.	Customer master keys (CMKs) are used to control access to data encryption keys that encrypt and decrypt your data. You can create new keys when you want, and you can manage who has access to these keys and who can use them. You can also import keys from your own key management infrastructure into AWS KMS.
	0.	AWS KMS integrates with most AWS services, which means that you can use AWS KMS CMKs to control the encryption of the data that you store in these services. To learn more, see AWS Key Management Service features at https://aws.amazon.com/kms/features/.
Amazon Cognito
	0.	Amazon Cognito provides solutions to control access to AWS resources from your application.
	0.	You can define roles and map users to different roles so your application can access only the resources that are authorized for each user.
	0.	Amazon Cognito uses common identity management standards, such as Security Assertion Markup Language (SAML) 2.0. SAML is an open standard for exchanging identity and security information with applications and service providers. Applications and service providers that support SAML enable you to sign in by using your corporate directory credentials, such as your username and password from Microsoft Active Directory. With SAML, you can use single sign-on (SSO) to sign in to all of your SAML-enabled applications by using a single set of credentials.
	0.	Amazon Cognito helps you meet multiple security and compliance requirements, including requirements for highly regulated organizations such as healthcare companies and merchants. Amazon Cognito is eligible for use with the US Health Insurance Portability and Accountability Act (HIPAA - see more on HIPAA at https://aws.amazon.com/compliance/hipaa-compliance/). It can also be used for workloads that are compliant with the Payment Card Industry Data Security Standard (PCI DSS - more on PCI DSS at https://aws.amazon.com/compliance/pci-dss-level-1-faqs/); the American Institute of CPAs (AICPA) Service Organization Control (SOC - more on SOC at https://aws.amazon.com/compliance/soc-faqs/); the International Organization for Standardization (ISO) and International Electrotechnical Commission (IC) standards. More on ISO/IEC 27001 at https://aws.amazon.com/compliance/iso-27001-fags/, ISO/IEC 27017 at https://aws.amazon.com/compliance/iso-27017-faqs/, and ISO/IC 27018 at https://aws.amazon.com/compliance/iso-27018-faqs/; and ISO 9001 at https://aws.amazon.com/compliance/iso-9001-faqs/.
AWS Shield
	0.	AWS Shield is a managed distributed denial of service (DDoS) protection service that safeguards applications that run on AWS. It provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection.
	0.	AWS Shield helps protects your website from all types of DoS attacks, including Infrastructure layer attacks (like User Datagram Protocol-or UDP-floods), state exhaustion attacks (like TCP SYN floods), and application-layer attacks (like HTTP GET or POST floods). For examples, see the AWS WAF Developer Guide at https://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html.
	0.	AWS Shield Standard is automaticall enabled to all AWS customers at no additional cost.
	0.	AWS Shield Advanced is an optional paid service. AWS Shield Advanced provides additional protections against more sophisticated and larger attacks for your applications that run on Amazon EC2, Elastic Load Balancing, Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53. AWS Shield Advanced is available to all customers. However, to contact the DDoS Response Team, customers need to have either Enterprise Support or Business Support from AWS Support.

Section 5: Securing data on AWS

Encryption of data at rest
	0.	Data encryption is an essential tool to use when your objective is to protect digital data. Data encryption takes data that is legible and encodes it so that it is unreadable to anyone who does not have access to the secret key that can be used to decode it. Thus, even if an attacker gains access to your data, they cannot make sense of it.
	0.	Data at rest refers to data that is physically stored on disk or on tape.
	0.	You can create encrypted file systems on AWS so that all your data and metadata is encrypted at rest by using the open standard Advanced Encryption Standard (AES)-256 encryption algorithm. When you use AWS KMS, encryption and decryption are handled automatically and transparently, so that you do not need to modify your applications. If your organization is subject to corporate or regulatory policies that require encryption of data and metadata at rest, AWS recommends enabling encryption on all services that store your data. You can encrypt data stored in any service that is supported by AWS KMS. See How AWS Services use AWS KMS for a list of supported services at https://docs.aws.amazon.com/kms/latest/developerguide/service integration.html.
Encryption of Data in transit
	0.	Data in transit refers to data that is moving across the network. Encryption of data in transit is accomplished by using Transport Layer Security (TLS) 1.2 with an open standard AES-256 cipher. TLS was formerly called Secure Sockets Layer (SSL).
	0.	AWS Certificate Manager is a service that enables you to provision, manage, and deploy SSL or TLS certificates for use with AWS services and your internal connected resources. SSL or TLS certificates are used to secure network communications and establish the identity of websites over the internet, and also resources on private networks. With AWS Certificate Manager, you can request a certificate and then deploy it on AWS resources such as load balancers or CloudFront distributions). AWS Certificate Manager also handles certificate renewals.
	0.	Web traffic that runs over HTTP is not secure. However, traffic that runs over Secure HTTP (HTTPS) is encrypted by using TLS or SSL. HTTPS traffic is protected against eavesdropping and man-in-the-middle attacks because of the bidirectional encryption of the communication.
	0.	AWS services support encryption for data in transit. Two examples of encryption for data in transit are shown. The first example shows an EC2 instance that has mounted an Amazon EFS shared file system. All data traffic between the instance and Amazon EFS is encrypted by using TLS or SSL. For further details about this configuration, see Encryption of EFS Data in Transit at https://docs.aws.amazon.com/whitepapers/latest/efs-encrypted-file-systems/encryption-of-data-in-transit.html.
	0.	The second example shows the use of AWS Storage Gateway, a hybrid cloud storage service that provides on-premises access to AWS Cloud storage. In this example, the storage gateway is connected across the internet to Amazon S3, and the connection encrypts the data in transit.
Securing Amazon S3 buckets and objects
	0.	By default, all Amazon S3 buckets are private and can be accessed only by users who are explicitly granted access. It is essential to manage and control access to Amazon S3 data. AWS provides many tools and options for controlling access to your S3 buckets or objects, including:
	•	Using Amazon S3 Block Public Access. These settings override any other policies or object permissions. Enable Block Public Access for all buckets that you don't want to be publicly accessible. This feature provides a straightforward method for avoiding unintended exposure of Amazon S3 data.
	•	Writing IAM policies that specify the users or roles that can access specific buckets and objects. This method was discussed in detail earlier in this module.
	•	Writing bucket policies that define access to specific buckets or objects. This option is typically used when the user or system cannot authenticate by using IAM. Bucket policies can be configured to grant access across AWS accounts or to grant public or anonymous access to Amazon S3 data. If bucket policies are used, they should be written carefully and tested fully. You can specify a deny statement in a bucket policy to restrict access. Access will be restricted even if the users have permissions that are granted in an identity-based policy that is attached to the users.
	•	Setting access control lists (ACLs) on your buckets and objects. ACLs are less commonly used (ACLs predate IAM). If you do use ACLs, do not set access that is too open or permissive.
	•	AWS Trusted Advisor provides a bucket permission check feature that is a useful tool for discovering if any of the buckets in your account have permissions that grant global access.
AWS compliance programs
	0.	AWS engages with external certifying bodies and independent auditors to provide customers with information about the policies, processes, and controls that are established and operated by AWS.
	0.	A full Listing of AWS Compliance Programs is available at https://aws.amazon.com/compliance/programs/. Also, for details about which AWS services are in scope of AWS assurance programs, see AWS Services in Scope by Compliance Program at https://aws.amazon.com/compliance/services-in-scope/.
	0.	As an example of a certification for which you can use AWS services to meet your compliance goals, consider the ISO/IC 27001:2013 certification. It specifies the requirements for establishing, implementing, maintaining, and continually improving an Information Security Management System. The basis of this certification is the development and implementation of a rigorous security program, which includes the development and implementation of an Information Security Management System. The Information Security Management System defines how AWS perpetually manages security in a holistic, comprehensive manner.
	0.	AWS also provides security features and legal agreements that are designed to help support customers with common regulations and laws. One example is the Health Insurance Portability and Accountability Act (HIPAA) regulation. Another example, the European Union (EU) General Data Protection Regulation (GDP) protects European Union data subjects' fundamental right to privacy and the protection of personal data. It introduces robust requirements that will raise and harmonize standards for data protection, security, and compliance. The GDPR Center at https://aws.amazon.com/compliance/gdpr-center/ contains many resources to help customers meet their compliance requirements with this regulation.
AWS Config
	0.	AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations, and it enables you to automate the evaluation of recorded configurations against desired configurations. With AWS Config, you can review changes in configurations and relationships between AWS resources, review detailed resource configuration histories, and determine your overall compliance against the configurations that are specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting.
	0.	As you can see in the AWS Config Dashboard screen capture shown here, AWS Config keeps an inventory listing of all resources that exist in the account, and it then checks for configuration rule compliance and resource compliance. Resources that are found to be noncompliant are flagged, which alerts you to the configuration issues that should be addressed within the account.
	0.	AWS Config is a Regional service. To track resources across Regions, enable it in every Region that you use. AWS Config offers an aggregator feature that can show an aggregated view of resources across multiple Regions and even multiple accounts.
AWS Artifact
	0.	AWS Artifact provides on-demand downloads of AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use. You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls. AWS Artifact provides documents about AWS only. AWS customers are responsible for developing or obtaining documents that demonstrate the security and compliance of their companies.
	0.	You can also use AWS Artifact to review, accept, and track the status of AWS agreements such as the Business Associate Agreement (BAA). A BAA typically is required for companies that are subject to HIPAA to ensure that protected health information (PHI) is appropriately safeguarded. With AWS Artifact, you can accept agreements with AWS and designate AWS accounts that can legally process restricted information. You can accept an agreement on behalf of multiple accounts. To accept agreements for multiple accounts, use AWS Organizations to create an organization. To learn more, see Managing agreements in AWS Artifact at https://docs.aws.amazon.com/artifact/latest/ug/managing-agreements.html.
Quiz
	0.	In the shared responsibility model, AWS is responsible for providing security of the cloud
	0.	Encryption of data at rest and data in transit and security configurations are examples of security in the cloud
	0.	Maintaining physical hardware is the responsibility of AWS under the shared responsibility model
	0.	When creating an IAM policy, a user can be granted AWS management console access and programatic access
	0.	When creating an IAM policy, a user can be granted AWS Management Console access and programatic access 
	0.	Managing access to AWS resources and defining fine-grained access rights are best practices when securing accounts with AWS IAM
	0.	Changing the AWS support plan can only be done by the AWS account root user. The other tasks are done with IAM 
	0.	After initial login, AWS recommends deleting the access keys of the AWS account root user as the best practice 
	0.	To add an additional layer of login security to a user’s AWS Management Console, enable multi-factor authentication
	0.	AWS Key management service (AWS KMS) is a service that allows you to create and manage encryption across a wide range of AWS services and in your application 
Module 5 Networking and content delivery

Section 1 Networking basics
	0.	In this section, you will review a few basic networking concepts that provide the necessary foundation to your understanding of the AWS networking service, Amazon Virtual Private Cloud (Amazon VPC).
Networks
	0.	A computer network is two or more client machines that are connected together to share resources. A network can be logically partitioned into subnets. Networking requires a networking device (such as a router or switch) to connect all the clients together and enable communication between them.
IP addresses
	0.	Each client machine in a network has a unique Internet Protocol (IP) address that identifies it. An IP address is a numerical label in decimal format. Machines convert that decimal number to a binary format.
	0.	In this example, the IP address is 192.0.2.0. Each of the four dot (.) -separated numbers of the IP address represents 8 bits in octal number format. That means each of the four numbers can be anything from 0 to 255. The combined total of the four numbers for an IP address is 32 bits in binary format.

IPv4 and IPv6 addresses
	0.	A 32-bit IP address is called an IPv4 address.
	0.	IPv6 addresses, which are 128 bits, are also available. IPv6 addresses can accommodate more user devices.
	0.	An IPv6 address is composed of eight groups of four letters and numbers that are separated by colons . In this example, the IPv6 address is 2600:1f18:22ba:8c00:ba86:05e:a5ba:00FF. Each of the eight colon-separated groups of the IPv6 address represents 16 bits in hexadecimal number format. That means each of the eight groups can be anything from 0 to FFFF. The combined total of the eight groups for an IPv6 address is 128 bits in binary format.

Classless Inter-Domain routing (CIDR)

	0.	A common method to describe networks is Classless Inter-Domain Routing (CID). The CIDR address is expressed as follows:
	⁃	An IP address (which is the first address of the network)
	⁃	Next, a slash character (/)
	⁃	Finally, a number that tells you how many bits of the routing prefix must be fixed or allocated for the network identifier
	0.	The bits that are not fixed are allowed to change. CID is a way to express a group of IP addresses that are consecutive to each other.
	0.	In this example, the CIDR address is 192.0.2.0/24. The last number (24) tells you that the first 24 bits must be fixed. The last 8 bits are flexible, which means that 28 (or 256) IP addresses are available for the network, which range from 192.0.2.0 to 192.0.2.255. The fourth decimal digit is allowed to change from 0 to 255.
	0.	If the CIDR was 192.0.2.0/16, the last number (16) tells you that the first 16 bits must be fixed. The last 16 bits are flexible, which means that 216 (or 65,536) IP addresses are available for the network, ranging from 192.0.0.0 to 192.0.255.255. The third and fourth decimal digits can each change from 0 to 255.
	0.	There are two special cases:
	⁃	Fixed IP addresses, in which every bit is fixed, represent a single IP address (for example, 192.0.2.0/32). This type of address is helpful when you want to set up a firewall rule and give access to a specific host.
	⁃	The internet, in which every bit is flexible, is represented as 0.0.0.0/0
Open system interconnection (OSI) model
	0.	The Open Systems Interconnection (OSI) model is a conceptual model that is used to explain how data travels over a network. It consists of seven layers and shows the common protocols and addresses that are used to send data at each layer. For example, hubs and switches work at layer 2 (the data link layer). Routers work at layer 3 (the network layer). The OS model can also be used to understand how communication takes place in a virtual private cloud (VPC), which you will learn about in the next section.

Section 2 Amazon VPC
	0.	Many of the concepts of an on-premises network apply to a cloud-based network, but much of the complexity of setting up a network has been abstracted without sacrificing control, security, and usability. In this section, you learn about Amazon VPC and the fundamental components of a VPC.
Amazon VPC
	0.	Amazon Virtual Private Cloud (Amazon VPC) is a service that lets you provision a logically isolated section of the AWS Cloud (called a virtual private cloud, or VPC) where you can launch your AWS resources.
	0.	Amazon VPC gives you control over your virtual networking resources, including the selection of your own IP address range, the creation of subnets, and the configuration of route tables and network gateways. You can use both IPv4 and IPv6 in your VPC for secure access to resources and applications.
	0.	You can also customize the network configuration for your VPC. For example, you can create a public subnet for your web servers that can access the public internet. You can place your backend systems (such as databases or application servers) in a private subnet with no public internet access.
	0.	Finally, you can use multiple layers of security, including security groups and network access control lists (network ACLs), to help control access to Amazon Elastic Compute Cloud (Amazon EC2) instances in each subnet.
VPCs and subnets
	0.	Amazon VPC enables you to provision virtual private clouds (VPCs). A VPC is a virtual network that is logically isolated from other virtual networks in the AS Cloud. A VPC is dedicated to your account. VPCs belong to a single AWS Region and can span multiple Availability Zones.
	0.	After you create a VPC, you can divide it into one or more subnets. A subnet is a range of IP addresses in a VPC. Subnets belong to a single Availability Zone. You can create subnets in different Availability Zones for high availability. Subnets are generally classified as public or private. Public subnets have direct access to the internet, but private subnets do not.
IP addressing
	0.	IP addresses enable resources in your VPC to communicate with each other and with resources over the internet. When you create a VPC, you assign an IPV4 CIDR block (a range of private IPV4 addresses) to it. After you create a VPC, you cannot change the address range, so it is important that you choose it carefully. The IPv4 CIDR block might be as large as /16 (which is 216, or 65,536 addresses) or as small as /28 (which is 24, or 16 addresses).
	0.	You can optionally associate an IPV6 CID block with your VPC and subnets, and assign IPV6 addresses from that block to the resources in your VPC. IPV6 CIDR blocks have a different block size limit.
	0.	The CIDR block of a subnet can be the same as the CIDR block for a VPC. In this case, the VPC and the subnet are the same size (a single subnet in the VPC). Also, the CID block of a subnet can be a subset of the CID block for the VPC. This structure enables the definition of multiple subnets. If you create more than one subnet in a VPC, the CID blocks of the subnets cannot overlap. You cannot have duplicate IP addresses in the same VPC.
	0.	To learn more about IP addressing in a VPC, see the AWS Documentation at https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html.
Reserved IP address
	0.	When you create a subnet, it requires its own CID block. For each CID block that you specify, AWS reserves five IP addresses within that block, and these addresses are not available for use. AWS reserves these IP addresses for:
	•	Network address
	•	VPC local router (internal communications)
	•	Domain Name System (DNS) resolution
	•	Future use
	•	Network broadcast address
	0.	For example, suppose that you create a subnet with an IPv4 CID block of 10.0.0.0/24 (which has 256 total IP addresses). The subnet has 256 IP addresses, but only 251 are available because five are reserved.
Public IP address types
	0.	When you create a VPC, every instance in that VPC gets a private IP address automatically. You can also request a public IP address to be assigned when you create the instance by modifying the subnet's auto-assign public IP address properties.
	0.	An Elastic IP address is a static and public IPv4 address that is designed for dynamic cloud computing. You can associate an Elastic IP address with any instance or network interface for any VPC in your account. With an Elastic IP address, you can mask the failure of an instance by rapidly remapping the address to another instance in your VPC. Associating the Elastic IP address with the network interface has an advantage over associating it directly with the instance. You can move all of the attributes of the network interface from one instance to another in a single step.
	0.	Additional costs might apply when you use Elastic IP addresses, so it is important to release them when you no longer need them.
	0.	To learn more about Elastic IP addresses, see Elastic IP Addresses in the AWS Documentation at https://docs.aws.amazon.com/vpc/latest/userguide/vpc-eips.html.
Elastic network interface
	0.	An elastic network interface is a virtual network interface that vou can attach or detach from an instance in a VPC. A network interface's attributes follow it when it is reattached to another instance. When you move a network interface from one instance to another, network traffic is redirected to the new instance.
	0.	Each instance in your VPC has a default network interface (the primary network interface) that is assigned a private IPv4 address from the IPv4 address range of your VPC. You cannot detach a primary network interface from an instance. You can create and attach an additional network interface to any instance in your VPC. The number of network interfaces you can attach varies by instance type.
	0.	For more information about Elastic Network Interfaces, see the AWS Documentation at https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html.
Route tables and routes
	0.	A route table contains a set of rules (called routes) that directs network traffic from your subnet. Each route specifies a destination and a target. The destination is the destination CID block where you want traffic from your subnet to go. The target is the target that the destination traffic is sent through. By default, every route table that you create contains a local route for communication in the VPC. You can customize route tables by adding routes. You cannot delete the local route entry that is used for internal communications.
	0.	Each subnet in your VPC must be associated with a route table. The main route table is the route table is automatically assigned to your VPC. It controls the routing for all subnets that are not explicitly associated with any other route table. A subnet can be associated with only one route table at a time, but you can associate multiple subnets with the same route table.
	0.	To learn more about route tables, see the AWS Documentation at https://docs.aws.amazon.com/vpc/latest/userguide/VPC Route Tables.html.
Section 3: VPC Networking

Internet gateway
	0.	An internet gateway is a scalable, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation for instances that were assigned public IPv4 addresses.
	0.	To make a subnet public, you attach an internet gateway to your VPC and add a route to the route table to send non-local traffic through the internet gateway to the internet (0.0.0.0/0).
	0.	For more information about internet gateways, see Internet Gateways in the AWS Documentation at https://docs.aws.amazon.com/vpc/latest/userguide/VPC Internet Gateway.html.
Network address translation (NAT) gateway
	0.	A network address translation (NAT) gateway enables instances in a private subnet to connect to the internet or other AWS services, but prevents the internet from initiating a connection with those instances.
	0.	To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. You must also specify an Elastic IP address to associate with the NAT gateway when you create it. After you create a NAT gateway, you must update the route table that is associated with one or more of your private subnets to point internet-bound traffic to the NAT gateway. Thus, instances in vour private subnets can communicate with the internet.
	0.	You can also use a NAT instance in a public subnet in your VPC instead of a NAT gateway. However, a NAT gateway is a managed NAT service that provides better availability, higher bandwidth, and less administrative effort. For common use cases, AWS recommends that you use a NAT gateway instead of a NAT instance.
	0.	See the AWS Documentation for more information about
	•	NAT gateways at https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html.
	•	NAT instances at https://docs.aws.amazon.com/vpc/latest/userguide/VPC NAT Instance.html.
	•	Differences between NAT gateways and NAT instances athttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html.
VPC Sharing
	0.	VPC sharing enables customers to share subnets with other AWS accounts in the same organization in AWS Organizations. VPC sharing enables multiple AWS accounts to create their application resources-such as Amazon EC2 instances, Amazon Relational Database Service (Amazon RDS) databases, Amazon Redshift clusters, and AWS Lambda functions - into shared, centrally managed VPCs. In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization in AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets that are shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner.
	0.	VPC sharing offers several benefits:
	•	Separation of duties - Centrally controlled VPC structure, routing, IP address allocation
	•	Ownership - Application owners continue to own resources, accounts, and security groups
	•	Security groups - VPC sharing participants can reference the security group IDs of each other
	•	Efficiencies - Higher density in subnets, efficient use of VPNs and AWS Direct Connect
	•	No hard limits - Hard limits can be avoided -for example, 50 virtual interfaces per AWS Direct Connect connection through simplified network architecture
	•	Optimized costs - Costs can be optimized through the reuse of NAT gateways, VPC interface endpoints, and intra-Availability Zone traffic
	0.	VP sharing enables you to decouple accounts and networks. You have fewer, larger, centrally managed VPCs. Highly interconnected applications automatically benefit from this approach.
VPC Peering
	0.	A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them privately. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.
	0.	When you set up the peering connection, you create rules in your route table to allow the VPCs to communicate with each other through the peering resource. For example, suppose that you have two VPCs. In the route table for VPC A, you set the destination to be the IP address of VPC B and the target to be the peering resource ID. In the route table for VPC B, you set the destination to be the IP address of VPC A and the target to be the peering resource ID.
	0.	VPC peering has some restrictions:
	•	IP address ranges cannot overlap.
	•	Transitive peering is not supported. For example, suppose that you have three VPCs: A, B, and C. VPC A is connected to VPC B, and VPC A is connected to VPC C. However, VPC B is not connected to VPC C implicitly. To connect VPC B to VPC C, you must explicitly establish that connectivity.
	•	You can only have one peering resource between the same two VPCs.
	0.	For more information about VPC peering, see VPC Peering in the AWS Documentation at https://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html.
AWS Site-to-site VPN
	0.	By default, instances that you launch into a VPC cannot communicate with a remote network. To connect your VPC to your remote network (that is, create a virtual private network or VPN connection), you:
	0.	 Create a new virtual gateway device (called a virtual private network (VPN) gateway) and attach it to your VPC.
	0.	 Define the configuration of the VPN device or the customer gateway. The customer gateway is not a device but an AWS resource that provides information to AWS about your VPN device.
	0.	 Create a custom route table to point corporate data center-bound traffic to the VPN gateway.You also must update security group rules. (You will learn about security groups in the next section.)
	0.	 Establish an AWS Site-to-Site VPN (Site-to-Site VPN) connection to link the two systems together.
	0.	 Configure routing to pass traffic through the connection.
	0.	For more information about AWS Site-to-Site VPN and other VPN connectivity options, see VPN Connections in the AWS Documentation at https://docs.aws.amazon.com/vpc/latest/userguide/vpn-connections.html.
AWS Direct Connect
	0.	One of the challenges of network communication is network performance. Performance can be negatively affected if your data center is located far away from your AWS Region. For such situations, AWS offers AWS Direct Connect, or DX. AWS Direct Connect enables you to establish a dedicated, private network connection between your network and one of the DX locations. This private connection can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections. DX uses open standard 802.1q virtual local area networks (VLANs).
	0.	For more information about DX, see the AWS Direct Connect product page at https://aws.amazon.com/directconnect/.
VPC endpoints
	0.	A VPC endpoint is a virtual device that enables you to privately connect your VPC to supported
	0.	AWS services and VPC endpoint services that are powered by AWS PrivateLink. Connection to these services does not require an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.
	0.	There are two types of VPC endpoints:
	•	An interface VPC endpoint (interface endpoint) enables you to connect to services that are powered by AWS PrivateLink. These services include some AWS services, services that are hosted by other AWS customers and AWS Partner Network (APN) Partners in their own VPCs (referred to as endpoint services), and supported AWS Marketplace APN Partner services. The owner of the service is the service provider, and you as the principal who creates the interface endpoint -are the service consumer. You are charged for creating and using an interface endpoint to a service. Hourly usage rates and data processing rates apply. See the AWS Documentation for a list of supported interface endpoints and for more information about the example shown here at https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html.
	•	Gateway endpoints: The use of gateway endpoints incurs no additional charge. Standard charges for data transfer and resource usage apply.
	0.	For more information about VPC endpoints, see VPC Endpoints in the AWS Documentation at
https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html.
AWS transit gateway
	0.	You can configure your VPCs in several ways, and take advantage of numerous connectivity options and gateways. These options and gateways include AWS Direct Connect (via DX gateways), NAT gateways, internet gateways, VPC peering, etc. It is not uncommon to find AWS customers with hundreds of VPCs distributed across AWS accounts and Regions to serve multiple lines of business, teams, projects, and so forth. Things get more complex when customers start to set up connectivity between their VPCs. All the connectivity options are strictly point-to-point, so the number of VPC-to-VPC connections can grow quickly. As you grow the number of workloads that run on AWS, you must be able to scale your networks across multiple accounts and VPCs to keep up with the growth.
	0.	Though you can use VPC peering to connect pairs of VPCs, managing point-to-point connectivity across many VPCs without the ability to centrally manage the connectivity policies can be operationally costly and difficult. For on-premises connectivity, you must attach your VPN to each individual VPC. This solution can be time-consuming to build and difficult to manage when the number of VPCs grows into the hundreds.
	0.	To solve this problem, you can use AWS Transit Gateway to simplify your networking model. With AWS Transit Gateway, you only need to create and manage a single connection from the central gateway into each VPC, on-premises data center, or remote office across your network. A transit gateway acts as a hub that controls how traffic is routed among all the connected networks, which act like spokes. This hub-and-spoke model significantly simplifies management and reduces operational costs because each network only needs to connect to the transit gateway and not to every other network. Any new VPC is connected to the transit gateway, and is then automatically available to every other network that is connected to the transit gateway. This ease of connectivity makes it easier to scale your network as you grow.

Section 4 VPC security

Security groups (1 of 2)
	0.	A security group acts as a virtual firewall for your instance, and it controls inbound and outbound traffic. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC can be assigned to a different set of security groups.
	0.	At the most basic level, a security group is a way for you to filter traffic to your instances.
Security groups (2 of 2)
	0.	Security groups have rules that control the inbound and outbound traffic. When you create a security group, it has no inbound rules. Therefore, no inbound traffic that originates from another host to your instance is allowed until you add inbound rules to the security group. By default, a security group includes an outbound rule that allows all outbound traffic. You can remove the rule and add outbound rules that allow specific outbound traffic only. If your security group has no outbound rules, no outbound traffic that originates from your instance is allowed.
	0.	Security groups are stateful, which means that state information is kept even after a request is processed. Thus, if you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Responses to allowed inbound traffic are allowed to flow out, regardless of outbound rules.
Custom security group eample

Network access control lists (network ACLs 1 of 2)
	0.	A network access control list (network ACL) is an optional layer of security for your Amazon VPC. It acts as a firewall for controlling traffic in and out of one or more subnets. To add another layer of security to your VPC, you can set up network ACLs with rules that are similar to your security groups.
	0.	Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL. You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.
Network access control lists (network ACLs 2 of 2)
	0.	A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic. The table shows a default network ACL.
	0.	Network ACLs are stateless, which means that no information about a request is maintained after a request is processed.
Custom network ACLs example
	0.	You can create a custom network ACL and associate it with a subnet. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.
	0.	A network ACL contains a numbered list of rules that are evaluated in order, starting with the lowest numbered rule. The purpose is to determine whether traffic is allowed in or out of any subnet that is associated with the network ACL. The highest number that you can use for a rule is 32,766. AWS recommends that you create rules in increments (for example, increments of 10 or 100) so that you can insert new rules where you need them later.
	0.	For more information about network ACLs, see Network ACLs in the AWS Documentation at https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html.
Security groups versus network ACLs
	0.	Here is a summary of the differences between security groups and network ACLs:
Section 5: Amazon Route 53

Amazon Route 53
	0.	Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses a reliable and cost-effective way to route users to internet applications by translating names like www.example.com) into the numeric IP addresses (like 192.0.2.1) that computers use to connect to each other. In addition, Amazon Route 53 is fully compliant with IPv6. See more on Domain Name Systems at https://aws.amazon.com/route53/what-is-dns/.
	0.	Amazon Route 53 effectively connects user requests to infrastructure running in AWS-such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets-and can also be used to route users to infrastructure that is outside of AWS.
	0.	You can use Amazon Route 53 to configure DNS health checks so you that can route traffic to healthy endpoints or independently monitor the health of your application and its endpoints.
	0.	Amazon Route 53 traffic flow helps you manage traffic globally through several routing types, which can be combined with DNS failover to enable various low-latency, fault-tolerant architectures. You can use Amazon Route 53 traffic flow's simple visual editor to manage how your users are routed to your application's endpoints-whether in a single AWS Region or distributed around the globe.
	0.	Amazon Route 53 also offers Domain Name Registration-you can purchase and manage domain names (like example.com), and Amazon Route 53 will automatically configure DNS settings for your domains.
Amazon Route 53 DNS resolution
	0.	Here is the basic pattern that Amazon Route 53 follows when a user initiates a DNS request. The DNS resolver checks with your domain in Route 53, gets the IP address, and returns it to the user.
Amazon Route 53 supported routing
	0.	Amazon Route 53 supports several types of routing policies, which determine how Amazon Route 53 responds to queries:
	•	Simple routing (round robin) - Use for a single resource that performs a given function for your domain (such as a web server that serves content for the example.com website.
	•	Weighted round robin routing - Use to route traffic to multiple resources in proportions that you specify. Enables you to assign weights to resource record sets to specify the frequency with which different responses are served. You might want to use this capability to do A/B testing, which is when you send a small portion of traffic to a server where you made a software change. For instance, suppose you have two record sets that are associated with one DNS name: one with weight 3 and one with weight 1. In this case, 75 percent of the time, Amazon Route 53 will return the record set with weight 3, and 25 percent of the time, Amazon Route 53 will return the record set with weight 1. Weights can be any number between 0 and 255.
	•	Latency routing (LBR) - Use when you have resources in multiple AWS Regions and you want to route traffic to the Region that provides the best latency. Latency routing works by routing your customers to the AWS endpoint (for example, Amazon EC2 instances, Elastic IP addresses, or load balancers) that provides the fastest experience based on actual performance measurements of the different AWS Regions where your application runs.
	•	Geolocation routing - Use when you want to route traffic based on the location of your users. When you use geolocation routing, you can localize your content and present some or all of your website in the language of your users. You can also use geolocation routing to restrict the distribution of content to only the locations where you have distribution rights. Another possible use is for balancing the load across endpoints in a predictable, easy-to-manage way, so that each user location is consistentlv routed to the same endpoint.
	•	Geoproximity routing - Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.
	•	Failover routing (DNS failover) - Use when you want to configure active-passive failover. Amazon Route 53 can help detect an outage of your website and redirect your users to alternate locations where your application is operating properly. When you enable this feature, Amazon Route 53 health-checking agents will monitor each location or endpoint of your application to determine its availability. You can take advantage of this feature to increase the availability of your customer-facing application.
	•	Multivalue answer routing - Use when you want Route 53 to respond to DNS queries with up to eight healthy records that are selected at random. You can configure Amazon Route 53 to return multiple values-such as IP addresses for your web servers-in response to DNS queries. You can specify multiple values for almost any record, but multivalue answer routing also enables you to check the health of each resource so that Route 53 returns only values for healthy resources. It's not a substitute for a load balancer, but the ability to return multiple health-checkable IP addresses is a way to use DNS to improve availability and load balancing.
Use case: multi-region deployment
	0.	Multi-Region deployment is an example use case for Amazon Route 53. With Amazon Route 53, the user is automatically directed to the Elastic Load Balancing load balancer that's closest to the user.
	0.	The benefits of multi-region deployment of Route 53 include:
	•	Latency-based routing to the Region
	•	Load balancing routing to the Availability Zone
Amazon Route 53 DNS failover
	0.	Amazon Route 53 enables you to improve the availability of your applications that run on AWS by:
	•	Configuring backup and failover scenarios for your own applications.
	•	Enabling highly available multi-Region architectures on AWS.
	•	Creating health checks to monitor the health and performance of your web applications, web servers, and other resources. Each health check that you create can monitor one of the following-the health of a specified resource, such as a web server; the status of other health checks; and the status of an Amazon CloudWatch alarm.
DNS failover for a multi-tier web application
	0.	This diagram indicates how DNS failover works in a typical architecture for a multi-tiered web application. Route 53 passes traffic to a load balancer, which then distributes traffic to a fleet of EC2 instances.
	0.	You can do the following tasks with Route 53 to ensure high availability:
	0.	 Create two DNS records for the Canonical Name Record (CAME) www with a routing policy of Failover Routing. The first record is the primary route policy, which points to the load balancer for your web application. The second record is the secondary route policy, which points to your static Amazon S3 website.
	0.	 Use Route 53 health checks to make sure that the primary is running. If it is, all traffic defaults to your web application stack. Failover to the static backup site would be triggered if either the web server goes down (or stops responding), or the database instance goes down.
Section 6: Amazon CloudFront
	0.	Section 6: Amazon CloudFront
	0.	The purpose of networking is to share information between connected resources. So far in this module, you learned about VPC networking with Amazon VPC. You learned about the different options for connecting your VPC to the internet, to remote networks, to other VPCs, and to AWS services.
	0.	Content delivery occurs over networks, too -for example, when you stream a movie from your favorite streaming service. In this final section, you learn about Amazon CloudFront, which is a content delivery network (CDN) service.
Content delivery and network latency
	0.	As explained earlier in this module when you were learning about AWS Direct Connect, one of the challenges of network communication is network performance. When you browse a website or stream a video, your request is routed through many different networks to reach an origin server. The origin server (or origin) stores the original, definitive versions of the objects (webpages, images, and media files. The number of network hops and the distance that the request must travel significantly affect the performance and responsiveness of the website. Further, network latency is different in various geographic locations. For these reasons, a content delivery network might be the solution.
Content Delivery Network (CDN)
	0.	A content delivery network (CDN) is a globally distributed system of caching servers. A CDN caches copies of commonly requested files (static content, such as Hypertext Markup Language, or HTML; Cascading Style Sheets, or CSS; JavaScript; and image files) that are hosted on the application origin server. The CDN delivers a local copy of the requested content from a cache edge or Point of Presence that provides the fastest delivery to the requester.
	0.	CDNs also deliver dynamic content that is unique to the requester and is not cacheable. Having a CDN deliver dynamic content improves application performance and scaling. The CDN establishes and maintains secure connections closer to the requester. If the CDN is on the same network as the origin, routing back to the origin to retrieve dynamic content is accelerated. In addition, content such as form data, images, and text can be ingested and sent back to the origin, thus taking advantage of the low-latency connections and proxy behavior of the PoP.
Amazon CloudFront
	0.	Amazon CloudFront is a fast CD service that securely delivers data, videos, applications, and application programming interfaces (APIs) to customers globally with low latency and high transfer speeds. It also provides a developer-friendly environment. Amazon CloudFront delivers files to users over a global network of edge locations and Regional edge caches. Amazon CloudFront is different from traditional content delivery solutions because it enables you to quickly obtain the benefits of high-performance content delivery without negotiated contracts, high prices, or minimum fees. Like other AWS services, Amazon CloudFront is a self-service offering with pay-as-you-go pricing.
Amazon CloudFront Infrastructure
	0.	Amazon CloudFront delivers content through a worldwide network of data centers that are called edge locations. When a user requests content that you serve with CloudFront, the user is routed to the edge location that provides the lowest latency (or time delay) so that content is delivered with the best possible performance. CloudFront edge locations are designed to serve popular content quickly to your viewers.
	0.	As objects become less popular, individual edge locations might remove those objects to make room for more popular content. For the less popular content, CloudFront has Regional edge caches. Regional edge caches are CloudFront locations that are deployed globally and are close to your viewers. They are located between your origin server and the global edge locations that serve content directly to viewers. A Regional edge cache has a larger cache than an individual edge location, so objects remain in the Regional edge cache longer. More of your content remains closer to your viewers, which reduces the need for CloudFront to go back to your origin server and improves overall performance for viewers.
	0.	For more information about how Amazon CloudFront works, see How CloudFront Delivers Content in the AWS Documentation at
	0.	https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html#HowCloudFrontWorksContentDelivery.
Amazon CloudFront benefits
	0.	Amazon CloudFront provides the following benefits:
	•	Fast and global - Amazon CloudFront is massively scaled and globally distributed. To deliver content to end users with low latency, Amazon CloudFront uses a global network that consists of edge locations and regional caches.
	•	Security at the edge - Amazon CloudFront provides both network-level and application-level protection. Your traffic and applications benefit through various built-in protections, such as AWS Shield Standard, at no additional cost. You can also use configurable features, such as AWS Certificate Manager (ACM), to create and manage custom Secure Sockets Layer (SSL) certificates at no extra cost.
	•	Highly programmable - Amazon CloudFront features can be customized for specific application requirements. It integrates with Lambda@Edge so that you can run custom code across AWS locations worldwide, which enables you to move complex application logic closer to users to improve responsiveness. The CD also supports integrations with other tools and automation interfaces for DevOps. It offers continuous integration and continuous delivery (CI/CD) environments.
	•	Deeply integrated with AWS - Amazon CloudFront is integrated with AWS, with both physical locations that are directly connected to the AWS Global Infrastructure and other AWS services.You can use APIs or the AWS Management Console to programmatically configure all features in the CDN.
	•	Cost-effective - Amazon CloudFront is cost-effective because it has no minimum commitments and charges you only for what you use. Compared to self-hosting, Amazon CloudFront avoids the expense and complexity of operating a network of cache servers in multiple sites across the internet. It eliminates the need to overprovision capacity to serve potential spikes in traffic. Amazon CloudFront also uses techniques like collapsing simultaneous viewer requests at an edge location for the same file into a single request to your origin server. The result is reduced load on your origin servers and reduced need to scale your origin infrastructure, which can result in further cost savings. If you use AWS origins such as Amazon Simple Storage Service Amazon S3) or Elastic Load Balancing, you pay only for storage costs, not for any data transferred between these services and CloudFront.
Amazon CloudFront pricing
	0.	Amazon CloudFront charges are based on actual usage of the service in four areas:
	•	Data transfer out - You are charged for the volume of data that is transferred out from Amazon CloudFront edge locations, measured in GB, to the internet or to your origin (both AWS origins and other origin servers). Data transfer usage is totaled separately for specific geographic regions, and then cost is calculated based on pricing tiers for each area. If you use other AWS services as the origins of your files, you are charged separately for your use of those services, including storage and compute hours.
	•	HTTP(S) requests - You are charged for the number of HTTP(S) requests that are made to Amazon CloudFront for your content.
	•	Invalidation requests - You are charged per path in your invalidation request. A path that is listed in your invalidation request represents the URL or multiple URLs if the path contains a wildcard character) of the object that you want to invalidate from CloudFront cache. You can request up to 1,000 paths each month from Amazon CloudFront at no additional charge.Beyond the first 1,000 paths, you are charged per path that is listed in your invalidation requests.
	•	Dedicated IP custom Secure Sockets Layer (SSL) - You pay $600 per month for each custom SSL certificate that is associated with one or more CloudFront distributions that use the Dedicated IP version of custom SSL certificate support. This monthly fee is prorated by the hour. For example, if your custom SSL certificate was associated with at least one CloudFront distribution for just 24 hours (that is, 1 day) in the month of June, your total charge for using the custom SSL certificate feature in June is (1 day / 30 days) * $600 = $20.
	0.	For the latest pricing information, see the Amazon CloudFront pricing page at https://aws.amazon.com/cloudfront/pricing/.
Quiz
	0.	The smallest size subnet you can have in a VPC is /28
	0.	The maximum size IP address range you can have in a VPC is /16
	0.	If you need to allow resources in a private subnet to access the Internet, a NAT gateway must be present to enable this access.
	0.	Amazon Virtual Private Cloud enables a company to create a virtual network within AWS.
	0.	Private subnets do not have direct access to the internet
	0.	To ensure low-latency delivery, Amazon CloudFront uses AWS edge locations
	0.	A network ACL is an optional security control that can be applied at the subnet layer of a VPC
	0.	When you create a VPC, a route table is created by default. You must manually create subnets and an internet gateway.
	0.	A security group acts as a virtual firewall for your instance to control inbound and outbound traffic
	0.	The subnet has 256 IP addresses but only 251 are available because 5 are reserve

W5

Section 1: Compute services overview

AWS Compute Services
	0.	Amazon Web Services (AWS) offers many compute services. Here is a brief summary of what each compute service offers:
	•	Amazon Elastic Compute Cloud (Amazon EC2) provides resizable virtual machines.
	•	Amazon EC2 Auto Scaling supports application availability by allowing you to define conditions that will automatically launch or terminate EC2 instances.
	•	Amazon Elastic Container Registry Amazon ECR) is used to store and retrieve Docker images.
	•	Amazon Elastic Container Service Amazon ECS) is a container orchestration service that supports Docker.
	•	VMware Cloud on AWS enables you to provision a hybrid cloud without custom hardware.
	•	AWS Elastic Beanstalk provides a simple way to run and manage web applications.
	•	AWS Lambda is a serverless compute solution. You pay only for the compute time that you use.
	•	Amazon Elastic Kubernetes Service (Amazon EKS) enables you to run managed Kubernetes onAWS.
	•	Amazon Lightsail provides a simple-to-use service for building an application or website.
	•	AWS Batch provides a tool for running batch jobs at any scale.
	•	AWS Fargate provides a way to run containers that reduce the need for you to manage servers or clusters.
	•	AWS Outposts provides a way to run select AWS services in your on-premises data center.
	•	AWS Serverless Application Repository provides a way to discover, deploy, and publish serverless applications.
	0.	This module will discuss details of the services that are highlighted on the slide.
Categorizing compute service
	0.	You can think of each AWS compute service as belonging to one of four broad categories: virtual machines (VMs) that provide infrastructure as a service (laaS), serverless, container-based, and platform as a service (Paas).
	0.	Amazon EC2 provides virtual machines, and you can think of it as infrastructure as a service (laaS). laaS services provide flexibility and leave many of the server management responsibilities to you. You choose the operating system, and you also choose the size and resource capabilities of the servers that you launch. For IT professionals who have experience using on-premises computing, virtual machines are a familiar concept. Amazon EC2 was one of the first AWS services, and it remains one of the most popular services.
	0.	AWS Lambda is a zero-administration compute platform. AWS Lambda enables you to run code without provisioning or managing servers. You pay only for the compute time that is consumed.
	0.	This serverless technology concept is relatively new to many IT professionals. However, it is becoming more popular because it supports cloud-native architectures, which enable massive scalability at a lower cost than running servers 24/7 to support the same workloads.
	0.	Container-based services- including Amazon Elastic Container Service, Amazon Elastic Kubernetes Service, AWS Fargate, and Amazon Elastic Container Registry -enable you to run multiple workloads on a single operating system (OS). Containers spin up more quickly than virtual machines, thus offering responsiveness. Container-based solutions continue to grow in popularity.
	0.	Finally, AWS Elastic Beanstalk provides a platform as a service (PaaS). It facilitates the quick deployment of applications that you create by providing all the application services that you need. AWS manages the OS, the application server, and the other infrastructure components so that you can focus on developing your application code
Choosing the optimal compute service
	0.	AWS offers many compute services because different use cases benefit from different compute environments. The optimal compute service or services that you use will depend on your use case.
	0.	Often, the compute architecture that you use is determined by legacy code. However, that does not mean that you cannot evolve the architecture to take advantage of proven cloud -native designs.
	0.	Best practices include:
	•	Evaluate the available compute options
	•	Understand the available compute configuration options
	•	Collect computer-related metrics
	•	Use the available elasticity of resources
	•	Re-evaluate compute needs based on metrics
	0.	Sometimes, a customer will start with one compute solution and decide to change the design based on their analysis of metrics. If you are interested in seeing an example of how a customer modified their choice of compute services for a particular use case, view this Inventory Tracking solution video at https://www.youtube.com/watch?v=zr3KibOi-0Q&feature=youtu.be&did=ta card&trk=ta card.
Section 2 Amazon EC2

Amazon elastic compute cloud (Amazon EC2)
	0.	Running servers on-premises is an expensive undertaking. Hardware must be procured, and this procurement can be based on project plans instead of the reality of how the servers are used. Data centers are expensive to build, staff, and maintain. Organizations also need to permanently provision a sufficient amount of hardware to handle traffic spikes and peak workloads. After traditional on-premises deployments are built, server capacity might be unused and idle for a significant portion of the time that the servers are running, which is wasteful.
	0.	Amazon Elastic Compute Cloud (Amazon EC2) provides virtual machines where you can host the same kinds of applications that you might run on a traditional on-premises server. It provides secure, resizable compute capacity in the cloud. EC2 instances can support a variety of workloads. Common uses for EC2 instances include, but are not limited to:
	•	Application servers
	•	Web servers
	•	Database servers
	•	Game servers
	•	Mail servers
	•	Media servers
	•	Catalog servers
	•	File servers
	•	Computing servers
	•	Proxy servers
Amazon EC2 overview
	0.	The EC2 in Amazon EC2 stands for Elastic Compute Cloud:
	•	Elastic refers to the fact that you can easily increase or decrease the number of servers you run to support an application automatically, and you can also increase or decrease the size of existing servers.
	•	Compute refers to reason why most users run servers in the first place, which is to host running applications or process data -actions that require compute resources, including processing power (CPU) and memory (RAM).
	•	Cloud refers to the fact that the EC2 instances that you run are hosted in the cloud.
	0.	Amazon EC2 provides virtual machines in the cloud and gives you full administrative control over the Windows or Linux operating system that runs on the instance. Most server operating systems are supported, including: Windows 2008, 2012, 2016, and 2019, Red Hat, SuSE, Ubuntu, and Amazon Linux.
	0.	An operating system that runs on a virtual machine is often called a guest operating system to distinguish it from the host operating system. The host operating system is directly installed on any server hardware that hosts one or more virtual machines.
	0.	With Amazon EC2, you can launch any number of instances of any size into any Availability Zone anywhere in the world in a matter of minutes. Instances launch from Amazon Machine Images (AMIs), which are effectively virtual machine templates. AMIs are discussed in more detail later in this module.
	0.	You can control traffic to and from instances by using security groups. Also, because the servers run in the AWS Cloud, you can build solutions that take use multiple AWS services.
Launching an Amazon EC2 instance
	0.	The first time you launch an Amazon EC2 instance, you will likely use the AWS Management Console Launch Instance Wizard. You will have the opportunity to experience using the Launch Wizard in the lab that is in this module.
	0.	The Launch Instance Wizard makes it easy to launch an instance. For example, if you choose to accept all the default settings, you can skip most of the steps that are provided by the wizard and launch an EC2 instance in as few as six clicks. An example of this process is shown in the demonstration at the end of this section.
	0.	However, for most deployments you will want to modify the default settings so that the servers you launch are deployed in a way that matches your specific needs.
	0.	The next series of slides introduce you to the essential choices that you must make when you launch an instance. The slides cover essential concepts that are good to know when you make these choices. These concepts are described to help you understand the options that are available, and the effects of the decisions that you will make.
Select an AMI
	0.	An Amazon Machine Image (AMI) provides information that is required to launch an EC2 instance. You must specify a source AMI when you launch an instance. You can use different AMIs to launch different types of instances. For example, you can choose one AMI to launch an instance that will become a web server and another AMI to deploy an instance that will host an application server. You can also launch multiple instances from a single AMI.
	0.	An AMI includes the following components:
	•	A template for the root volume of the instance. A root volume typically contains an operating system (OS) and everything that was installed in that OS (applications, libraries, etc.). AmazonEC2 copies the template to the root volume of a new EC2 instance, and then starts it.
	•	Launch permissions that control which AWS accounts can use the AMI.
	•	A block device mapping that specifies the volumes to attach to the instance (if any) when it is launched.
	0.	You can choose many AMIs:
	•	Quick Start - AWS offers a number of pre-built AMIs for launching your instances. These AMIs include many Linux and Windows options.
	•	My AMIs - These AMls are AMls that you created.
	•	AWS Marketplace - The AWS Marketplace offers a digital catalog that lists thousands of software solutions. These AMIs can offer specific use cases to help you get started quickly.
	•	Community AMIs - These AMIs are created by people all around the world. These AMls are not checked by AWS, so use them at your own risk. Community AMIs can offer many different solutions to various problems, but use them with care. Avoid using them in any production or corporate environment.
Creating a new AMI: example
	0.	An AMI is created from an EC2 instance. You can import a virtual machine so that it becomes an EC2 instance, and then save the EC2 instance as an AMI. You can then launch an EC2 instance from that AMI. Alternatively, you can start with an existing AMI-such as of the Quick Start AMIs provided by AWS-and create an EC2 instance from it.
	0.	Regardless of which options you chose (step 1), you will have what the diagram refers to as an unmodified instance. From that instance, you might then create a golden instance -that is, a virtual machine that you configured with the specific OS and application settings that you want (step 2)-and then capture that as a new AMI (step 3). When you create an AMI, Amazon EC2 stops the instance, creates a snapshot of its root volume, and finally registers the snapshot as an AMI.
	0.	After an AMI is registered, the AMI can be used to launch new instances in the same AWS Region.
	0.	The new AMI can now be thought of as a new starter AMI. You might want to also copy the AMI to other Regions (step 4), so that EC2 instances can also be launched in those locations.
Select an instance
	0.	After you choose the AMI for launching the instance, you must choose on an instance type.
	0.	Amazon EC2 provides a selection of instance types that optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity. The different instance types give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, which enable you to scale your resources to the requirements of your target workload.
	0.	Instance type categories include general purpose, compute optimized, memory optimized, storage optimized, and accelerated computing instances. Each instance type category offers many instance types to choose from.
EC2 instance type naming and sizes
	0.	When you look at an EC2 instance type, you will see that its name has several parts. For example, consider the T type.
	0.	T is the family name, which is then followed by a number. Here, that number is 3.
	0.	The number is the generation number of that type. So, a t3 instance is the third generation of the T family. In general, instance types that are of a higher generation are more powerful and provide a better value for the price.
	0.	The next part of the name is the size portion of the instance. When you compare sizes, it is important to look at the coefficient portion of the size category.
	0.	For example, a t3.2xlarge has twice the CPU and memory of a t3.large. The t3.xlarge has, in turn, twice the CPU and memory of a t3.large.
	0.	It is also important to note that network bandwidth is also tied to the size of the Amazon EC2 instance. If you will run jobs that will be very network-intensive, you might be required to increase the instance specifications to meet your needs.
Select instance type: based on use case
	0.	Instance types vary in several ways, including: CPU type, CPU or core count, storage type, storage amount, memory amount, and network performance. The chart provides a high-level view of the different instance categories, and which instance type families and generation numbers fit into each category type. Consider a few of the instance types in more detail:
	•	3 instances provide burstable performance general purpose instances that provide a baseline level of CPU performance with the ability to burst above the baseline. Use cases for this type of instance include websites and web applications, development environments, build servers, code repositories, microservices, test and staging environments, and line-of-business applications.
	•	C5 instances are optimized for compute-intensive workloads, and deliver cost-effective high performance at a low price per compute ratio. Use cases include scientific modeling, batch processing, ad serving, highly scalable multiplayer gaming, and video encoding.
	•	R5 instances are optimized for memory-intensive applications. Use cases include high-performance databases, data mining and analysis, in-memory databases, distributed web-scale in-memory caches, applications that perform real-time processing of unstructured big data, Apache Hadoop or Apache Spark clusters, and other enterprise applications.
	0.	To learn more about each instance type, see the Amazon EC2 Instance Types documentation at https://aws.amazon.com/ec2/instance-types/.

Instance types: networking features
	0.	In addition to considering the CPU, RAM, and storage needs of your workloads, it is also important to consider your network bandwidth requirements.
	0.	Each instance type provides a documented network performance level. For example, an a1.medium instance will provide up to 10 Gbps, but a p3d. 24xlarge instance provides up to 100 Gps. Choose an instance type that meets your requirements.
	0.	When you launch multiple new EC2 instances, Amazon EC2 attempts to place the instances so that they are spread out across the underlying hardware by default. It does this to minimize correlated failures. However, if you want to specify specific placement criteria, you can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. For example, you might specify that three instances should all be deployed in the same Availability Zone to ensure lower network latency and higher network throughput between instances. See the Placement Group documentation at https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.htm| for details.
	0.	Many instance types also enable you to configure enhanced networking to get significantly higher packet per second (PPS) performance, lower delay variation in the arrival of packets over the network (network jitter), and lower latencies. See the Elastic Network Adapter (ENA) documentation at https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.htm|for details.
Specify network settings
	0.	 After you have choose an AMI and an instance type, you must specify the network location where the EC2 instance will be deployed. The choice of Region must be made before you start the Launch Instance Wizard. Verify that you are in the correct Region page of the Amazon EC2 console before you choose Launch Instance. When you launch an instance in a default VPC, AWS will assign it a public IP address by default.
	0.	When you launch an instance into a nondefault VPC, the subnet has an attribute that determines whether instances launched into that subnet receive a public IP address from the public IPV4 address pool. By default, AWS will not assign a public IP address to instances that are launched in a nondefault subnet. You can control whether your instance receives a public IP address by either modifying the public IP addressing attribute of your subnet, or by enabling or disabling the public IP addressing feature during launch (which overrides the subnet's public IP addressing attribute).
Attach IAM role (optional)
	0.	It is common to use EC2 instances to run an application that must make secure API calls to other AWS services. To support these use cases, AWS enables you to attach an AWS Identity and Access Management (IAM) role to an EC2 instance. Without this feature, you might be tempted to place AWS credentials on an EC2 instance so an application that runs on that instance to use. However, you should never store AWS credentials on an EC2 instance. It is highly insecure. Instead, attach an IAM role to the EC2 instance. The IAM role then grants permission to make application programming interface (API) requests to the applications that run on the EC2 instance.
	0.	An instance profile is a container for an IAM role. If you use the AWS Management Console to create a role for Amazon EC2, the console automatically creates an instance profile and gives it the same name as the role. When you then use the Amazon EC2 console to launch an instance with an IAM role, you can select a role to associate with the instance. In the console, the list that displays is actually a list of instance profile names.
	0.	In the example, you see that an IAM role is used to grant permissions to an application that runs on an EC2 instance. The application must access a bucket in Amazon S3.
	0.	You can attach an IAM role when you launch the instance, but you can also attach a role to an already running EC2 instance. When you define a role that can be used by an EC2 instance, you define which accounts or AWS services can assume the role. You also define which API actions and resources the application can use after it assumes the role. If you change a role, the change is propagated to all instances that have the role attached to them.
User data script (optional)
	0.	When you create your EC2 instances, you have the option of passing user data to the instance User data can automate the completion of installations and configurations at instance launch. For example, a user data script might patch and update the instance's operating system, fetch and install software license keys, or install additional software.
	0.	In the example user data script, you see a simple three-line Linux Bash shell script. The first line indicates that the script should be run by the Bash shell. The second line invokes the Yellowdog Updater, Modified (YUM) utility, which is commonly used in many Linux distributions -such as Amazon Linux, CentOS, and Red Hat Linux-to retrieve software from an online repository and install it. In line two of the example, that command tells YUM to update all installed packages to the latest versions that are known to the software repository that it is configured to access. Line three of the script indicates that the Wet utility should be installed. Wet is a common utility for downloading files from the web.
	0.	For a Windows instance, the user data script should be written in a format that is compatible with a Command Prompt window (batch commands) or with Windows PowerShell. See the Windows User Data Scripts documentation for details at https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2-windows-user-data.html.
	0.	When the EC2 instance is created, the user data script will run with root privileges during the final phases of the boot process. On Linux instances, it is run by the cloud-init service. On Windows instances, it is run by the EC2Config or EC2Launch utility. By default, user data only runs the first time that the instance starts up. However, if you would like your user data script to run every time the instance is booted, you can create a Multipurpose Internet Mail Extensions (MIME) multipart file user data script (this process is not commonly done). See https://aws.amazon.com/premiumsupport/knowledge-center/execute-user-data-ec2/for more information
Specify storage
	0.	When you launch an EC2 instance, you can configure storage options. For example, you can configure the size of the root volume where the guest operating system is installed. You can also attach additional storage volumes when you launch the instance. Some AMIs are also configured to launch more than one storage volume by default to provide storage that is separate from the root volume.
	0.	For each volume that your instance will have, you can specify the size of the disks, the volume types, and whether the storage will be retained if the instance is terminated. You can also specify if encryption should be used.
Amazon EC2 storage options
	0.	Amazon Elastic Block Store (Amazon EBS) is an easy-to-use, high-performance durable block storage service that is designed to be used with Amazon EC2 for both throughput- and transaction-intensive workloads. With Amazon EBS, you can choose from four different volume types to balance the optimal price and performance. You can change volume types or increase volume size without disrupting your critical applications, so you can have cost-effective storage when you need it.
	0.	Amazon EC2 Instance Store provides ephemeral, or temporary, block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance Store works well when you must temporarily store information that changes frequently, such as buffers, caches, scratch data, and other temporary content. You can also use Instance Store for data that is replicated across a fleet of instances, such as a load balanced pool of web servers. If the instances are stopped -either because of user error or a malfunction-the data on the instance store will be deleted.
	0.	Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic Network File System (NFS) file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications. It grows and shrinks automatically as you add and remove files, which reduces the need to provision and manage capacity to accommodate growth.
	0.	Amazon Simple Storage Service (Amazon S3) is an object storage service that offers scalability, data availability, security, and performance. You can store and protect any amount of data for a variety of use cases, such as websites, mobile apps, backup and restore, archive, enterprise applications, Internet of Things (loT) devices, and big data analytics.
Example storage options


	0.	Here, you see two examples of how storage options could be configured for EC2 instances.
	0.	The Instance 1 example shows that the root volume-which contains the OS and possibly other data-is stored on Amazon EBS. This instance also has two attached volumes. One volume is a 500-GB Amazon EBS storage volume, and the other volume is an Instance Store volume. If this instance was stopped and then started again, the OS would survive and any data that was stored on either the 20-GB Amazon EBS volume or the 500-GB Amazon EBS volume would remain intact. However, any data that was stored on Ephemeral volume 1 would be permanently lost. Instance Store works well for temporarily storing information that changes frequently, such as buffers, caches, scratch data, and other temporary content.
	0.	The Instance 2 example shows that the root volume is on an instance store (Ephemeral volume 2). An instance with an Instance Store root volume cannot be stopped by an Amazon EC2 API call. It can only be terminated. However, it could be stopped from within the instance's OS (for example, by issuing a shutdown command)-or it could stop because of OS or disk failure -which would cause the instance to be terminated. If the instance was terminated, all the data that was stored on Ephemeral volume 2 would be lost, including the OS. You would not be able to start the instance again. Therefore, do not rely on Instance Store for valuable, long-term data. Instead, use more durable data storage, such as Amazon EBS, Amazon EFS, or Amazon S3.
	0.	If an instance reboots (intentionally or unintentionally), data on the instance store root volume does persist.
Add tags
	0.	A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define. Tags enable you to categorize AWS resources, such as EC2 instances, in different ways. For example, you might tag instances by purpose, owner, or environment.
	0.	Tagging is how you can attach metadata to an EC2 instance.
	0.	Tag keys and tag values are case-sensitive. For example, a commonly used tag for EC2 instances is a tag key that is called Name and a tag value that describes the instance, such as My Web Server. The Name tag is exposed by default in the Amazon EC2 console Instances page. However, if you create a key that is called name (with lower-case n), it will not appear in the Name column for the list of instances (though it will still appear in the instance details panel in the Tags tab).
	0.	It is a best practice to develop tagging strategies. Using a consistent set of tag keys makes it easier for you to manage your resources. You can also search and filter the resources based on the tags that you add. See https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf for more information.
Security group settings
	0.	A security group acts as a virtual firewall that controls network traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, the default security group is used.
	0.	You can add rules to each security group. Rules allow traffic to or from its associated instances. You can modify the rules for a security group at any time, and the new rules will be automatically applied to all instances that are associated with the security group. When AWS decides whether to allow traffic to reach an instance, all the rules from all the security groups that are associated with the instance are evaluated. When you launch an instance in a virtual private cloud (VPC), you must either create a new security group or use one that already exists in that VPC. After you launch an instance, you can change its security groups.
	0.	When you define a rule, you can specify the allowable source of the network communication (inbound rules) or destination (outbound rules). The source can be an IP address, an IP address range, another security group, a gateway VPC endpoint, or anywhere (which means that all sources will be allowed). By default, a security group includes an outbound rule that allows all outbound traffic. You can remove the rule and add outbound rules that only allow specific outbound traffic. If your security group has no outbound rules, no outbound traffic that originates from your instance is allowed.
	0.	In the example rule, the rule allows Secure Shell (SSH) traffic over Transmission Control Protocol (TCP) port 22 if the source of the request is My IP. The My IP IP address is calculated by determining what IP address you are currently connected to the AWS Cloud from when you define the rule.
	0.	Network access control lists network ACLs can also be used are firewalls to protect subnets in a VPC.
	0.	For accessibility: Screenshot of the EC2 console screen where you can define a security group rule. It shows a rule with type SSH, protocol TCP, port range 22, source My IP, and a CID block that shows an example My IP address. End of accessibility description.
Identify or create the key pair
	0.	After you specify all the required configurations to launch an EC2 instance, and after you customize any optional EC2 launch wizard configuration settings, you are presented with a Review Instance Launch window. If you then choose Launch, a dialog asks you to choose an existing key pair, proceed without a key pair, or create a new key pair before you can choose Launch Instances and create the EC2 instance.
	0.	Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. The technology uses a public key to encrypt a piece of data, and then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. Public-key cryptography enables you to securely access your instances by using a private key instead of a password.
	0.	When you launch an instance, you specify a key pair. You can specify an existing key pair or a new key pair that you create at launch. If you create a new key pair, download it and save it in a safe location. This opportunity is the only chance you get to save the private key file.
	0.	To connect to a Windows instance, use the private key to obtain the administrator password, and then log in to the EC2 instance's Windows Desktop by using Remote Desktop Protocol (RDP). To establish an SSH connection from a Windows machine to an Amazon EC2 instance, you can use a tool such as PuTTY, which will require the same private key.
	0.	With Linux instances, at boot time, the public key content is placed on the instance. An entry is created in within ~/ .ssh/authorized_keys. To log in to your Linux instance (for example, by using SSH), you must provide the private key when you establish the connection.
With Linux instances, at boot time, the public key content is placed on the instance. An entry is created in within ~/ .ssh/authorized_keys. To log in to your Linux instance (for example, by using SSH), you must provide the private key when you establish the connection.
Amazon EC2 console view of a running EC2 instance
	0.	After you choose Launch Instances and then choose View Instances, you will be presented with a screen that looks similar to the example.
	0.	Many of the settings that you specified during launch are visible in the Description panel.
	0.	Information about the available instance includes IP address and DNS address information, the instance type, the unique instance ID that was assigned to the instance, the AMI ID of the AMI that you used to launch the instance, the VPC ID, the subnet ID, and more.
	0.	Many of these details provide hyperlinks that you can choose to learn more information about the resources that are relevant to the EC2 instance you launched.
Another option: launch an EC2 instance with the AWS command line interface
	0.	In the example AWS CLI command, you see a single command that specifies the minimal information that is needed to launch an instance. The command includes the following information:
	•	aws - Specifies an invocation of the aws command line utility.
	•	ec2 - Specifies an invocation of the ec2 service command.
	•	run-instances - Is the subcommand that is being invoked.
	0.	The rest of the command specifies several parameters, including:
	•	image-id - This parameter is followed by an AMI ID. All AMIs have a unique AMI ID.
	•	count - You can specify more than one.
	•	instance-type - You can specify the instance type to create (for example a c3.large instance
	•	key-name - In the example, assume that MyKey Pair already exists.
	•	security-groups - In this example, assume that MySecurityGroup already exists.
	•	region - AMls exist in an AWS Region, so you must specify the Region where the AWS CLI will find the AMI and launch the EC2 instance.
	0.	The command should successfully create an EC2 instance if:
	•	The command is properly formed
	•	The resources that the command needs already exist
	•	You have sufficient permissions to run the command
	•	You have sufficient capacity in the AWS account
	0.	If the command is successful, the API responds to the command with the instance ID and other relevant data for your application to use in subsequent API requests.
Amazon EC21 instance lifecycle
	0.	Here, you see the lifecycle of an instance. The arrows show actions that you can take and the boxes show the state the instance will enter after that action. An instance can be in one of the following states:
	•	Pending - When an instance is first launched from an AMI, or when you start a stopped instance, it enters the pending state when the instance is booted and deployed to a host computer. The instance type that you specified at launch determines the hardware of the host computer for your instance.
	•	Running - When the instance is fully booted and ready, it exits the pending state and enters the running state. You can connect over the internet to your running instance.
	•	Rebooting - AWS recommends you reboot an instance by using the Amazon EC2 console, AWS CLI, or AWS SDKs instead of invoking a reboot from within the guest operating system (OS). A rebooted instance stays on the same physical host, maintains the same public DNS name and public IP address, and if it has instance store volumes, it retains the data on those volumes.
	•	Shutting down - This state is an intermediary state between running and terminated.
	•	Terminated - A terminated instance remains visible in the Amazon EC2 console for a while before the virtual machine is deleted. However, you can't connect to or recover a terminated instance.
	•	Stopping - Instances that are backed by Amazon EBS can be stopped. They enter the stopping state before they attain the fully stopped state.
	•	Stopped - A stopped instance will not incur the same cost as a running instance. Starting a stopped instance puts it back into the pending state, which moves the instance to a new host machine.

Consider using an elastic IP address
	0.	A public IP address is an IPv4 address that is reachable from the internet. Each instance that receives a public IP address is also given an external DNS hostname. For example, if the public IP address assigned to the instance is 203.0.113.25, then the external DNS hostname might be ec2-203-0-113-25.compute-1.amazonaws.com.
	0.	If you specify that a public IP address should be assigned to your instance, it is assigned from the AWS pool of public IPv4 addresses. The public IP address is not associated with your AWS account. When a public IP address is disassociated from your instance, it is released back into the public IPv4 address pool, and you will not be able to specify that you want to reuse it. AWS releases your instance's public IP address when the instance is stopped or terminated. Your stopped instance receives a new public IP address when it is restarted.
	0.	If you require a persistent public IP address, you might want to associate an Elastic IP address with the instance. To associate an Elastic IP address, you must first allocate a new Elastic IP address in the Region where the instance exists. After the Elastic IP address is allocated, you can associate the Elastic IP address with an EC2 instance.
	0.	By default, all AWS accounts are limited to five (5) Elastic IP addresses per Region because public (IPv4) internet addresses are a scarce public resource. However, this is a soft limit, and you can request a limit increase (which might be approved).
EC2 instance metadata
	0.	Instance metadata is data about your instance. You can view it while you are connected to the instance. To access it in a browser, go to the following URL: http://169.254.169.254/latest/meta-data/. The data can also be read programmatically, such as from a terminal window that has the cURL utility. In the terminal window, run curl http://169.254.169.254/latest/meta-data/ to retrieve it. The IP address 169.254.169.254 is a link-local address and it is valid only from the instance.
	0.	Instance metadata provides much of the same information about the running instance that you can find in the AWS Management Console. For example, you can discover the public IP address, private IP address, public hostname, instance ID, security groups, Region, Availability Zone, and more.
	0.	Any user data that is specified at instance launch can also be accessed at the following URL: http://169.254.169.254/latest/user-data.
	0.	EC2 instance metadata can be used to configure or manage a running instance. For example, you can author a configuration script that accesses the metadata information and uses it to configure applications or OS settings.
Amazon cloudWatch for monitoring
	0.	You can monitor your instances by using Amazon CloudWatch, which collects and processes raw data from Amazon EC2 into readable, near-real-time metrics. These statistics are recorded for a period of 15 months, so you can access historical information and gain a better perspective on how your web application or service is performing.
	0.	By default, Amazon EC2 provides basic monitoring, which sends metric data to CloudWatch in 5-minute periods. To send metric data for your instance to CloudWatch in 1-minute periods, you can enable detailed monitoring on the instance. For more information, see Enable or Disable Detailed Monitoring for Your Instances at https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html.
	0.	The Amazon EC2 console displays a series of graphs based on the raw data from Amazon CloudWatch. Depending on your needs, you might prefer to get data for your instances from Amazon CloudWatch instead of through the graphs in the console. By default, Amazon CloudWatch does not provide RAM metrics for EC2 instances, though that is an option that you can configure if you want to CloudWatch to collect that data.
Section 3: Amazon EC2 cost optimization

Amazon EC2 pricing models
	0.	Amazon offers different pricing models to choose from when you want to run EC2 instances.
	•	Per second billing is only available for On-Demand Instances, Reserved Instances, and Spot Instances that run Amazon Linux or Ubuntu.
	•	On-Demand Instances are eligible for the AWS Free Tier (https://aws.amazon.com/free/). They have the lowest upfront cost and the most flexibility. There are no upfront commitments or long-term contracts. It is a good choice for applications with short-term, spiky, or unpredictable workloads.
	•	Dedicated Hosts are physical servers with instance capacity that is dedicated to your use. They enable you to use your existing per-socket, per-core, or per-VM software licenses, such as for Microsoft Windows or Microsoft SQL Server.
	•	Dedicated Instances are instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. They are physically isolated at the host hardware level from instances that belong to other AWS accounts.
	•	Reserved Instance enable you to reserve computing capacity for 1-year or 3-year term with lower hourly running costs. The discounted usage price is fixed for as long as you own the Reserved Instance. If you expect consistent, heavy use, they can provide substantial savings compared to On-Demand Instances.
	•	Scheduled Reserved Instances enable vou to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified duration, for a 1-year term. You pay for the time that the instances are scheduled, even if you do not use them.
	•	Spot Instances enable you to bid on unused EC2 instances, which can lower your costs. The hourly price for a Spot Instance fluctuates depending on supply and demand. Your Spot Instance runs whenever your bid exceeds the current market price.
Amazon EC2 pricing models: Benefits
	0.	Each Amazon EC2 pricing model provides a different set of benefits.
	0.	On-Demand Instances offer the most flexibility, with no long-term contract and low rates.
	0.	Spot Instances provide large scale at a significantly discounted price.
	0.	Reserved Instances are a good choice if you have predictable or steady-state compute needs (for example, an instance that you know you want to keep running most or all of the time for months or years.
	0.	Dedicated Hosts are a good choice when you have licensing restrictions for the software you want to run on Amazon EC2, or when you have specific compliance or regulatory requirements that preclude you from using the other deployment options.
Amazon EC2 pricing models: Use Cases
	0.	Here is a review of some use cases for the various pricing options.
	0.	On-Demand Instance pricing works well for spiky workloads or if you only need to test or run an application for a short time (for example, during application development or testing). Sometimes, your workloads are unpredictable, and On-Demand Instances are a good choice for these cases.
	0.	Spot Instances are a good choice if your applications can tolerate interruption with a 2-minute warning notification. By default, instances are terminated, but you can configure them to stop or hibernate instead. Common use cases include fault-tolerant applications such as web servers, API backends, and big data processing. Workloads that constantly save data to persistent storage such as Amazon S3) are also good candidates.
	0.	Reserved Instances are a good choice when you have long-term workloads with predictable usage patterns, such as servers that you know you will want to run in a consistent way over many months.
	0.	Dedicated Hosts are a good choice when you have existing per-socket, per-core, or per-VM software licenses, or when you must address specific corporate compliance and regulatory requirements.
The four pillars of cost optimization
	0.	To optimize costs, you must consider four consistent, powerful drivers:
	•	  Right-size - Choose the right balance of instance types. Notice when servers can be either sized down or turned off, and still meet your performance requirements.
	•	  Increase elasticity - Design your deployments to reduce the amount of server capacity that is idle by implementing deployments that are elastic, such as deployments that use automatic scaling to handle peak loads.
	•	  Optimal pricing model - Recognize the available pricing options. Analyze your usage patterns so that you can run EC2 instances with the right mix of pricing options.
	•	  Optimize storage choices - Analyze the storage requirements of your deployments. Reduce unused storage overhead when possible, and choose less expensive storage options if they can still meet your requirements for storage performance.
Pillar 1: Right size
	0.	First, consider right-sizing. AWS offers approximately 60 instance types and sizes. The wide choice of options enables customers to select the instance that best fits their workload. It can be difficult to know where to start and what instance choice will prove to be the best, from both a technical perspective and a cost perspective. Right-sizing is the process of reviewing deployed resources and looking for opportunities to downsize when possible.
	0.	To right-size:
	•	Select the cheapest instance available that still meets your performance requirements.
	•	Review CPU, RAM, storage, and network utilization to identify instances that could be downsized. You might want to provision a variety of instance types and sizes in a test environment, and then test your application on those different test deployments to identify which instances offer the best performance-to-cost ratio. For right-sizing, use techniques such as load testing to your advantage.
	•	Use Amazon CloudWatch metrics and set up custom metrics. A metric represents a time-ordered set of values that are published to CloudWatch (for example, the CPU usage of a particular EC2 instance). Data points can come from any application or business activity for which you collect data.
Pillar 2: increase elasticity
	0.	One form of elasticity is to create, start, or use EC2 instances when they are needed, but then to turn them off when they are not in use. Elasticity is one of the central tenets of the cloud, but customers often go through a learning process to operationalize elasticity to drive cost savings.
	0.	The easiest way for large customers to embrace elasticity is to look for resources that look like good candidates for stopping or hibernating, such as non-production environments, development workloads, or test workloads. For example, if you run development or test workloads in a single time zone, you can easily turn off those instances outside of business hours and thus reduce runtime costs by perhaps 65 percent. The concept is similar to why there is a light switch next to the door, and why most offices encourage employees to turn off the lights on their way out of the office each night.
	0.	For production workloads, configuring more precise and granular automatic scaling policies can help you take advantage of horizontal scaling to meet peak capacity needs and to not pay for peak capacity all the time.
	0.	As a rule of thumb, you should target 20-30 percent of your Amazon EC2 instances to run as On-Demand Instances or Spot Instances, and you should also actively look for ways to maximize elasticity.
Pillar 3: Optimal Pricing Model
	0.	AWS provides a number of pricing models for Amazon EC2 to help customers save money. The models available were discussed in detail earlier in this module. Customers can combine multiple purchase types to optimize pricing based on their current and forecast capacity needs.
	0.	Customers are also encouraged to consider their application architecture. For example, does the functionality provided by your application need to run on an EC2 virtual machine? Perhaps by making use of the AWS Lambda service instead, you could significantly decrease your costs.
	0.	AWS Lambda is discussed later in this module.
Pillar 4: Optimize storage choices
	0.	Customers can also reduce storage costs. When you launch EC2 instances, different instance types offer different storage options. It is a best practice to try to reduce costs while also maintaining storage performance and availability.
	0.	One way you can accomplish this is by resizing EBS volumes. For example, if you originally provisioned a 500-GB volume for an EC2 instance that will only need a maximum of 20 GB of storage space, you can reduce the size of the volume and save on costs.
	0.	There are also a variety of EBS volume types. Choose the least expensive type that still meets your performance requirements. For example, Amazon EBS Throughput Optimized HDD (st1) storage typically costs half as much as the default General Purpose SSD (gp2) storage option. If an st1 drive will meet the needs of your workload, take advantage of the cost savings.
	0.	Customers often use EBS snapshots to create data backups. However, some customers forget to delete snapshots that are no longer needed. Delete these unneeded snapshots to save on costs.
	0.	Finally, try to identify the most appropriate destination for specific types of data. Does your application need the data it uses to reside on Amazon EBS? Would the application run equally as well if it used Amazon S3 for storage instead? Configuring data lifecycle policies can also reduce costs. For example, you might automate the migration of older infrequently accessed data to cheaper storage locations, such as Amazon Simple Storage Service Glacier.
Measure, Monitor, and Improve
	0.	If it is done correctly, cost optimization is not a one-time process that a customer completes. Instead, by routinely measuring and analyzing your systems, you can continually improve and adiust your costs.
	0.	Tagging helps provide information about what resources are being used by whom and for what purpose. You can activate cost allocation tags in the Billing and Cost Management console, and AWS can generate a cost allocation report with usage and costs grouped by your active tags. Apply tags that represent business categories (such as cost centers, application names, or owners) to organize your costs across multiple services.
	0.	Encourage teams to architect for cost. AWS Cost Explorer is a free tool that you can use to view graphs of your costs. You can use Cost Explorer to see patterns in how much you spend on AWS resources over time, identify areas that need further inquiry, and see trends that you can use to understand your costs.
	0.	Use AWS services such as AWS Trusted Advisor, which provides real-time guidance to help you provision resources that follow AWS best practices.
	0.	Cost-optimization efforts are typically more successful when the responsibility for cost optimization is assigned to an individual or to a team.
Section 4: Container services

Container basics
	0.	Containers are a method of operating system virtualization that enables you to run an application and its dependencies in resource-isolated processes. By using containers, you can easily package an application's code, configurations, and dependencies into easy-to-use building blocks that deliver environmental consistency, operational efficiency, developer productivity, and version control.
	0.	Containers are smaller than virtual machines, and do not contain an entire operating system. Instead, containers share a virtualized operating system and run as resource-isolated processes, which ensure quick, reliable, and consistent deployments. Containers hold everything that the software needs to run, such as libraries, system tools, code, and the runtime.
	0.	Containers deliver environmental consistency because the application's code, configurations, and dependencies are packaged into a single object.
	0.	In terms of space, container images are usually an order of magnitude smaller than virtual machines. Spinning up a container happens in hundreds of milliseconds. Thus, by using containers, you can use a fast, portable, and infrastructure-agnostic environments.
	0.	Containers can help ensure that applications deploy quickly, reliably, and consistently, regardless of deployment environment. Containers also give you more granular control over resources, which gives your infrastructure improved efficiency.
What is Docker?
	0.	Docker is a software platform that packages software (such as applications) into containers.
	0.	Docker is installed on each server that will host containers, and it provides simple commands that you can use to build, start, or stop containers.
	0.	By using Docker, you can quickly deploy and scale applications into any environment.
	0.	Docker is best used as a solution when you want to:
	•	  Standardize environments
	•	  Reduce conflicts between language stacks and versions
	•	  Use containers as a service
	•	  Run microservices using standardized code deployments
	•	  Require portability for data processing
Containers versus virtual machines
	0.	 Many people who are first introduced to the concept of a container think that containers are exactly like virtual machines. However, the differences are in the details. One significant difference is that virtual machines run directly on a hypervisor, but containers can run on any Linux OS if they have the appropriate kernel feature support and the Docker daemon is present. This makes containers very portable. Your laptop, your VM, your EC2 instance, and your bare metal server are all potential hosts where you can run a container.
	0.	The right of the diagram has a virtual machine (VM)-based deployment. Each of the three EC2 instances runs directly on the hypervisor that is provided by the AWS Global Infrastructure. Each EC2 instance runs a virtual machine. In this VM-based deployment, each of the three apps runs on its own VM, which provides process isolation.
	0.	The left of the diagram has a container-based deployment. There is only one EC2 instance that runs a virtual machine. The Docker engine is installed on the Linux guest OS of the EC2 instance, and there are three containers. In this container-based deployment, each app runs in its own container (which provides process isolation), but all the containers run on a single EC2 instance. The processes that run in the containers communicate directly to the kernel in the Linux guest OS and are largely unaware of their container silo. The Docker engine is present to manage how the containers run on the Linux guest OS, and it also provides essential management functions throughout the container lifecycle.
	0.	In an actual container-based deployment, a large EC2 instance could run hundreds of containers.
Amazon elastic container service (Amazon ECS)
	0.	Given what you now know about containers, you might think that you could launch one or more
	0.	Amazon EC2 instances, install Docker on each instance, and manage and run the Docker containers on those Amazon EC2 instances yourself. While that is an option, AWS provides a service called Amazon Elastic Container Service Amazon ECS) that simplifies container management.
	0.	Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container management service that supports Docker containers. Amazon ECS enables you to easily run applications on a managed cluster of Amazon EC2 instances.
	0.	Essential Amazon ECS features include the ability to:
	•	Launch up to tens of thousands of Docker containers in seconds
	•	Monitor container deployment
	•	Manage the state of the cluster that runs the containers
	•	Schedule containers by using a built-in scheduler or a third-party scheduler (for example,Apache Mesos or Blox)
	0.	Amazon ECS clusters can also use Spot Instances and Reserved Instances.
Amazon ECS orchestrates containers
	0.	To prepare your application to run on Amazon ECS, you create a task definition which is a text file that describes one or more containers, up to a maximum of ten, that form your application. It can be thought of as a blueprint for your application. Task definitions specify parameters for your application, for example which containers to use, which ports should be opened for your application, and what data volumes should be used with the containers in the task.
	0.	A task is the instantiation of a task definition within a cluster. You can specify the number of tasks that will run on your cluster. The Amazon ECS task scheduler is responsible for placing tasks within your cluster. A task will run anywhere from one to ten containers, depending on the task definition you defined.
	0.	When Amazon ECS runs the containers that make up your task, it places them on an ECS cluster. The cluster (when you choose the EC2 launch type) consists of a group of EC2 instances each of which is running an Amazon ECS container agent.
	0.	Amazon ECS provides multiple scheduling strategies that will place containers across your clusters based on your resource needs (for example, CPU or RAM) and availability requirements.
Amazon ECS cluster options
	0.	When you create an Amazon ECS cluster, you have three options:
	•	A Networking Only cluster (powered by AWS Fargate)
	•	An EC2 Linux + Networking cluster
	•	An EC2 Windows + Networking cluster
	0.	If you choose one of the two EC2 launch type options, you will then be prompted to choose whether the cluster EC2 instances will run as On-Demand Instances or Spot Instances. In addition, you will need to specify many details about the EC2 instances that will make up your cluster-the same details that you must specify when you launch a stand lone EC2 instance. In this way, the EC2 launch type provides more granular control over the infrastructure that runs your container applications because you manage the EC2 instances that make up the cluster. Amazon ECS keeps track of all the CPU, memory, and other resources in your cluster. Amazon ECS also finds the best server for your container on based on your specified resource requirements.
	0.	If you choose the networking-only Fargate launch type, then the cluster that will run your containers will be managed by AWS. With this option, you only need to package your application in containers, specify the CPU and memory requirements, define networking and IAM policies, and launch the application. You do not need to provision, configure, or scale the cluster. It removes the need to choose server types, decide when to scale your clusters, or optimize cluster packing. The Fargate option enables you to focus on designing and building your applications.
What is Kubernetes?
	0.	Kubernetes is open source software for container orchestration. Kubernetes can work with many containerization technologies, including Docker. Because it is a popular open source project, a large community of developers and companies build extensions, integrations, and plugins that keep the software relevant, and new and in-demand features are added frequently.
	0.	Kubernetes enables you to deploy and manage containerized applications at scale. With Kubernetes, you can run any type of containerized application by using the same toolset in both on-premises data centers and the cloud. Kubernetes manages a cluster of compute instances (called nodes). It runs containers on the cluster, which are based on where compute resources are available and the resource requirements of each container. Containers are run in logical groupings called pods. You can run and scale one or many containers together as a pod. Each pod is given an IP address and a single Domain Name System (DNS) name, which Kubernetes uses to connect your services with each other and external traffic.
	0.	A key advantage of Kubernetes is that you can use it to run your containerized applications anywhere without needing to change your operational tooling. For example, applications can be moved from local on-premises development machines to production deployments in the cloud by using the same operational tooling.
Amazon Elastic Kubernetes Service (Amazon EKS)
	0.	You might think that you could launch one or more Amazon EC2 instances, install Docker on each instance, install Kubernetes on the cluster, and manage and run Kubernetes yourself. While that is an option, AWS provides a service called Amazon Elastic Kubernetes Service (Amazon EKS) that simplifies the management of Kubernetes clusters.
	0.	Amazon Elastic Kubernetes Service (Amazon EKS) is a managed Kubernetes service that makes it easy for you to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane. It is certified Kubernetes conformant, so existing applications that run on upstream Kubernetes are compatible with Amazon EKS.
	0.	Amazon EKS automatically manages the availability and scalability of the cluster nodes that are responsible for starting and stopping containers, scheduling containers on virtual machines, storing cluster data, and other tasks. It automatically detects and replaces unhealthy control plane nodes for each cluster. You can take advantage of the performance, scale, reliability, and availability of the AWS Cloud, which includes AWS networking and security services like Application Load Balancers for load distribution, IAM for role-based access control, and VPC for pod networking.
	0.	You may be wondering why Amazon offers both Amazon ECS and Amazon EKS, since they are both capable of orchestrating Docker containers. The reason that both services exist is to provide customers with flexible options. You can decide which option best matches your needs.
Amazon Elastic Container Registry (Amazon ECR)
	0.	Amazon Elastic Container Registry (Amazon ECR) is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. It is integrated with Amazon ECS, so you can store, run, and manage container images for applications that run on Amazon ECS. Specify the Amazon ECR repository in your task definition, and Amazon ECS will retrieve the appropriate images for your applications.
	0.	Amazon ECR supports Docker Registry HTTP API version 2, which enables you to interact with Amazon ECR by using Docker CLI commands or your preferred Docker tools. Thus, you can maintain your existing development workflow and access Amazon ECR from any Docker environment-whether it is in the cloud, on premises, or on your local machine.
	0.	You can transfer your container images to and from Amazon ECS via HTTPS. Your images are also automatically encrypted at rest using Amazon S3 server-side encryption. 
	0.	It is also possible to use Amazon ECR images with Amazon EKS. See the Using Amazon ECR Images with Amazon EKS documentation at https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR on EKS.htmlfordetails.
Section 5: Introduction to AWS Lambda

AWS Lambda: Run code without servers
	0.	As you saw in the earlier sections of this module, AWS offers many compute options. For example, Amazon EC2 provides virtual machines. As another example, Amazon ECS and Amazon EKS are container-based compute services.
	0.	However, there is another approach to compute that does not require you to provision or manage servers. This third approach is often referred to as serverless computing.
	0.	AWS Lambda is an event-driven, serverless compute service. Lambda enables you to run code without provisioning or managing servers.
	0.	You create a Lambda function, which is the AWS resource that contains the code that you upload. You then set the Lambda function to be triggered, either on a scheduled basis or in response to an event. Your code only runs when it is triggered.
	0.	You pay only for the compute time you consume-you are not charged when your code is not running.
Benefits of Lambda
	0.	With Lambda, there are no new languages, tools, or frameworks to learn. Lambda supports multiple programming languages, including Java, Go, PowerShell, Node.js, C#, Python, and Ruby. Your code can use any library, either native or third-party.
	0.	Lambda completely automates the administration. It manages all the infrastructure to run your code on highly available, fault-tolerant infrastructure, which enables you to focus on building differentiated backend services. Lambda seamlessly deploys your code; does all the administration, maintenance, and security patches; and provides built-in logging and monitoring through Amazon CloudWatch.
	0.	Lambda provides built-in fault tolerance. It maintains compute capacity across multiple Availability Zones in each Region to help protect your code against individual machine failures or data center failures. There are no maintenance windows or scheduled downtimes.
	0.	You can orchestrate multiple Lambda functions for complex or long-running tasks by building workflows with AWS Step Functions. Use Step Functions to define workflows. These workflows trigger a collection of Lambda functions by using sequential, parallel, branching, and error-handling steps. With Step Functions and Lambda, you can build stateful, long-running processes for applications and backends.
	0.	With Lambda, you pay only for the requests that are served and the compute time that is required to run your code. Billing is metered in increments of 100 milliseconds, which make it cost-effective and easy to scale automatically from a few requests per day to thousands of requests per second.
AWS Lambda event sources
	0.	An event source is an AWS service or a developer-created application that produces events that trigger an AWS Lambda function to run.
	0.	Some services publish events to Lambda by invoking the Lambda function directly. These services that invoke Lambda functions asynchronously include, but are not limited to, Amazon S3, Amazon Simple Notification Service (Amazon SNS), and Amazon CloudWatch Events.
	0.	Lambda can also poll resources in other services that do not publish events to Lambda. For example, Lambda can pull records from an Amazon Simple Queue Service (Amazon SQ) queue and run a Lambda function for each fetched message. Lambda can similarly read events from Amazon DynamoDB.
	0.	Some services, such as Elastic Load Balancing (Application Load Balancer) and Amazon API Gateway can invoke your Lambda function directly.
	0.	You can invoke Lambda functions directly with the Lambda console, the Lambda API, the AWS software development kit (SD), the AWS CLI, and AWS toolkits. The direct invocation approach can be useful, such as when you are developing a mobile app and want the app to call Lambda functions. See the Using Lambda with Other Services documentation at https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.htm| for further details about all supported services.
	0.	AWS Lambda automatically monitors Lambda functions by using Amazon CloudWatch. To help you troubleshoot failures in a function, Lambda logs all requests that are handled by your function. It also automatically stores logs that are generated by your code through Amazon CloudWatch Logs.
AWS Lambda function configuration
	0.	Remember that a Lambda function is the custom code that you write to process events, and that Lambda runs the Lambda function on your behalf.
	0.	When you use the AWS Management Console to create a Lambda function, you first give the function a name. Then, you specify:
	•	The runtime environment the function will use (for example, a version of Python or Node.js)
	•	An execution role (to grant IAM permission to the function so that it can interact with otherAWS services as necessary)
	0.	Next, after you click Create Function, you configure the function. Configurations include:
	•	Add a trigger (specify one of the available event sources from the previous slide)
	•	Add your function code (use the provided code editor or upload a file that contains your code)
	•	Specify the memory in MB to allocate to your function (128 MB to 10,240 MB)
	•	Optionally specify environment variables, description, timeout, the specific virtual private cloud (VPC) to run the function in, tags you would like to use, and other settings. For more information, see Configuring functions in the AWS Lambda console https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html in the AWS Documentation.
	0.	All of the above settings end up in a Lambda deployment package which is a ZIP archive that contains your function code and dependencies. When you use the Lambda console to author your function, the console manages the package for you. However, you need to create a deployment package if you use the Lambda API to manage functions.
Schedule-based Lambda functions example: Start and stop EC2 instances
	0.	Consider an example use case for a schedule-based Lambda function. Say that you are in a situation where you want to reduce your Amazon EC2 usage. You decide that you want to stop instances at a predefined time (for example, at night when no one is accessing them) and then you want to start the instances back up in the morning (before the workday starts).
	0.	In this situation, you could configure AWS Lambda and Amazon CloudWatch Events to help you accomplish these actions automatically.
	0.	Here is what happens at each step in the example:
	0.	A CloudWatch event is scheduled to run a Lambda function to stop your EC2 instances at (for example) 22:00 GMT.
	0.	The Lambda function is triggered and runs with the IAM role that gives the function permission to stop the EC2 instances.
	0.	The EC2 instances enter the stopped state.
	0.	Later, at (for example) 05:00 AM GMT, a CloudWatch event is scheduled to run a Lambda function to start the EC2 instances.
	0.	The Lambda function is triggered and runs with the IAM role that gives it permission to start the EC2 instances.
	0.	The EC2 instances enter the running state.
Event-based Lambda function example: create thumbnail images
	0.	Now, consider an example use case for an event-based Lambda function. Suppose that you want to create a thumbnail for each image (jpg or png object) that is uploaded to an S3 bucket.
	0.	To build a solution, you can create a Lambda function that Amazon S3 invokes when objects are uploaded. Then, the Lambda function reads the image object from the source bucket and creates a thumbnail image in a target bucket. Here's how it works:
	0.	A user uploads an object to the source bucket in Amazon S3 (object-created event).
	0.	Amazon S3 detects the object-created event.
	0.	Amazon S3 publishes the object-created event to Lambda by invoking the Lambda function and passing event data.
	0.	Lambda runs the Lambda function by assuming the execution role that you specified when you created the Lambda function.
	0.	Based the event data that the Lambda function receives, it knows the source bucket name and object key name. The Lambda function reads the object and creates a thumbnail by using graphics libraries, and saves the thumbnail to the target bucket.
AWS Labda quotes
	0.	AWS Lambda does have some quotas that you should know about when you create and deploy Lambda functions.
	0.	AWS Lambda limits the amount of compute and storage resources that you can use to run and store functions. For example, as of this writing, the maximum memory allocation for a single Lambda function is 10,240 MB. It also has limits of 1,000 concurrent executions in a Region. Lambda functions can be configured to run up to 15 minutes per run. You can set the timeout to any value between 1 second and 15 minutes. If you are troubleshooting a Lambda deployment, keep these limits in mind.
	0.	There are limits on the deployment package size of a function (250 MB). A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Using layers can help avoid reaching the size limit for deployment package. Layers are also a good way to share code and data between Lambda functions.
	0.	For larger workloads that rely on sizable dependencies, such as machine learning or data intensive workloads, you can deploy your Lambda function to a container image up to 10 GB in size.
	0.	Limits are either soft or hard. Soft limits on an account can potentially be relaxed by submitting a support ticket and providing justification for the request. Hard limits cannot be increased.
	0.	For the details on current AWS Lambda quotas, refer to the AWS Lambda quotas documentation at https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html.
Section 6: Introduction to AWS Elastic Beanstalk

AWS Elastic Beanstalk
	0.	AWS Elastic Beanstalk is another AWS compute service option. It is a platform as a service (or PaaS) that facilitates the quick deployment, scaling, and management of your web applications and services.
	0.	You remain in control. The entire platform is already built, and you only need to upload your code. Choose your instance type, your database, set and adjust automatic scaling, update your application, access the server log files, and enable HTTPS on the load balancer.
	0.	You upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning and load balancing to automatic scaling and monitoring application health. At the same time, you retain full control over the AWS resources that power your application, and you can access the underlying resources at any time.
	0.	There is no additional charge for AWS Elastic Beanstalk. You pay for the AWS resources (for example, EC2 instances or S3 buckets) you create to store and run your application. You only pay for what you use, as you use it. There are no minimum fees and no upfront commitments.
AWS Elastic Beanstalk deployment
	0.	AWS Elastic Beanstalk enables you to deploy your code through the AWS Management Console, the AWS Command Line Interface (AWS CLI), Visual Studio, and Eclipse. It provides all the application services that you need for your application. The only thing you must create is your code. Elastic Beanstalk is designed to make deploying your application a quick and easy process.
	0.	Elastic Beanstalk supports a broad range of platforms. Supported platforms include Docker, Go, Java, .NET, Node.js, PHP, Python, and Ruby.
	0.	AWS Elastic Beanstalk deploys your code on Apache Tomcat for Java applications; Apache HTTP Server for PHP and Python applications; NGINX or Apache HTTP Server for Node.js applications; Passenger or Puma for Ruby applications; and Microsoft Internet Information Services (IS) for .NET applications, Java SE, Docker, and Go.
Benefits of Elastic Beanstalk
	0.	Elastic Beanstalk is fast and simple to start using. Use the AWS Management Console, a Git repository, or an integrated development environment (IDE) such as Eclipse or Visual Studio to upload your application. Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, automatic scaling, and monitoring application health.
	0.	You can improve your developer productivity by focusing on writing code instead of managing and configuring servers, databases, load balancers, firewalls, and networks. AWS updates the underlying platform that runs your application with patches and updates.
	0.	Elastic Beanstalk is difficult to outgrow. With Elastic Beanstalk, your application can handle peaks in workload or traffic while minimizing your costs. It automatically scales your application up or down based on your application's specific needs by using easily adjustable automatic scaling settings. You can use CPU utilization metrics to trigger automatic scaling actions.
	0.	You have the freedom to select the AWS resources-such as Amazon EC2 instance type-that are optimal for your application. Elastic Beanstalk enables you to retain full control over the AWS resources that power your application. If you decide that you want to take over some (or all) of the elements of your infrastructure, you can do so seamlessly by using the management capabilities that are provided by Elastic Beanstalk.
Quiz
	0.	AWS is more economical than traditional data centers with varying compute workloads because Amazon EC2 instances can be launched on-demand.
	0.	If your project requires you to run monthly reports that iterate through very large amounts of data, you should consider purchasing Scheduled Reserved Instances.
	0.	An Amazon Machine Image (AMI) includes all of these.
	0.	Dedicated Instances ensure your instances will not share a physical host with instances from any other AWS customer.
	0.	AWS Lambda is a compute service that lets you run code without provisioning or managing servers.
	0.	Elastic Beanstalk is an AWS compute service option. It is a Platform as a Service (or PaaS) that facilitates quick deployment, scaling and managing of your web applications and services.
	0.	Reserved Instances provide cost savings when you can commit to running instances full time, such as to handle the base traffic. On-Demand instances provide the flexibility to handle traffic spikes.
	0.	Containers are smaller than virtual machines, and do not contain an entire operating system.
	0.	Reserved instances would be the best EC2 option for long-term workloads with predictable usage patterns.
	0.	AWS assigns the EC2 instance ID as part of the launch process and the administrator password, which is encrypted via the public key. The instance type defines the virtual hardware and the AMI defines the initial software state. Both must be specified on launch.
 W6

Section1: Amazon Elastic Block Store (Amazon EBS)

Storage
	0.	Amazon EBS provides persistent block storage volumes for use with Amazon EC2 instances. Persistent storage is any data storage device that retains data after power to that device is shut off. It is also sometimes called non-volatile storage.
	0.	Each Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure. It is designed for high availability and durability. Amazon EBS volumes provide the consistent and low-latency performance that is needed to run your workloads.
	0.	With Amazon EBS, you can scale your usage up or down within minutes, while paying a low price for only what you provision.
AWS storage options: Block storage versus object storage
	0.	What happens if you want to change one character in a 1-GB file? With block storage, you change only the block that contains the character. With object storage, the entire file must be updated.
	0.	One critical difference between some storage types is whether they offer block-level storage or object-level storage.
	0.	This difference has a major effect on the throughput, latency, and cost of your storage solution. Block storage solutions are typically faster and use less bandwidth, but they can cost more than object-level storage.
Amazon EBS
	0.	Amazon EBS enables you to create individual storage volumes and attach them to an Amazon EC2 instance. Amazon EBS offers block-level storage, where its volumes are automatically replicated within its Availability Zone. Amazon EBS is designed to provide durable, detachable, block-level storage (which is like an external hard drive) for your Amazon EC2 instances. Because they are directly attached to the instances, they can provide low latency between where the data is stored and where it might be used on the instance.
	0.	For this reason, they can be used to run a database with an Amazon EC2 instance. Amazon EBS volumes are included as part of the backup of your instances into Amazon Machine Images (or AMIs). AMIs are stored in Amazon S3 and can be reused to create new Amazon EC2 instances later.
	0.	A backup of an Amazon EBS volume is called a snapshot. The first snapshot is called the baseline snapshot. Any other snapshot after the baseline captures only what is different from the previous snapshot.
	0.	Amazon EBS volumes uses include:
	•	Boot volumes and storage for Amazon EC2 instances
	•	Data storage with a file system
	•	Database hosts
	•	Enterprise applications
Amazon EBS volume types
	0.	Volume types

	0.	Matching the correct technology to your workload is a best practice for reducing storage costs. Provisioned IOPS SSD-backed Amazon EBS volumes can give you the highest performance. However, if your application doesn't require or won't use performance that high, General Purpose SSD is usually sufficient. Only SSDs can be used as boot volumes for EC2 instances. The lower-cost options might be a solution for additional storage or use cases other than boot volumes.
	0.	To learn more about Amazon EBS volume types, see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html.
Amazon EBS volume type use cases
	0.	As mentioned previously an Amazon EBS volume is a durable, block-level storage device that you can attach to a single EC2 instance. You can use Amazon EBS volumes as primary storage for data that requires frequent updates, such as the system drive for an instance or storage for a database application. You can also use them for throughput-intensive applications that perform continuous disk scans. Amazon EBS volumes persist independently from the running life of an EC2 instance.
	0.	Use cases for EBS vary by the storage type used and whether you are using General Purpose of Provisioned OPS.
Amazon EBS features
	0.	To provide an even higher level of data durability, Amazon EBS enables you to create point-in-time snapshots of your volumes, and you can re-create a new volume from a snapshot at any time. You can also share snapshots or even copy snapshots to different AWS Regions for even greater disaster recovery (DR) protection. For example, you can encrypt and share your snapshots from Virginia in the US to Tokyo, Japan.
	0.	You can also have encrypted Amazon EBS volumes at no additional cost, so the data that moves between the EC2 instance and the EBS volume inside AWS data centers is encrypted in transit.
	0.	As your company grows, the amount of data that is stored on your Amazon EBS volumes is also likely to grow. Amazon EBS volumes can increase capacity and change to different types, so you can change from hard disk drives (HDDs) to solid state drives (SSDs) or increase from a 50-GB volume to a 16-TB volume. For example, you can do this resize operation dynamically without needing to stop the instances.
Amazon EBS: Volumes, IOPS, and pricing
	0.	When you begin to estimate the cost for Amazon EBS, you must consider the following:
	0.	Volumes - Volume storage for all Amazon EBS volume types is charged by the amount you provision in GB per month, until you release the storage.
	0.	IOPS - 1/O is included in the price of General Purpose SSD volumes. However, for Amazon EBS magnetic volumes, 1/O is charged by the number of requests that you make to your volume.With Provisioned IOPS SSD volumes, you are also charged by the amount you provision in OPS (multiplied by the percentage of days that you provision for the month).
	0.	The pricing and provisioning of Amazon EBS are complex. In general, you pay for the size of the volume and its usage. To learn more about the full, highly complex pricing and provisioning concepts of Amazon BS, see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html.
Amazon EBS: Snapshots and data transfer
	0.	   Snapshots - Amazon EBS enables you to back up snapshots of your data to Amazon S3 for durable recovery. If you opt for Amazon EBS snapshots, the added cost is per GB-month of data stored.
	0.	   Data transfer - When you copy Amazon EBS snapshots, you are charged for the data that is transferred across Regions. After the snapshot is copied, standard Amazon EBS snapshot charges apply for storage in the destination Region.
Section 2: Amazon Simple Storage Service (Amazon S3)

Storage
	0.	Amazon S3 is object-level storage, which means that if you want to change a part of a file, you must make the change and then re-upload the entire modified file. Amazon S3 stores data as objects within resources that are called buckets.
	0.	You will now learn more about Amazon S3.
Amazon S3 overview
	0.	Amazon S3 is a managed cloud storage solution that is designed to scale seamlessly and provide 11 9s of durability. You can store virtually as many objects as you want in a bucket, and you can write, read, and delete objects in your bucket. Bucket names are universal and must be unique across all existing bucket names in Amazon S3. Objects can be up to 5 TB in size. By default, data in Amazon S3 is stored redundantly across multiple facilities and multiple devices in each facility.
	0.	The data that you store in Amazon S3 is not associated with any particular server, and you do not need manage any infrastructure yourself. You can put as many objects into Amazon S3 as you want. Amazon S3 holds trillions of objects and regularly peaks at millions of requests per second.
	0.	Objects can be almost any data file, such as images, videos, or server logs. Because Amazon S3 supports objects as large as several terabytes in size, you can even store database snapshots as objects. Amazon S3 also provides low-latency access to the data over the internet by Hypertext Transfer Protocol (HTTP) or Secure HTTP (HTTPS), so you can retrieve data anytime from anywhere. You can also access Amazon S3 privately through a virtual private cloud (VPC endpoint. You get fine-grained control over who can access your data by using AWS Identity and Access Management (IAM) policies, Amazon S3 bucket policies, and even per-object access control lists.
	0.	By default, none of your data is shared publicly. You can also encrypt your data in transit and choose to enable server-side encryption on your objects. You can access Amazon S3 through the web-based AWS Management Console; programmatically through the API and SDKs; or with third-party solutions, which use the API or the SDKs.
	0.	Amazon S3 includes event notifications that enable you to set up automatic notifications when certain events occur, such as when an object is uploaded to a bucket or deleted from a specific bucket. Those notifications can be sent to you, or they can be used to trigger other processes, such as AWS Lambda functions. With storage class analysis, you can analyze storage access patterns and transition the right data to the right storage class. The Amazon S3 Analytics feature automatically identifies the optimal lifecycle policy to transition less frequently accessed storage to Amazon S3 Standard - Infrequent Access (Amazon S3 Standard-IA). You can configure a storage class analysis policy to monitor an entire bucket, a prefix, or an object tag. When an infrequent access pattern is observed, you can easily create a new lifecycle age policy that is based on the results. Storage class analysis also provides daily visualizations of your storage usage in the AWS Management Console. You can export them to an Amazon S3 bucket to analyze by using the business intelligence (BI) tools of vour choice, such as Amazon QuickSight.
Amazon S3 storgage classes
	0.	Amazon S3 offers a range of object-level storage classes that are designed for different use cases. These classes include:
	•	Amazon S3 Standard - Amazon S3 Standard is designed for high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, Amazon S3 Standard is appropriate for a variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.
	•	Amazon S3 Intelligent-Tiering - The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in Amazon S3 Intelligent-Tiering, and moves the objects that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. There are no retrieval fees when you use the Amazon S3 Intelligent-Tiering storage class, and no additional fees when objects are moved between access tiers. It works well for long-lived data with access patterns that are unknown or unpredictable.
	•	Amazon S3 Standard-Infrequent Access (Amazon S3 Standard-IA) - The Amazon S3 Standard-lA storage class is used for data that is accessed less frequently, but requires rapid access when needed. Amazon S3 Standard-IA is designed to provide the high durability, high throughput, and low latency of Amazon S3 Standard, with a low per-GB storage price and per-GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA good for long-term storage and backups, and as a data store for disaster recovery files.
	•	Amazon S3 One Zone-Infrequent Access Amazon S3 One Zone-IA) - Amazon S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other Amazon S3 storage classes, which store data in a minimum of three Availability Zones, Amazon S3 One Zone-lA stores data in a single Availability Zone and it costs less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA works well for customers who want a lower-cost option for infrequently accessed data, but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. It is a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. You can also use it as cost-effective storage for data that is replicated from another AWS Region by using Amazon S3 Cross-Region Replication.
	•	Amazon S3 Glacier - Amazon S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with-or cheaper than -on-premises solutions. To keep costs low yet suitable for varying needs, Amazon S3 Glacier provides three retrieval options that range from a few minutes to hours. You can upload objects directly to Amazon S3 Glacier, or use Amazon S3 lifecycle policies to transfer data between any of the Amazon S3 storage classes for active data (Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA) and Amazon S3 Glacier.
	•	Amazon S3 Glacier Deep Archive - Amazon S3 Glacier Deep Archive is the lowest-cost storage class for Amazon S3. It supports long-term retention and digital preservation for data that might be accessed once or twice in a year. It is designed for customers - particularly customers in highly regulated industries, such as financial services, healthcare, and public sectors - that retain datasets for 7-10 years (or more) to meet regulatory compliance requirements. Amazon S3 Glacier Deep Archive can also be used for backup and disaster recovery use cases. It is a cost-effective and easy-to-manage alternative to magnetic tape systems, whether these tape systems are on-premises libraries or off-premises services. Amazon S3 Glacier Deep Archive complements Amazon S3 Glacier, and it is also designed to provide 11 9s of durability. All objects that are stored in Amazon S3 Glacier Deep Archive are replicated and stored across at least three geographically dispersed Availability Zones, and these objects can be restored within 12 hours.
	0.	For more information about Amazon S3 storage classes, see https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html.
Amazon S3 bucket URLs (two styles)
	0.	To use Amazon S3 effectively, you must understand a few simple concepts. First, Amazon S3 stores data inside buckets. Buckets are essentially the prefix for a set of files, and must be uniquely named across all of Amazon S3 globally. Buckets are logical containers for objects. You can have one or more buckets in your account. You can control access for each bucket -who can create, delete, and list objects in the bucket. You can also view access logs for the bucket and its objects, and choose the geographical region where Amazon S3 stores the bucket and its contents.
	0.	To upload your data (such as photos, videos, or documents), create a bucket in an AWS Region, and then upload almost any number of objects to the bucket.
	0.	In the example, Amazon S3 was used to create a bucket in the Tokyo Region, which is identified within AWS formally by its Region code: ap-northeast-1
	0.	The URL for a bucket is structured like the examples. You can use two different URL styles to refer to buckets.
	0.	Amazon S3 refers to files as objects. As soon as you have a bucket, you can store almost any number of objects inside it. An object is composed of data and any metadata that describes that file, including a URL. To store an object in Amazon S3, you upload the file that you want to store to a bucket.
	0.	When you upload a file, you can set permissions on the data and any metadata.
	0.	In this example the object Preview2.mp4 is stored inside the bucket. The URL for the file includes the object name at the end.
Data is redundantly stored in the Region
	0.	When you create a bucket in Amazon S3, it is associated with a specific AWS Region. When you store data in the bucket, it is redundantly stored across multiple AWS facilities within your selected Region.
	0.	Amazon S3 is designed to durably store your data, even if there is concurrent data loss in two AWS facilities.
Designed for seamless scaling
	0.	Amazon S3 automatically manages the storage behind your bucket while your data grows. You can get started immediately, and your data storage will grow with your application needs.
	0.	Amazon S3 also scales to handle a high volume of requests. You do not need to provision the storage or throughput, and you are billed only for what you use.
Access the data anywhere
	0.	You can access Amazon S3 through the console, AWS Command Line Interface (AWS CLI), or AWS SDK. You can also access the data in your bucket directly by using REST-based endpoints.
	0.	The endpoints support HTTP or HTTPS access. To support this type of URL-based access, Amazon S3 bucket names must be globally unique and Domain Name Server (DNS)-compliant.
	0.	Also, object keys should use characters that are safe for URLs.
Common use cases
	0.	This flexibility to store a virtually unlimited amount of data-and to access that data from anywhere-means that Amazon S3 is suitable for a variety of scenarios. You will now consider some use cases for Amazon S3
	•	As a location for any application data, Amazon S3 buckets provide a shared location for storing objects that any instances of your application can access-including applications on Amazon EC2 or even traditional servers. This feature can be useful for user-generated media files, server logs, or other files that your application must store in a common location. Also, because the content can be fetched directly over the internet, you can offload serving that content from your application and enable clients to directly fetch the data from Amazon S3 themselves.
	•	For static web hosting, Amazon S3 buckets can serve the static contents of your website, including HTML, CSS, JavaScript, and other files.
	•	The high durability of Amazon S3 makes it a good candidate for storing backups of your data. For greater availability and disaster recovery capability, Amazon S3 can even be configured to support cross-Region replication so that data in an Amazon S3 bucket in one Region can be automatically replicated to another Amazon S3 Region.
Amazon S3 common scenarios
	0.	Backup and storage - Provide data backup and storage services for others
	0.	Application hosting - Provide services that deploy, install, and manage web applications
	0.	Media hosting - Build a redundant, scalable, and highly available infrastructure that hosts video, photo, or music uploads and downloads
	0.	Software delivery - Host your software applications that customers can download
Amazon S3 pricing
	0.	With Amazon S3, specific costs vary depending on the Region and the specific requests that were made. You pay only for what you use, including gigabytes per month; transfer out of other Regions; and PUT, COPY, POST, LIST, and GET requests.
	0.	As a general rule, you pay only for transfers that cross the boundary of your Region, which means you do not pay for transfers in to Amazon S3 or transfers out from Amazon S3 to Amazon CloudFront edge locations within that same Region.
Amazon S3: storage pricing (1 of 2)
	0.	When you begin to estimate the costs of Amazon S3, you must consider the following:
	0.	Storage class type -
	•	Standard storage is designed to provide 11 9s of durability and four 9s of availability.
	•	S3 Standard - Infrequent Access (S-IA) is a storage option within Amazon S3 that you can use to reduce your costs by storing less frequently accessed data at slightly lower levels of redundancy than Amazon S3 standard storage. Standard - Infrequent Access is designed to provide the same 11 9s of durability as Amazon S3, with three 9s of availability in a given year. Each class has different rates.
	0.	Amount of storage - The number and size of objects stored in your Amazon S3 buckets.
Amazon S3: Storage pricing (2 of 2)
3. Requests - Consider the number and type of requests. GET requests incur charges at different rates than other requests, such as PUT and COPY requests.
	•	GET - Retrieves an object from Amazon S3. You must have READ access to use this operation.
	•	PUT - Adds an object to a bucket. You must have WRITE permissions on a bucket to add an object to it.
	•	COPY - Creates a copy of an object that is already stored in Amazon S3. A COPY operation is the same as performing a GET and then a PUT.
4. Data transfer - Consider the amount of data that is transferred out of the Amazon S3 Region.Remember that data transfer in is free, but there is a charge for data transfer out.
Section 3: Amazon Elastic File System (Amazon EFS)

Storage
	0.	Amazon Elastic File System (Amazon EFS) provides simple, scalable, elastic file storage for use with AWS services and on-premises resources. It offers a simple interface that enables you to create and configure file systems quickly and easily.
	0.	Amazon EFS is built to dynamically scale on demand without disrupting applications-it will grow and shrink automatically as you add and remove files. It is designed so that your applications have the storage they need, when they need it.
Amazon EFS features
	0.	Amazon EFS is a fully managed service that makes it easy to set up and scale file storage in the AWS Cloud. You can use Amazon EFS to build a file system for big data and analytics, media processing workflows, content management, web serving, and home directories.
	0.	You can create file systems that are accessible to Amazon EC2 instances through a file system interface (using standard operating system file I/O APIs). These file systems support full file system access semantics, such as strong consistency and file locking.
	0.	Amazon EFS file systems can automatically scale from gigabytes to petabytes of data without the need to provision storage. Thousands of Amazon EC2 instances can access an Amazon EFS file system at the same time, and Amazon EFS is designed to provide consistent performance to each Amazon EC2 instance. Amazon EFS is also designed to be highly durable and highly available. Amazon EFS requires no minimum fee or setup costs, and you pay only for the storage that you use.
Amazon EFS architecture
	0.	Amazon EFS provides file storage in the cloud. With Amazon EFS, you can create a file system, mount the file system on an Amazon EC2 instance, and then read and write data from to and from your file system. You can mount an Amazon EFS file system in your VPC, through NFS versions 4.0 and 4.1 (NFSv4).
	0.	You can access vour Amazon EFS file svstem concurrentl from Amazon EC2 instances in vour VPC, so applications that scale beyond a single connection can access a file system. Amazon EC2 instances that run in multiple Availability Zones within the same AWS Region can access the file system, so many users can access and share a common data source.
	0.	In the diagram, the VPC has three Availability Zones, and each Availability Zone has one mount target that was created in it. We recommend that you access the file system from a mount target within the same Availability Zone. One of the Availability Zones has two subnets. However, a mount target is created in only one of the subnets.
Amazon EFS implementation
	0.	You must complete five steps to create and use your first Amazon EFS file system, mount it on an Amazon EC2 instance in your VPC, and test the end-to-end setup:
	0.	Create your Amazon EC2 resources and launch your instance. (Before you can launch and connect to an Amazon EC2 instance, you must create a key pair, unless you already have one.)
	0.	Create your Amazon EFS file system.
	0.	In the appropriate subnets, create your mount targets.
	0.	Next, connect to your Amazon EC2 instance and mount the Amazon EFS file system.
	0.	Finally, clean up your resources and protect your AWS account.
Amazon EFS resources
	0.	In Amazon EFS, a file system is the primary resource. Each file system has properties such as:
	•	ID
	•	Creation token
	•	Creation time
	•	File system size in bytes
	•	Number of mount targets that are created for the file system
	•	File system state
	0.	Amazon EFS also supports other resources to configure the primary resource. These resources include mount targets and tags.
	0.	Mount target: To access your file system, you must create mount targets in your VPC. Each mount target has the following properties:
	•	The mount target ID
	•	The subnet ID for the subnet where it was created
	•	The file system ID for the file system where it was created
	•	An IP address where the file system can be mounted
	•	The mount target state
	0.	You can use the IP address or the Domain Name System (DNS) name in your mount command.
	0.	Tags: To help organize your file systems, you can assign your own metadata to each of the file systems that you create. Each tag is a key-value pair.
	0.	Think of mount targets and tags as subresources that do not exist unless they are associated with a file system.
Section 4: Amazon S3 Glacier

Amazon S3 Glacier review
	0.	When you use Amazon S3 Glacier to archive data, you can store your data at an extremely low cost (even in comparison to Amazon S3), but you cannot retrieve your data immediately when you want it.
	0.	Data that is stored in Amazon S3 Glacier can take several hours to retrieve, which is why it works well for archiving.
	0.	There are three key Amazon S3 Glacier terms you should be familiar with:
	•	Archive - Any object (such as a photo, video, file, or document) that you store in Amazon S3Glacier. It is the base unit of storage in Amazon S3 Glacier. Each archive has its own unique ID and it can also have a description.
	•	Vault - A container for storing archives. When you create a vault, you specify the vault name and the Region where you want to locate the vault.
	•	Vault access policy - Determine who can and cannot access the data that is stored in the vault, and what operations users can and cannot perform. One vault access permissions policy can be created for each vault to manage access permissions for that vault. You can also use a vault lock policy to make sure that a vault cannot be altered. Each vault can have one vault access policy and one vault lock policy that are attached to it.
	0.	You have three options for retrieving data, each with varying access times and cost:
	•	Expedited retrievals are typically made available within 1-5 minutes (highest cost).
	•	Standard retrievals typically complete within 3-5 hours (less time than expedited, more time than bulk).
	•	Bulk retrievals typically complete within 5-12 hours (lowest cost).
	0.	You might compare these options to choosing the cost for shipping a package by using the most economical method for your needs.
Amazon S3 Glacier
	0.	Amazon S3 Glacier's data archiving means that although you can store your data at an extremely low cost (even in comparison to Amazon S3), you cannot retrieve your data immediately when you want it.
	0.	Data stored in Amazon S3 Glacier can take several hours to retrieve.
	0.	You should be familiar with three key Amazon S3 Glacier terms:
	•	Archive: Any object such as a photo, video, file, or document that you store in Amazon S3Glacier. It is the base unit of storage in Amazon S3 Glacier. Each archive has its own unique ID and can also have a description.
	•	Vault: A container for storing archives. When you create a vault, you specify the vault name and the region in which you would like to locate the vault.
	•	Vault Access Policy: Determine who can and cannot access the data stored in the vault and what operations users can and cannot perform. One vault access policy can be created for each vault to manage access permissions for that vault. You can also use a vault lock policy to make sure a vault cannot be altered. Each vault can have one vault access policy and one vault lock policy that is attached to it.
	0.	Three options are available for retrieving data with varying access times and cost: expedited, standard, and bulk retrievals. They are listed as follows:
	•	Expedited retrievals are typically made available within 1 - 5 minutes (highest cost).
	•	Standard retrievals typically complete within 3 - 5 hours (less than expedited, more than bulk).
	•	Bulk retrievals typically complete within 5 - 12 hours (lowest cost).
	0.	Compare it to choosing the cost to most economically ship a package
Amazon S3 Glacier use cases
	0.	Media asset archiving Media assets-such as video and news footage-require durable storage and can grow to many petabytes over time. Amazon S3 Glacier enables you to archive older media content affordably and then move it to Amazon S3 for distribution when it is needed.
	0.	Healthcare information archiving To meet regulatory requirements, hospital systems must retain petabytes of patient records-such as Low-Income Subsidy (LIS) information, picture archiving and communication system (PACS) data, or Electronic Health Records (EHR) -for decades. Amazon S3 Glacier can help you reliably archive patient record data securely at a very low cost.
	0.	Regulatory and compliance archiving Many enterprises, like those in financial services and healthcare, must retain regulatory and compliance archives for extended durations. Amazon S3 Glacier Vault Lock can help you set compliance controls so you can work towards meeting your compliance objectives, such as the U.S. Securities and Exchange Commission (SEC) Rule 17a-4(f).
	0.	Scientific data archiving Research organizations generate, analyze, and archive large amounts of data. By using Amazon S3 Glacier, you can reduce the complexities of hardware and facility management and capacity planning.
	0.	Digital preservation Libraries and government agencies must handle data integrity challenges in their digital preservation efforts. Unlike traditional systems-which can require laborious data verification and manual repair-Amazon S3 Glacier performs regular, systematic data integrity checks, and it is designed to be automatically self-healing
Using Amazon S3 Glacier
	0.	To store and access data in Amazon S3 Glacier, you can use the AWS Management Console. However, only a few operations -such as creating and deleting vaults, and creating and managing archive policies -are available in the console.
	0.	For almost all other operations and interactions with Amazon S3 Glacier, you must use either the Amazon S3 Glacier REST APIs, the AWS Java or .NET SDKs, or the AWS CLI.
	0.	You can also use lifecycle policies to archive data into Amazon S3 Glacier. Next, you will learn about lifecycle policies.
Lifecycle policies
	0.	You should automate the lifecycle of the data that you store in Amazon S3. By using lifecycle policies, you can cycle data at regular intervals between different Amazon S3 storage types. This automation reduces your overall cost, because you pay less for data as it becomes less important with time.
	0.	In addition to setting lifecycle rules per object, you can also set lifecycle rules per bucket.
	0.	Consider an example of a lifecycle policy that moves data as it ages from Amazon S3 Standard to Amazon S3 Standard - Infrequent Access, and finally, into Amazon S3 Glacier before it is deleted. Suppose that a user uploads a video to your application and your application generates a thumbnail preview of the video. This video preview is stored to Amazon S3 Standard, because it is likely that the user wants to access it right away.
	0.	Your usage data indicates that most thumbnail previews are not accessed after 30 days. Your lifecycle policy takes these previews and moves them to Amazon S3 - Infrequent Access after 30 days. After another 30 days elapse, the preview is unlikely to be accessed again. The preview is then moved to Amazon S3 Glacier, where it remains for 1 year. After 1 year, the preview is deleted. The important thing is that the lifecycle policy manages all this movement automatically.
	0.	To learn more about object lifecycle management, see http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html
Storage comparison
	0.	While Amazon S3 and Amazon S3 Glacier are both object storage solutions that enable you to store a virtually unlimited amount of data, they have some critical differences between them. The chart outlines some of these differences.
	0.	Be careful when you decide which storage solution is correct for your needs. These two services serve very different storage needs. Amazon S3 is designed for frequent, low-latency access to your data, but Amazon S3 Glacier is designed for low-cost, long-term storage of infrequently accessed data.
	0.	The maximum item size in Amazon S3 is 5 TB, but Amazon S3 Glacier can store items that are up to 40 TB.
	0.	Because Amazon S3 gives you faster access to your data, the storage cost per gigabyte is higher than it is with Amazon S3 Glacier.
	0.	While both services have per-request charges, Amazon S3 charges for PUT, COPY, POST, LIST, GET operations. In contrast, Amazon S3 Glacier charges for UPLOAD and retrieval operations.
	0.	Because Amazon S3 Glacier was designed for less-frequent access to data, it costs more for each retrieval request than Amazon S3.
Server-side encryption
	0.	Another important difference between Amazon S3 and Amazon S3 Glacier is how data is encrypted. Server-side encryption is focused on protecting data at rest. With both solutions, you can securely transfer your data over HTTPS. Any data that is archived in Amazon S3 Glacier is encrypted by default. With Amazon S3, your application must initiate server-side encryption. You can accomplish server-side encryption in Amazon S3 in several ways:
	•	Server-side encryption with Amazon S3-managed encryption keys (SSE-S3) employs strong multi-factor encryption. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key with a main key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.
	•	Using server-side encryption with Customer-provided Encryption Keys (SSE-C enables you to set your own encryption keys. You include the encryption key as part of your request, and Amazon S3 manages both encryption (as it writes to disks), and decryption (when you access your objects).
	•	Using server-side encryption with AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system that is scaled for the cloud. AWS KMS uses Customer Master Keys (CMKs) to encrypt your Amazon S3 objects. You use AWS KMS through the Encryption Keys section in the IAM console. You can also access AWS KMS through the API to centrally create encryption keys, define the policies that control how keys can be used, and audit key usage to prove that they are being used correctly. You can use these keys to protect your data in Amazon S3 buckets.
Security with Amazon S3 Glacier
	0.	By default, only you can access your data. You can enable and control access to your data in Amazon S3 Glacier by using IAM. You set up an IAM policy that specifies user access.
Quiz
	0.	It’s true that S3 is an object storage suitable for the storage of ‘flat’ files like Word documents, photos, etc.
	0.	S3 replicates all objects in multiple availability zones within the same region
	0.	Glacier, S3 infrequent Access and S3 Standard Access can be used as a storage class for an S3 object lifecycle policy.
	0.	The name of an S3 bucket must be unique worldwide across all AWS accounts
	0.	You can use EFS to implement storage for EC2 instances that multiple virtual machines can access at the same time
	0.	Amazon EBS is recommended when data must be quickly accessible, requiring long-term persistence and requires an encryption solution
	0.	False (instances are private when they are first created and can’t be viewed publicly.)
	0.	The vault in Amazon Glacier is a container for storing archives.
	0.	When you create a bucket in Amazon S3, it is associated with specific AWS region.
	0.	Amazon EBS volumes persist when the instance is stopped. The data is automatically replicated within an Availability Zone, can be encrypted upon creation and used by an instance, as if they were not encrypted.

W8

Section 1: Amazon Relational Database Service

Amazon Relational Database Service
	0.	Welcome to an introduction to the foundational database services that are available on Amazon Web Services (AWS). This module begins with Amazon Relational Database Service (Amazon RDS).
	0.	This section starts by reviewing the differences between a managed and unmanaged service in relation to Amazon RDS.
Unmanaged verses managed services
	0.	AWS solutions typically fall into one of two categories: unmanaged or managed.
	0.	Unmanaged services are typically provisioned in discrete portions as specified by the user. You must manage how the service responds to changes in load, errors, and situations where resources become unavailable. Say that you launch a web server on an Amazon Elastic Compute Cloud (Amazon EC2) instance. Because Amazon EC2 is an unmanaged solution, that web server will not scale to handle increased traffic load or replace unhealthy instances with healthy ones unless you specify that it use a scaling solution, such as AWS Automatic Scaling. The benefit to using an unmanaged service is that you have more fine-tuned control over how your solution handles changes in load, errors, and situations where resources become unavailable.
	0.	Managed services require the user to configure them. For example, you create an Amazon Simple Storage Service (Amazon S3) bucket and then set permissions for it. However, managed services typically require less configuration. Say that you have a static website that you host in a cloud-based storage solution, such as Amazon S3. The static website does not have a web server. However, because Amazon S3 is a managed solution, features such as scaling, fault-tolerance, and availability would be handled automatically and internally by Amazon S3.
	0.	Now, you will look at the challenges of running an unmanaged, standalone relational database. Then, you will learn how Amazon RDS addresses these challenges.
Challenges of relational databases
	0.	When you run your own relational database, you are responsible for several administrative tasks, such as server maintenance and energy footprint, software, installation and patching, and database backups. You are also responsible for ensuring high availability, planning for scalability, data security, and operating system (OS) installation and patching. All these tasks take resources from other items on your to-do list, and require expertise in several areas.
Amazon RDS
	0.	Amazon RDS is a managed service that sets up and operates a relational database in the cloud.
	0.	To address the challenges of running an unmanaged, standalone relational database, AWS provides a service that sets up, operates, and scales the relational database without any ongoing administration. Amazon RDS provides cost-efficient and resizable capacity, while automating time-consuming administrative tasks.
	0.	Amazon RDS enables you to focus on your application, so you can give applications the performance, high availability, security, and compatibility that they need. With Amazon RDS, your primary focus is your data and optimizing your application.
From on-premises databases to Amazon RDS
	0.	What does the term managed services mean?
	0.	When your database is on premises, the database administrator is responsible for everything. Database administration tasks include optimizing applications and queries; setting up the hardware; patching the hardware; setting up networking and power; and managing heating, ventilation, and air conditioning (HVAC).
	0.	If you move to a database that runs on an Amazon Elastic Compute Cloud (Amazon EC2) instance, you no longer need to manage the underlying hardware or handle data center operations. However, you are still responsible for patching the OS and handling all software and backup operations.
	0.	If you set up your database on Amazon RDS or Amazon Aurora, you reduce your administrative responsibilities. By moving to the cloud, you can automatically scale your database, enable high availability, manage backups, and perform patching. Thus, you can focus on what really matters most-optimizing your application.
Managed services responsibilities
	0.	With Amazon RDS, you manage your application optimization. AWS manages installing and patching the operating system, installing and patching the database software, automatic backups, and high availability.
	0.	AWS also scales resources, manages power and servers, and performs maintenance.
	0.	Offloading these operations to the managed Amazon RDS service reduces your operational workload and the costs that are associated with your relational database. You will now go through a brief overview of the service and a few potential use cases.
Amazon RDS DB instances
	0.	The basic building block of Amazon RDS is the database instance. A database instance is an isolated database environment that can contain multiple user-created databases. It can be accessed by using the same tools and applications that you use with a standalone database instance. The resources in a database instance are determined by its database instance class, and the type of storage is dictated by the type of disks.
	0.	Database instances and storage differ in performance characteristics and price, which enable you to customize your performance and cost to the needs of your database. When you choose to create a database instance, you must first specify which database engine to run. Amazon RDS currently supports six databases: MySQL, Amazon Aurora, Microsoft SQL Server, PostgreSQL, MariaDB, and Oracle.
Amazon RDS in virtual private cloud (VPC)
	0.	You can run an instance by using Amazon Virtual Private Cloud (Amazon VPC). When you use a virtual private cloud (VPC), you have control over your virtual networking environment.
	0.	You can select your own IP address range, create subnets, and configure routing and access control lists (ACLs). The basic functionality of Amazon RDS is the same whether or not it runs in a VPC. Usually, the database instance is isolated in a private subnet and is only made directly accessible to indicated application instances. Subnets in a VPC are associated with a single Availability Zone, so when you select the subnet, you are also choosing the Availability Zone (or physical location) for your database instance.
High availability with Multi-AZ deployment (1 of 2)
	0.	One of the most powerful features of Amazon RDS is the ability to configure your database instance for high availability with a Multi-AZ deployment. After a Multi-AZ deployment is configured, Amazon RDS automatically generates a standby copy of the database instance in another Availability Zone within the same VPC. After seeding the database copy, transactions are synchronously replicated to the standby copy. Running a database instance in a Multi-AZ deployment can enhance availability during planned system maintenance, and it can help protect your databases against database instance failure and Availability Zone disruption.
High availability with Multi-AZ deployment (2 of 2)
	0.	Therefore, if the main database instance fails in a Multi-AZ deployment, Amazon RDS automatically brings the standby database instance online as the new main instance. The synchronous replication minimizes the potential for data loss. Because your applications reference the database by name by using the Amazon RDS Domain Name System (DNS) endpoint, you don't need to change anything in your application code to use the standby copy for failover.
Amazon RDS read replies
	0.	Amazon RDS also supports the creation of read replicas for MySQL, MariaDB, PostgreSQL, and Amazon Aurora. Updates that are made to the source database instance are asynchronously copied to the read replica instance. You can reduce the load on your source database instance by routing read queries from your applications to the read replica. Using read replicas, you can also scale out beyond the capacity constraints of a single database instance for read-heavy database workloads. Read replicas can also be promoted to become the primary database instance, but this requires manual action because of asynchronous replication.
	0.	Read replicas can be created in a different Region than the primary database. This feature can help satisfy disaster recovery requirements or reduce latency by directing reads to a read replica that is closer to the user.
Use Cases
	0.	Amazon RDS works well for web and mobile applications that need a database with high throughput, massive storage scalability, and high availability. Because Amazon RDS does not have any licensing constraints, it fits the variable usage pattern of these applications. For small and large ecommerce businesses, Amazon RDS provides a flexible, secure, and low-cost database solution for online sales and retailing. Mobile and online games require a database platform with high throughput and availability. Amazon RDS manages the database infrastructure, so game developers do not need to worry about provisioning, scaling, or monitoring database servers.
When to use Amazon RDS
	0.	Use Amazon RDS when your application requires:
	•	Complex transactions or complex queries
	•	A medium to high query or write rate - up to 30,000 lOPS (15,000 reads + 15,000 writes)
	•	No more than a single worker node or shard
	•	High durability
	0.	Do not use Amazon RDS when your application requires:
	•	Massive read/write rates (for example 150,000 writes per second)
	•	Sharding due to high data size or throughput demands
	•	Simple GET or PUT requests and queries that a NoSQL database can handle
	•	Or, relational database management system (RDBMS) customization
	0.	For circumstances when you should not use Amazon RDS, consider either using a NoSQ database solution (such as DynamoDB) or running your relational database engine on Amazon EC2 instances instead of Amazon RDS (which will provide you with more options for customizing your database).
Amazon RDS clock-hour billing and database characteristics
	0.	When you begin to estimate the cost of Amazon RDS, you must consider the clock hours of service time, which are resources that incur charges when they are running (for example, from the time you launch a database instance until you terminate the instance).
	0.	Database characteristics should also be considered. The physical capacity of the database you choose will affect how much you are charged. Database characteristics vary depending on the database engine, size, and memory class.
Amazon RDS: DB purchase type and multiple DB instances
	0.	Consider the database purchase type. When you use On-Demand Instances, you pay for compute capacity for each hour that your database instance runs, with no required minimum commitments. With Reserved Instances, you can make a low, one-time, upfront payment for each database instance you want to reserve for a 1-year or 3-year term.
	0.	Also, you must consider the number of database instances. With Amazon RDS, you can provision multiple database instances to handle peak loads.
Amazon RDS: storage
	0.	Consider provisioned storage. There is no additional charge for backup storage of up to 100 percent of your provisioned database storage for an active database instance. After the database instance is terminated, backup storage is billed per GB, per month.
	0.	Also consider the amount of backup storage in addition to the provisioned storage amount, which is billed per GB, per month.
Amazon RDS: Deployment type and data transfer
	0.	Also consider the number of input and output requests that are made to the database.
	0.	Consider the deployment type. You can deploy your DB instance to a single Availability Zone (which is analogous to a standalone data center) or to multiple Availability Zones (which is analogous to a secondary data center for enhanced availability and durability). Storage and 1/O charges vary, depending on the number of Availability Zones that you deploy to.
	0.	Finally, consider data transfer. Inbound data transfer is free, and outbound data transfer costs are tiered.
	0.	Depending on the needs of your application, it's possible to optimize your costs for Amazon RDS database instances by purchasing Reserved Instances. To purchase Reserved Instances, you make a low, one-time payment for each instance that you want to reserve. As a result, you receive a significant discount on the hourly usage charge for that instance.
Section 2: Amazon DynamoDB

Relational versus non-relational databases
	0.	With DynamoDB, this module transitions from relational databases to non-relational databases.
	0.	Here is a review of the differences between these two types of databases:
	•	A relational database (RDB) works with structured data that is organized by tables, records, and columns. RDBs establish a well-defined relationship between database tables. RDBs use structured query language (SQL), which is a standard user application that provides a programming interface for database interaction. Relational databases might have difficulties scaling out horizontally or working with semistructured data, and might also require many joins for normalized data.
	•	A non-relational database is any database that does not follow the relational model that is provided by traditional relational database management systems (RDBMS). Non-relational databases have grown in popularity because they were designed to overcome the limitations of relational databases for handling the demands of variable structured data. Non-relational databases scale out horizontally, and they can work with unstructured and semistructured data.
	0.	Here is a look at what DynamoDB offers.

What is Amazon DynamoDB
	0.	DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit-millisecond latency at any scale.
	0.	Amazon manages all the underlying data infrastructure for this service and redundantly stores data across multiple facilities in a native US Region as part of the fault-tolerant architecture. With DynamoDB, you can create tables and items. You can add items to a table. The system automatically partitions your data and has table storage to meet workload requirements. There is no practical limit on the number of items that you can store in a table. For instance, some customers have production tables that contain billions of items.
	0.	One of the benefits of a NoSQL database is that items in the same table can have different attributes. This gives you the flexibility to add attributes as your application evolves. You can store newer format items side by side with older format items in the same table without needing to perform schema migrations.
	0.	As your application becomes more popular and as users continue to interact with it, your storage can grow with your application's needs. All the data in DynamoDB is stored on solid state drives (SSDs) and its simple query language enables consistent low-latency query performance. In addition to scaling storage, DynamoDB also enables you to provision the amount of read or write throughput that you need for your table. As the number of application users grows, DynamoDB tables can be scaled to handle the increased numbers of read/write requests with manual provisioning. Alternatively, you can enable automatic scaling so that DynamoDB monitors the load on the table and automatically increases or decreases the provisioned throughput.
	0.	Some additional key features include global tables that enable you to automatically replicate across your choice of AWS Regions, encryption at rest, and item Time-to-Live (TTL).
	0.	Characteristics
	•	NoSQL database tables
	•	Virtually unlimited storage
	•	Items can have differing
	•	attributes
	•	Low-latency queries
	•	Scalable read/write throughput
Amazon DynamoDB core components
	0.	The core DynamoDB components are tables, items, and attributes.
	•	A table is a collection of data.
	•	Items are a group of attributes that is uniquely identifiable among all the other items.
	•	Attributes are a fundamental data element, something that does not need to be broken down any further.
	0.	DynamoDB supports two different kinds of primary keys.
	0.	The partition key is a simple primary key, which is composed of one attribute called the sort key. The partition key and sort key are also known as the composite primary key, which is composed of two attributes.
	0.	To learn more about how DynamoDB works, see table item attributes at https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowltWorks.CoreComp onents.html#HowltWorks.CoreComponents.TablesItemsAttributes.
Partitioning
	0.	As data grows, table data is partitioned and indexed by the primary key.
	0.	You can retrieve data from a DynamoDB table in two different ways:
	•	In the first method, the query operation takes advantage of partitioning to effectively locate items by using the primary key.
	•	The second method is via a scan, which enables you to locate items in the table by matching conditions on non-key attributes. The second method gives you the flexibility to locate items by other attributes. However, the operation is less efficient because DynamoDB will scan through all the items in the table to find the ones that match your criteria.
	0.	For accessibility: Partitioning allows large tables to be scanned and queried quickly. As data grows, table is partitioned by key. QUERY by Key to find items by any attribute. End of accessibility description.
Items in a table must have a key
	0.	To take full advantage of query operations and DynamoDB, it's important to think about the key that you use to uniquely identify items in the DynamoDB table. You can set up a simple primary key that is based on a single attribute of the data values with a uniform distribution, such as the Globally Unique Identifier (GUID) or other random identifiers.
	0.	For example, if you wanted to model a table with products, you could use some attributes like the product ID. Alternatively, you can specify a compound key, which is composed of a partition key and a secondary key. In this example, if you had a table with books, you might use the combination of author and title to uniquely identify table items. This method could be useful if you expect to frequently look at books by author because you could then use query.
	0.	For accessibility: The two different types of keys. A single key means the data is identified by an item in the data that uniquely identifies each record. A compound key is made up of a partition key and a second key that can be used for sorting data. End of accessibility description.
Section 3: Amazon Redshift

Amazon Redshift
	0.	Amazon Redshift is a fast, fully managed data warehouse that makes it simple and cost-effective to analyze all your data by using standard SQL and your existing business intelligence (BI) tools. Here is a look at Amazon Redshift and how you can use it for analytic applications.
Introduction to Amazon Redshift
	0.	Analytics is important for businesses today, but building a data warehouse is complex and expensive. Data warehouses can take months and significant financial resources to set up.
	0.	Amazon Redshift is a fast and powerful, fully managed data warehouse that is simple and cost-effective to set up, use, and scale. It enables you to run complex analytic queries against petabytes of structured data by using sophisticated query optimization, columnar storage on high-performance local disks, and massively parallel data processing. Most results come back in seconds.
	0.	You will next review a slightly more detailed exploration of key Amazon Redshift features and some common use cases.
Parallel processing architecture
	0.	The leader node manages communications with client programs and all communication with compute nodes. It parses and develops plans to carry out database operations-specifically, the series of steps that are needed to obtain results for complex queries. The leader node compiles code for individual elements of the plan and assigns the code to individual compute nodes. The compute nodes run the compiled code and send intermediate results back to the leader node for final aggregation.
	0.	Like other AWS services, you only pay for what you use. You can get started for as little as 25 cents per hour and, at scale, Amazon Redshift can deliver storage and processing for approximately $1,000 dollars per terabyte per year (with 3-Year Partial Upfront Reserved Instance pricing).
	0.	The Amazon Redshift Spectrum feature enables you to run queries against exabytes of data directly in Amazon S3.
Automation and Scaling
	0.	It is straightforward to automate most of the common administrative tasks to manage, monitor, and scale your Amazon Redshift cluster-which enables you to focus on your data and your business.
	0.	Scalability is intrinsic in Amazon Redshift. Your cluster can be scaled up and down as your needs change with a few clicks in the console.
	0.	Security is the highest priority for AWS. With Amazon Redshift, security is built in, and it is designed to provide strong encryption of your data both at rest and in transit.
Compatibility
	0.	Finally, Amazon Redshift is compatible with the tools that you already know and use. Amazon Redshift supports standard SQL. It also provides high-performance Java Database Connectivity (JDBC) and Open Database Connectivity (ODBC) connectors, which enable you to use the SQL clients and BI tools of your choice.
	0.	Next, you will review some common Amazon Redshift use cases.
Amazon Redshift use cases (1 of 2)
	0.	This slide discusses some Amazon Redshift use cases.
	0.	Many customers migrate their traditional enterprise data warehouses to Amazon Redshift with the primary goal of agility. Customers can start at whatever scale they want and experiment with their data without needing to rely on complicated processes with their IT departments to procure and prepare their software.
	0.	Big data customers have one thing in common: massive amounts of data that stretch their existing systems to a breaking point. Smaller customers might not have the resources to procure the hardware and expertise that is needed to run these systems. With Amazon Redshift, smaller customers can quickly set up and use a data warehouse at a comparatively low price point.
	0.	As a managed service, Amazon Redshift handles many of the deployment and ongoing maintenance tasks that often require a database administrator. This enables customers to focus on querying and analyzing their data.
Amazon Redshift use cases (2 of 2)
	0.	Software as a service (SaaS) customers can take advantage of the scalable, easy-to-manage features that Amazon Redshift provides. Some customers use the Amazon Redshift to provide analytic capabilities to their applications. Some users deploy a cluster per customer, and use tagging to simplify and manage their service level agreements (SLAs) and billing. Amazon Redshift can help you reduce hardware and software costs.
Section 4: Amazon Aurora

Amazon Aurora
	0.	Amazon Aurora is a MySQL- and PostgreSQL-compatible relational database that is built for the cloud. It combines the performance and availability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases. Using Amazon Aurora can reduce your database costs while improving the reliability and availability of the database. As a fully managed service, Aurora is designed to automate time-consuming tasks like provisioning, patching, backup, recovery, failure detection, and repair.
Amazon Aurora service benefits
	0.	This slide covers some of the benefits of Amazon Aurora. It is highly available and it offers a fast, distributed storage subsystem. Amazon Aurora is straightforward to set up and uses SQL queries. It is designed to have drop-in compatibility with MySQL and PostgreSQL database engines so that you can use most of your existing database tools with little or no change.
	0.	Amazon Aurora is a pay-as-you-go service, which means that you only pay for the services and features that you use. It's a managed service that integrates with features such as AWS Database Migration Service (AWS DMS) and the AWS Schema Conversion Tool. These features are designed to help you move your dataset into Amazon Aurora.
High availability
	0.	Why might you use Amazon Aurora over other options, like SQL with Amazon RDS? Most of that decision involves the high availability and resilient design that Amazon Aurora offers.
	0.	Amazon Aurora is designed to be highly available: it stores multiple copies of your data across multiple Availability Zones with continuous backups to Amazon S3. Amazon Aurora can use up to 15 read replicas can be used to reduce the possibility of losing your data. Additionally, Amazon Aurora is designed for instant crash recovery if your primary database becomes unhealthy.
Resilient design
	0.	After a database crash, Amazon Aurora does not need to replay the redo log from the last database checkpoint. Instead, it performs this on every read operation. This reduces the restart time after a database crash to less than 60 seconds in most cases.
	0.	With Amazon Aurora, the buffer cache is moved out of the database process, which makes it available immediately at restart. This reduces the need for you to throttle access until the cache is repopulated to avoid brownouts.
Quiz
	0.	NoSQL databases like Amazon Dynamo DB excel at scaling to hundreds of thousands of requests with key/value access to user profile and session.
	0.	To find an item in a DynamoDB table other than the item’s primary key, you would use the scan operation.
	0.	In Amazon DynamoDB, the query operation allows you to do all these things
	0.	Amazon Redshift is best suited for analyzing data.
	0.	In Amazon DynamoDB, an attribute is a fundamental data element
	0.	If you are developing an application that requires a database with extremely fast performance, fast scalability, and flexibility in the database schema, consider Amazon DynamoDB
	0.	Use Amazon RDS when your application requires complex transactions or complex queries.
	0.	Amazon Aurora is a MySQL - and PostgreSQL - compatible relational database that would be ideal for this use case.
	0.	Amazon RDS automatically patches the database software and backs up.
	0.	When choosing a database type, you should consider all of these
Section 1: AWS Well-architected framework

Architecture: designing and building
	0.	Architecture is the art and science of designing and building large structures. Large systems require architects to manage their size and complexity. Cloud architects:
	•	Engage with decision makers to identify the business goal and the capabilities that need improvement.
	•	Ensure alignment between technology deliverables of a solution and the business goals.
	•	Work with delivery teams that are implementing the solution to ensure that the technology features are appropriate.
	0.	Having well-architected systems greatly increases the likelihood of business success.
What is the AWS Well-Architected Framework
	0.	The AWS Well-Architected Framework is a guide that is designed to help you build the most secure, high-performing, resilient, and efficient infrastructure possible for your cloud applications and workloads. It provides a set of foundational questions and best practices that can help you evaluate and implement your cloud architectures. AWS developed the Well-Architected Framework after reviewing thousands of customer architectures on AWS.
Pillars of the AWS Well-architectured framework
	0.	The AWS Well-Architected Framework is organized into six pillars: operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. The first five pillars have been part of the framework since the framework's introduction in 2015. The sustainability pillar was added as the sixth pillar in 2021 to help organizations learn how to minimize the environmental impacts of running cloud workloads.
	0.	The remainder of this module focuses on the first five pillars (operational excellence, security, reliability, performance efficiency, cost optimization) and leads you through a review of an example architecture against each pillar's design principles. For more about the sustainability pillar, refer to the sustainability pillar section within the Well-Architected Framework documentation see https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html.
	0.	For accessibility: The pillars include operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. End of accessibility description.
Pillar organization
	0.	Each pillar includes a set of design principles and best practice areas. Each best practice area aligns to questions a reviewer should ask when designing an architecture. The questions for each pillar are part of the Well-Architected Framework Appendix.
AnyCompany Background
	0.	Here's the background of the company whose architecture you will be reviewing:
	0.	AnyCompany Corporation was founded in 2008 by John Doe. It sells high-quality three-dimensional (3D) printed cityscapes of neighborhoods that enable you to see individual buildings and trees. The cityscapes are printed in color, with brickwork, roofs, gardens, and even cars in their correct coloration.
	0.	The company is about to apply for private investment to fund their growth until their initial public offering (IPO). John and the board have asked you to perform an independent review of their technology platform to make sure that it will pass due diligence.
	0.	John was interested in using cloud computing from the start. In 2008, he created an account with AWS and spun up his first Amazon Elastic Compute Cloud (Amazon EC2) instance. Over the years, the architecture of the AnyCompany platform has evolved. John now has a team of five technologists who write and operate all the technology in the organization. John still writes core code for extracting structure from motion, but he has given the AWS account root user credentials to the rest of his team to manage.
AnyCompany background (continued)
	0.	AnyCompany Corporation has three main departments:
	•	  Fly and Snap - image acquisition, preprocessing, and storage
	•	  Show and Sell - promoting, selling, and working with customers
	•	  Make and Ship - manufacturing of products and delivery
	0.	The high-level design for the AnyCompany platform looks like the organizational structure of the company.
	0.	For accessibility: High-level design of mappahood platform: Show and Sell (promoting, selling, working with customers) sends orders and requests imagery. Make and Ship (manufacturing and delivery tracks orders from Show and Sell and requests imagery from Fly and Snap (acquisition, preprocessing, and storage. End of accessibility description.
AnyCompany architecture: Fly and Snap
	0.	Fly and Snap
	0.	Multiple devices (currently, camera and video cameras) are mounted on lightweight aircraft that capture imagery of major cities, including famous locations, on a scheduled basis. Each device generates imagery assets that are time-stamped with a clock that is synchronized with the aircraft's clock. The imagery assets are streamed to the onboard Capture machine that has an external storage array. The Capture machine is also connected to the aircraft's flight system and continuously captures navigation data-such as global positioning system (GPS) data, compass readings, and elevation.
	0.	When it returns to base, the storage array is disconnected and taken into an ingest bay. Here, the storage array is connected to an Ingest machine. The Ingest machine creates a compressed archive of the storage array and uses file transfer protocol (FTP) to send it to an EC2 instance Preprocessor machine. After the storage array has been processed, the archive is written to tape (for backup). The storage array is then cleared and ready for the next flight. Tapes are held offsite by a third-party backup provider.
	0.	The Preprocessor machine periodically processes new datasets that have been uploaded to it. It extracts all the imagery assets and stores them in an Amazon Simple Storage Service (Amazon S3) bucket. It notifies the Imagery service about the files and provides it with the flight information. The Imagery service uses the flight information to compute a 3D orientation and location for every moment of the flight, which it correlates to the imagery file timestamps. This information is stored in a relational database management system (RDBMS) that is based in Amazon EC2, with links to the imagery assets in Amazon S3.
	0.	For accessibility: Fly and snap architecture. A capture machine and an ingest machine sends imagery to a detachable storage array. The ingest machine also backs up to tape storage. End of accessibility description
Any Company architecture: show and sell
	0.	Show and Sell
	0.	When customers visit the AnyCompany website, they can see images and videos of the physical product. These images are in a variety of formats (for example, a large-scale, walk-around map). The website uses Elastic Load Balancing with Hypertext Transfer Protocol Secure (HTTPS), and an Auto Scaling group of EC2 instances that run a content management system. Static website assets are stored in an S3 bucket.
	0.	Customers can select a location on a map and see a video preview of their cityscape. Customers can also choose the physical size of the map, choose the color scheme (available in white, monochrome, or full color), and have the option to place light-emitting diode (LED) holes in the map to build illuminated maps. The Mapping service correlates the map location input from the website with the Imagery service to confirm if imagery is available for that location.
	0.	If the customers are happy with the preview, they can order their cityscape. Customers pay by credit card. Credit card orders are processed by a certified third-party payment card industry (PCI)-compliant provider. AnyCompany does not process or store any credit card information.
	0.	After the website receives payment confirmation, it instructs the Order service to push the order to production. Orders (including customer details) are recorded in the Show and Sell database, which is an RDBMS that is based in Amazon EC2.
	0.	To initiate a video preview or full print of an order, the Orders service places a message on the Production queue, which allows the Render service to indicate when a preview video is available. The Order service also reads from the Order status queue and records status changes in the Show and Sell database. Customers can track their order through manufacturing and see when it has been dispatched, which is handled by a third party through the broker Dispatch service
Anycompany architecture: make and ship
	0.	Make and Ship
	0.	AnyCompany has proprietary technology that enables it to generate 3D models from a combination of photographs and video (extracting structure from motion).
	0.	The Render service is a fleet of g2.2xlarge instances. The Render service takes orders from the Production queue and generates the 3D models that are stored in an S3 bucket. The Render service also uses the 3D models to create flyby videos so that customers can preview their orders on the AnyCompany website. These videos are stored in a separate S3 bucket. Once a year, the team deletes old previews. However, models are kept in case they are needed for future projects.
	0.	After a customer places an order, a message is placed in the Print queue with a link to the 3D model. At each stage of the Make and Ship process, order status updates are posted to the Order status queue. This queue is consumed by the AnyCompany website, which shows the order history.
	0.	The Make and Ship team has four 3D printers that print high-resolution and detailed color-control models. An on-premises Print conductor machine takes orders from the Print queue and sends them to the next available printer. The Print conductor sends order updates to the Order status queue. The Print conductor sends a final update when the order has been completed, passed quality assurance, and is ready for dispatch.
Operational excellence pillar - deliver business value
	0.	The Operational Excellence pillar focuses on the ability to run and monitor systems to deliver business value, and to continually improve supporting processes and procedures. Key topics include: automating changes, responding to events, and defining standards to manage daily operations.
Operational excellence design principal
	0.	There are five design principles for operational excellence in the cloud:
	•	Perform operations as code - Define your entire workload (that is, applications and infrastructure) as code and update it with code. Implement operations procedures as code and configure them to automatically trigger in response to events. By performing operations as code, you limit human error and enable consistent responses to events.
	•	Make frequent, small, reversible changes - Design workloads to enable components to be updated regularly. Make changes in small increments that can be reversed if they fail (without affecting customers when possible).
	•	Refine operations procedures frequently - Look for opportunities to improve operations procedures. Evolve your procedures appropriately as your workloads evolve. Set up regular game days to review all procedures, validate their effectiveness, and ensure that teams are familiar with them.
	•	Anticipate failure - Identify potential sources of failure so that they can be removed or mitigated. Test failure scenarios and validate your understanding of their impact. Test your response procedures to ensure that they are effective and that teams know how to run them. Set up regular game days to test workloads and team responses to simulated events.
	•	Learn from all operational failures - Drive improvement through lessons learned from all operational events and failures. Share what is learned across teams and through the entire organization.
Operational excellence questions
	0.	The foundational questions for operational excellence fall under three best practice areas: organization, prepare, operate, and evolve.
	0.	Operations teams must understand business and customer needs so they can effectively and efficiently support business outcomes. Operations teams create and use procedures to respond to operational events and validate the effectiveness of procedures to support business needs. Operations teams collect metrics that are used to measure the achievement of desired business outcomes. As business context, business priorities, and customer needs, change over time, it's important to design operations that evolve in response to change and to incorporate lessons learned through their performance.
Security Pillar - protect and monitor systems
	0.	The Security pillar focuses on the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. Key topics include: protecting confidentiality and integrity of data, identifying and managing who can do what (or privilege management), protecting systems, and establishing controls to detect security events.
Security design principals
	0.	There are seven design principles that can improve security:
	•	  Implement a strong identity foundation - Implement the principle of least privilege and enforce separation of duties with appropriate authorization for each interaction with yourAWS resources. Centralize privilege management and reduce or even eliminate reliance on long-term credentials.
	•	  Enable traceability - Monitor, alert, and audit actions and changes to your environment in real time. Integrate logs and metrics with systems to automatically respond and take action.
	•	  Apply security at all layers - Apply defense in depth and apply security controls to all layers of your architecture (for example, edge network, virtual private cloud, subnet, and load balancer; and every instance, operating system, and application).
	•	  Automate security best practices - Automate security mechanisms to improve your ability to securely scale more rapidly and cost effectively. Create secure architectures and implement controls that are defined and managed as code in version-controlled templates.
	•	  Protect data in transit and at rest - Classify your data into sensitivity levels and use mechanisms such as encryption, tokenization, and access control where appropriate.
	•	  Keep people away from data - To reduce the risk of loss or modification of sensitive data due to human error, create mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data.
	•	  Prepare for security events - Have an incident management process that aligns with organizational requirements. Run incident response simulations and use tools with automation to increase your speed of detection, investigation, and recovery.
Security questions
	0.	The foundational questions for security fall under six best practice areas: security, identity and access management, detection, infrastructure protection, data protection, and incident response.
	0.	Before you architect any system, you must put security practices in place. You must be able to control who can do what. In addition, you must be able to identify security incidents, protect your systems and services, and maintain the confidentiality and integrity of data through data protection. You should have a well-defined and practiced process for responding to security incidents. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations.
Reliability pillar - recover from failure and migrate disruption
	0.	The Reliability pillar focuses on ensuring a workload performs its intended function correctly and consistently when it's expected to. A resilient workload quickly recovers from failures to meet business and customer demand. Key topics include: designing distributed systems, recovery planning, and handling change.
Reliability design principles
	0.	There are five design principles that can increase reliability:
	•	  Automatically recover from failure - Monitor systems for key performance indicators and configure your systems to trigger an automated recovery when a threshold is breached. This practice enables automatic notification and failure-tracking, and for automated recovery processes that work around or repair the failure.
	•	  Test recovery procedures - Test how your systems fail and validate your recovery procedures.Use automation to simulate different failures or to recreate scenarios that led to failures before. This practice can expose failure pathways that you can test and rectify before a real failure scenario.
	•	  Scale horizontally to increase aggregate workload availability - Replace one large resource with multiple, smaller resources and distribute requests across these smaller resources to reduce the impact of a single point of failure on the overall system.
	•	  Stop guessing capacity - Monitor demand and system usage, and automate the addition or removal of resources to maintain the optimal level for satisfying demand.
	•	  Manage change in automation - Use automation to make changes to infrastructure and manage changes in automation.
Reliability questionnaire
	0.	The foundational questions for reliability fall under four best practice areas: foundations, workload architecture, change management, and failure management.
	0.	To achieve reliability, a system must have both a well-planned foundation and monitoring in place. It must have mechanisms for handling changes in demand or requirements. The system should be designed to detect failure and automatically heal itself.
Performance efficiency pillar - use resources sparingly
	0.	The Performance Efficiency pillar focuses on the ability to use IT and computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes or technologies evolve. Key topics include: selecting the right resource types and sizes based on workload requirements, monitoring performance, and making informed decisions to maintain efficiency as business needs evolve.
Performance efficiency design principles
	0.	There are five design principles that can improve performance efficiency:
	•	  Democratize advanced technologies - Consume technologies as a service. For example, technologies such as NoSQL databases, media transcoding, and machine learning require expertise that is not evenly dispersed across the technical community. In the cloud, these technologies become services that teams can consume. Consuming technologies enables teams to focus on product development instead of resource provisioning and management.
	•	  Go global in minutes - Deploy systems in multiple AWS Regions to provide lower latency and a better customer experience at minimal cost.
	•	  Use serverless architectures - Serverless architectures remove the operational burden of running and maintaining servers to carry out traditional compute activities. Serverless architectures can also lower transactional costs because managed services operate at cloud scale.
	•	  Experiment more often - Perform comparative testing of different types of instances, storage, or configurations.
	•	  Consider mechanical sympathy - Use the technology approach that aligns best to what you are trying to achieve. For example, consider your data access patterns when you select approaches for databases or storage.
Performance efficiency questions
	0.	The foundational questions for performance efficiency fall under four best practice areas: selection, review, monitoring, and tradeoffs.
	0.	Use data to design and build a high-performance architecture. Gather data on all aspects of the architecture, from the high-level design to the selection and configuration of resource types. Review your choices periodically to ensure that you are taking advantage of new AWS services. Perform monitoring so that you are aware of any deviance from expected performance and can take prompt action to remediate them. Finally, use tradeoffs in your architecture to improve performance, such as using compression, using caching, or relaxing consistency requirements.
Cost optimization pillar - eliminate unneeded expense
	0.	The Cost Optimization pillar focuses on the ability to avoid unnecessary costs. Key topics include: understanding and controlling where money is being spent, selecting the most appropriate and right number of resource types, analyzing spend over time, and scaling to meeting business needs without overspending.
Cost optimization design principles
	0.	There are five design principles that can optimize costs:
	•	  Implement Cloud Financial Management - To achieve financial success and accelerate business value realization in the cloud, you need to invest in cloud financial management and cost optimization. You need to build capability through knowledge building, programs, resources, and processes to become a cost-efficient organization.
	•	  Adopt a consumption model - Pay only for the computing resources that you require. Increase or decrease usage depending on business requirements, not by using elaborate forecasting.
	•	  Measure overall efficiency - Measure the business output of the workload and the costs that are associated with delivering it. Use this measure to know the gains that you make from increasing output and reducing costs.
	•	  Stop spending money on undifferentiated heavy lifting - AWS does the heavy lifting of racking, stacking, and powering servers, which means that you can focus on your customers and business projects instead of the IT infrastructure.
	•	  Analyze and attribute expenditure - The cloud makes it easier to accurately identify system usage and costs, and attribute IT costs to individual workload owners. Having this capability helps you measure return on investment (ROl) and gives workload owners an opportunity to optimize their resources and reduce costs.
Cost optimization questions
	0.	The foundational questions for cost optimization fall under five best practice areas: practice cloud financial management, expenditure and usage awareness, cost-effective resources, manage demand and supply resources, and optimize over time.
	0.	Similar to the other pillars, there are tradeoffs to consider when evaluating cost. For example, you may choose to prioritize for speed -going to market quickly, shipping new features, or simply meeting a deadline-instead of investing in upfront cost optimization. As another example, designing an application for a higher level of availability typically costs more. You should identify your true application needs and use empirical data to inform your architectural design decisions. Perform benchmarking to establish the most cost-optimal workload over time.
The AWS well-architectures tool
	0.	The activity that you just completed is similar to how you would use the AWS Well-Architected Tool.
	0.	The AWS Well-Architected Tool helps you review the state of your workloads and compare them to the latest AWS architectural best practices. It gives you access to knowledge and best practices used by AWS architects, whenever you need it.
	0.	This tool is available in the AWS Management Console. You define your workload and answer a series of questions in the areas of operational excellence, security, reliability, performance efficiency, and cost optimization as defined in the AWS Well-Architected Framework). The AWS Well-Architected Tool then delivers an action plan with step-by-step guidance on how to improve your workload for the cloud.
	0.	The AWS Well-Architected Tool provides a consistent process for you to review and measure your cloud architectures. You can use the results that the tool provides to identify next steps for improvement, drive architectural decisions, and bring architecture considerations into your corporate governance process.

Section 2: Reliability and availability

Reliability
	0.	Reliability is a measure of your system's ability to provide functionality when desired by the user. Because "everything fails, all the time," you should think of reliability in statistical terms. Reliability is the probability that an entire system will function as intended for a specified period. Note that a system includes all system components, such as hardware, firmware, and software. Failure of system components impacts the availability of the system.
	0.	To understand reliability, it is helpful to consider the familiar example of a car. The car is the system. Each of the car's components (for example, cooling, ignition, and brakes) must work together in order for the car to work properly. If you try to start the car and the ignition fails, you cannot drive anywhere-the car is not available. If the ignition fails repeatedly, your car is not considered reliable. A common way to measure reliability is to use statistical measurements, such as Mean Time Between Failures (MTBF). MTBF is the total time in service over the number of failures.
Understanding reliability metrics
	0.	Say that you have an application that you bring online Monday at noon. The application is said to be available. It functions normally until it fails Friday at noon. Therefore, the time to failure (or the length of time the application is available) is 96 hours. You spend from Friday at noon until Monday at noon diagnosing why the application failed and repairing it, at which point you bring the application back online. Therefore, the time to repair is 72 hours.
	0.	Then, it happens again: the application fails on Friday at noon, you spend from Friday at noon until Monday at noon repairing it, and you bring it online on Monday at noon.
	0.	Say this failure-repair-restore cycle happens every week. You can now calculate the average of these numbers. In this example, your mean time to failure (MTTF) is 96 hours, and your mean time to repair (MTTR) is 72 hours. Your mean time between failures (MTBF) is 168 hours or 1 week), which is the sum of MTTF and MTTR.
Availability
	0.	As you just learned, failure of system components impacts the availability of the system.
	0.	Formally, availability is the percentage of time that a system is operating normally or correctly performing the operations expected of it (or normal operation time over total time). Availability is reduced anytime the application isn't operating normally, including both scheduled and unscheduled interruptions.
	0.	Availability is also defined as the percentage of uptime (that is, length of time that a system is online between failures) over a period of time (commonly 1 year).
	0.	A common shorthand when referring to availability is number of 9s. For example, five 9s means 99.999 percent availability.
High availability
	0.	A highly available system is one that can withstand some measure of degradation while still remaining available. In a highly available system, downtime is minimized as much as possible and minimal human intervention is required.
	0.	A highly available system can be viewed as a set of system-wide, shared resources that cooperate to guarantee essential services. High availability combines software with open-standard hardware to minimize downtime by quickly restoring essential services when a system, component, or application fails. Services are restored rapidly, often in less than 1 minute.
Availability tiers
	0.	Availability requirements vary. The length of disruption that is acceptable depends on the type of application. Here is a table of common application availability design goals and the maximum length of disruption that can occur within a year while still meeting the goal. The table contains examples of the types of applications that are common at each availability tier.
	0.	For accessibility: Availability tiers with max disruption per year and application categories. The tiers range from availabilities of 99 percent to 99.999 percent. End of accessibility description.
Factors that influence availability
	0.	Though events that might disrupt an application's availability cannot always be predicted, you can build availability into your architecture design. There are three factors that determine the overall availability of your application:
	•	  Fault tolerance refers to the built-in redundancy of an application's components and the ability of the application to remain operational even if some of its components fail. Fault tolerance relies on specialized hardware to detect failure in a system component such as a processor, memory board, power supply, 1/O subsystem, or storage subsystem) and instantaneously switch to a redundant hardware component. The fault-tolerant model does not address software failures, which are the most common reason for downtime.
	•	  Scalability is the ability of your application to accommodate increases in capacity needs, remain available, and perform within your required standards. It does not guarantee availability, but it contributes to your application's availability.
	•	  Recoverability is the ability to restore service quickly and without lost data if a disaster makes your components unavailable, or it destroys data.
	0.	Keep in mind that improving availability usually leads to increased cost. When you consider how to make your environment more available, it's important to balance the cost of the improvement with the benefit to your users.
	0.	Do you want to ensure that your application is always alive or reachable, or do you want to ensure that it is servicing requests within an acceptable level of performance?
Section 3: AWS trusted advisor

AWS trusted advisor
	0.	AWS Trusted Advisor is an online tool that provides real-time guidance to help you provision your resources following AWS best practices.
	0.	AWS Trusted Advisor looks at your entire AWS environment and gives you recommendations in five categories:
	•	  Cost Optimization - AWS Trusted Advisor looks at your resource use and makes recommendations to help you optimize cost by eliminating unused and idle resources, or by making commitments to reserved capacity.
	•	  Performance - Improve the performance of your service by checking your service limits, ensuring you take advantage of provisioned throughput, and monitoring for overutilized instances.
	•	  Security - Improve the security of your application by closing gaps, enabling various AWS security features, and examining your permissions.
	•	  Fault Tolerance - Increase the availability and redundancy of your AWS application by taking advantage of automatic scaling, health checks, Multi-AZ deployments, and backup capabilities.
	•	  Service Limits - AWS Trusted Advisor checks for service usage that is more than 80 percent of the service limit. Values are based on a snapshot, so your current usage might differ. Limit and usage data can take up to 24 hours to reflect any changes.
	0.	For a detailed description of the information that AWS Trusted Advisor provides, see AWS
	0.	Trusted Advisor Best Practice Checks at https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor-check-reference.html.
Activity: interpret AWS trusted advisor recommendations
	0.	You have a friend who used AWS Trusted Advisor for the first time. She is trying to interpret its recommendations to improve her cloud environment and needs your help. This is her dashboard. While everything looks OK in the cost optimization and service limit categories, you notice that there are a few recommendations that you should review to help her improve her security, performance, and fault tolerance.
	0.	Help your friend interpret the following recommendations.
Quiz
	0.	Traceability is not one of the four areas of the performance efficiency pillar of the AWS well-architechtered framework
	0.	Assume everything will fail
	0.	Operational excellence, security, and cost optimization are pillars of the AWS well-architechtered framework
	0.	“Use server less architectures” and “ democratize advanced technologies” are design principles that are recommended when considering performance efficiency.
	0.	Performance, cost optimization, security, fault tolerance, and service limits are the five categories
	0.	Key topics of the sustainability pillar include a shared responsibility model for sustainability, understanding impact and maximizing utilization to minimize required resources and reduce downstream impacts/
	0.	AWS trusted advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. 
	0.	Reliability is a measure of your system’s ability to provide functionality when desired by the user.
	0.	Fault tolerance is the ability for a system to remain operational even if some of the components of that system fail.
	0.	A highly available system is always available, without the need for human intervention.
Section 1: Elastic Load Balancing

Elastic load balancing
	0.	Modern high-traffic websites must serve hundreds of thousands-if not millions-of concurrent requests from users or clients, and then return the correct text, images, video, or application data in a fast and reliable manner. Additional servers are generally required to meet these high volumes.
	0.	Elastic Load Balancing is an AWS service that distributes incoming application or network traffic across multiple targets-such as Amazon Elastic Compute Cloud (Amazon EC2) instances, containers, internet protocol (IP) addresses, and Lambda functions- in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancing scales your load balancer as traffic to your application changes over time. It can automatically scale to most workloads.
Types of load balancers
	0.	Elastic Load Balancing is available in three types:
	•	  An Application Load Balancer operates at the application level (Open Systems Interconnection, or OSI, model layer 7). It routes traffic to targets- Amazon Elastic Compute Cloud (Amazon EC2) instances, containers, Internet Protocol (IP) addresses, and Lambda functions-based on the content of the request. It is ideal for advanced load balancing of Hypertext Transfer Protocol (HTTP) and Secure HTTP (HTTPS) traffic. An Application Load Balancer provides advanced request routing that is targeted at delivery of modern application architectures, including microservices and container-based applications. An Application Load Balancer simplifies and improves the security of your application by ensuring that the latest Secure Sockets Layer/Transport Layer Security (SSL/TLS) ciphers and protocols are used at all times.
	•	  A Network Load Balancer operates at the network transport level (OSI model layer 4), routing connections to targets-EC2 instances, microservices, and containers- based on IP protocol data. It works well for load balancing both Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) traffic. A Network Load Balancer is capable of handling millions of requests per second while maintaining ultra-low latencies. A Network Load Balancer is optimized to handle sudden and volatile network traffic patterns.
	•	  A Classic Load Balancer provides basic load balancing across multiple EC2 instances, and it operates at both the application level and network transport level. A Classic Load Balancer supports the load balancing of applications that use HTTP, HTTPS, TCP, and SSL. The Classic Load Balancer is an older implementation. When possible, AWS recommends that you use a dedicated Application Load Balancer or Network Load Balancer.
	0.	To learn more about the differences between the three types of load balancers, see Product comparisons on the Elastic Load Balancing Features Page https://aws.amazon.com/elasticloadbalancing/features/?nc=sn&loc=2.
How elastic load balancing works
	0.	A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2 instances) in one or more Availability Zones.
	0.	You configure your load balancer to accept incoming traffic by specifying one or more listeners. A listener is a process that checks for connection requests. It is configured with a protocol and port number for connections from clients to the load balancer. Similarly, it is configured with a protocol and port number for connections from the load balancer to the targets.
	0.	You can also configure your load balancer to perform health checks, which are used to monitor the health of the registered targets so that the load balancer only sends requests to the healthy instances. When the load balancer detects an unhealthy target, it stops routing traffic to that target. It then resumes routing traffic to that target when it detects that the target is healthy again.
	0.	There is a key difference in how the load balancer types are configured. With Application Load Balancers and Network Load Balancers, you register targets in target groups, and route traffic to the target groups. With Classic Load Balancers, you register instances with the load balancer.
Elastic Load balancing use cases
	0.	There are many reasons to use a load balancer:
	•	  Achieve high availability and better fault tolerance for your applications - Elastic LoadBalancing balances traffic across healthy targets in multiple Availability Zones. If one or more of your targets in a single Availability Zone are unhealthy, Elastic Load Balancing will route traffic to healthy targets in other Availability Zones. After the targets return to a healthy state, load balancing will automatically resume traffic to them.
	•	  Automatically load balance your containerized applications - With enhanced container support for Elastic Load Balancing, you can now load balance across multiple ports on the same EC2 instance. You can also take advantage of deep integration with Amazon Elastic Container Service (Amazon ECS), which provides a fully-managed container offering. You only need to register a service with a load balancer, and Amazon ECS transparently manages the registration and de-registration of Docker containers. The load balancer automatically detects the port and dynamically reconfigures itself.
	•	  Automatically scale your applications - Elastic Load Balancing works with Amazon CloudWatch and Amazon EC2 Auto Scaling to help you scale your applications to the demands of your customers. Amazon CloudWatch alarms can trigger auto scaling for your EC2 instance fleet when the latency of any one of your EC2 instances exceeds a preconfigured threshold. Amazon EC2 Auto Scaling then provisions new instances and your applications will be ready to serve the next customer request. The load balancer will register the EC2 instance and direct traffic to it as needed.   Use Elastic Load Balancing in your virtual private cloud (VPC) - You can use Elastic Load Balancing to create a public entry point into your VPC, or to route request traffic between tiers of your application within your VPC. You can assign security groups to your load balancer to control which ports are open to a list of allowed sources. Because Elastic Load Balancing works with your VPC, all your existing network access control lists network ACLs) and routing tables continue to provide additional network controls. When you create a load balancer in your VPC, you can specify whether the load balancer is public (default) or internal. If you select internal, you do not need to have an internet gateway to reach the load balancer, and the private IP addresses of the load balancer will be used in the load balancer's Domain Name System (DNS) record.
	•	  Enable hybrid load balancing - Elastic Load Balancing enables you to load balance across AWS and on-premises resources by using the same load balancer. For example, if you must distribute application traffic across both AWS and on-premises resources, you can register all the resources to the same target group and associate the target group with a load balancer. Alternatively, you can use DNS-based weighted load balancing across AWS and on-premises resources by using two load balancers, with one load balancer for AWS and other load balancer for on-premises resources. You can also use hybrid load balancing to benefit separate applications where one application is in a VPC and the other application is in an on-premises location. Put the VPC targets in one target group and the on-premises targets in another target group, and then use content-based routing to route traffic to each target group.
	•	Invoking Lambda functions over HTTP(S) - Elastic Load Balancing supports invoking Lambda functions to serve HTTP(S) requests. This enables users to access serverless applications from any HTTP client, including web browsers. You can register Lambda functions as targets and use the support for content-based routing rules in Application Load Balancers to route requests to different Lambda functions. You can use an Application Load Balancer as a common HTTP endpoint for applications that use servers and serverless computing. You can build an entire website by using Lambda functions, or combine EC2 instances, containers, on-premises servers, and Lambda functions to build applications.

Load balancer monitoring
	0.	You can use the following features to monitor your load balancers, analyze traffic patterns, and troubleshoot issues with your load balancers and targets:
	•	  Amazon CloudWatch metrics - Elastic Load Balancing publishes data points to Amazon CloudWatch for your load balancers and your targets. CloudWatch enables you to retrieve statistics about those data points as an ordered set of time series data, known as metrics. You can use metrics to verify that your system is performing as expected. For example, you can create a CloudWatch alarm to monitor a specified metric and initiate an action (such as sending a notification to an email address) if the metric goes outside what you consider an acceptable range.
	•	  Access logs - You can use access logs to capture detailed information about the requests that were made to your load balancer and store them as log files in Amazon Simple Storage Service (Amazon S3). You can use these access logs to analyze traffic patterns and to troubleshoot issues with your targets or backend applications.
	•	  AWS CloudTrail logs - You can use AWS CloudTrail to capture detailed information about the calls that were made to the Elastic Load Balancing application programming interface (API) and store them as log files in Amazon S3. You can use these CloudTrail logs to determine who made the call, what calls were made, when the call was made, the source IP address of where the call came from, and so on.
Section 2: Amazon CloudWatch

Monitoring AWS resources
	0.	To use AWS efficiently, you need insight into your AWS resources.
	0.	For example, you might want to know:
	•	  When you should launch more Amazon EC2 instances?
	•	  If your application's performance or availability is being affected by a lack of sufficient capacity?
	•	  How much of your infrastructure is actually being used?
	0.	How do you capture this information?
Amazon CloudWatch
	0.	You can capture this intormation with Amazon CloudWatch.
	0.	Amazon CloudWatch is a monitoring and observability service that is built for DevOps engineers, developers, site reliability engineers (SE), and IT managers. CloudWatch monitors your AWS resources (and the applications that you run on AWS) in real time. You can use CloudWatch to collect and track metrics, which are variables that you can measure for your resources and applications.
	0.	You can create an alarm to monitor any Amazon CloudWatch metric in your account and use the alarm to automatically send a notification to an Amazon Simple Notification Service (Amazon SNS) topic or perform an Amazon EC2 Auto Scaling or Amazon EC2 action. For example, you can create alarms on the CPU utilization of an EC2 instance, Elastic Load Balancing request latency, Amazon DynamoDB table throughput, Amazon Simple Queue Service (Amazon SQ) queue length, or even the charges on your AWS bill. You can also create an alarm on custom metrics that are specific to your custom applications or infrastructure.
	0.	You can also use Amazon CloudWatch Events to define rules that match incoming events (or changes in your AWS environment) and route them to targets for processing. Targets can include Amazon EC2 instances, AWS Lambda functions, Kinesis streams, Amazon ECS tasks, Step Functions state machines, Amazon SNS topics, Amazon SQS queues, and built-in targets. Cloud Watch Events becomes aware of operational changes as they occur. CloudWatch Events responds to these operational changes and takes corrective action as necessary, by sending messages to respond to the environment, activating functions, making changes, and capturing state information. With CloudWatch, you gain system-wide visibility into resource utilization, application performance, and operational health. There is no upfront commitment or minimum fee; you simply pay for what you use. You are charged at the end of the month for what you use.
CloudWatch alarms
	0.	You can create a CloudWatch alarm that watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. You can create a CloudWatch alarm based on a static threshold, anomaly detection, or a metric math expression.
	0.	When you create an alarm based on a static threshold, you choose a CloudWatch metric for the alarm to watch and the threshold for that metric. The alarm goes to ALARM state when the metric breaches the threshold for a specified number of evaluation periods.
	0.	For an alarm based on a static threshold, you must specify the:
	•	  Namespace - A namespace contains the CloudWatch metric that you want, for example, AWS/EC2.
	•	  Metric - A metric is the variable you want to measure, for example, CPU Utilization.
	•	  Statistic - A statistic can be an average, sum, minimum, maximum, sample count, a predefined percentile, or a custom percentile.
	•	  Period - A period is the evaluation period for the alarm. When the alarm is evaluated, each period is aggregated into one data point.
	•	  Conditions - When you specify the conditions for a static threshold, you specify whenever the metric is Greater, Greater or Equal, Lower or Equal, or Lower than the threshold value, and you also specify the threshold value.
	•	  Additional configuration information - This includes the number of data points within the evaluation period that must be breached to trigger the alarm, and how CloudWatch should treat missing data when it evaluates the alarm.
	•	Actions - You can choose to send a notification to an Amazon SNS topic, or to perform an Amazon EC2 Auto Scaling action or Amazon EC2 action.
	0.	For more information on creating CloudWatch alarms, see the topics under Using Alarms in the AWS Documentation at https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html.

Section 3: Amazon EC2 Auto scaling

Why is scaling important 
	0.	Scaling is the ability to increase or decrease the compute capacity of your application. To understand why scaling is important, consider this example of a workload that has varying resource requirements. In this example, the most resource capacity is required on Wednesday, and the least resource capacity is required on Sunday.
	0.	One option is to allocate more than enough capacity so you can always meet your highest demand -in this case, Wednesday. However, this situation means that you are running resources that will be underutilized most days of the week. With this option, your costs are not optimized.
	0.	Another option is to allocate less capacity to reduce costs. This situation means that you are under capacity on certain days. If you don't solve your capacity problem, your application could underperform or potentially even become unavailable for users.
Amazon EC2 auto scaling
	0.	In the cloud, because computing power is a programmatic resource, you can take a flexible approach to scaling. Amazon EC2 Auto Scaling is an AWS service that helps you maintain application availability and enables you to automatically add or remove EC2 instances according to conditions you define. You can use the fleet management features of EC2 Auto Scaling to maintain the health and availability of your fleet.
	0.	Amazon EC2 Auto Scaling provides several ways to adjust scaling to best meet the needs of your applications. You can add or remove EC2 instances manually, on a schedule, in response to changing demand, or in combination with AWS Auto Scaling for predictive scaling. Dynamic scaling and predictive scaling can be used together to scale faster.
	0.	To learn more about Amazon EC2 Auto Scaling, see the Amazon EC2 Auto Scaling product page at https://aws.amazon.com/ec2/autoscaling.
Typical weekly traffic at Amazon.com
	0.	Automatic scaling is useful for predictable workloads-for example, the weekly traffic at the retail company Amazon.com.
November traffic to Amazon.com
	0.	Automatic scaling is also useful for dynamic on-demand scaling. Amazon.com experiences a seasonal peak in traffic in November on Black Friday and Cyber Monday, which are days at the end of November when US retailers hold major sales). If Amazon provisions a fixed capacity to accommodate the highest use, 76 percent of the resources are idle for most of the year. Capacity scaling is necessary to support the fluctuating demands for service. Without scaling, the servers could crash due to saturation, and the business would lose customer confidence.
Auto Scaling groups
	0.	An Auto Scaling group is a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. The size of an Auto Scaling group depends on the number of instances you set as the desired capacity. You can adjust its size to meet demand, either manually or by using automatic scaling. See more about Auto Scaling Groups at https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html.
	0.	You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2
	0.	Auto Scaling is designed to prevent your group from going below this size. You can specify the maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling is designed to prevent your group from going above this size. If you specify the desired capacity, either when you create the group or at any time afterwards, Amazon EC2 Auto Scaling is designed to adjust the size of your group so it has the specified number of instances. If you specify scaling policies, then Amazon EC2 Auto Scaling can launch or terminate instances as demand on your application increases or decreases.
	0.	For example, this Auto Scaling group has a minimum size of one instance, a desired capacity of two instances, and a maximum size of four instances. The scaling policies that you define adjust the number of instances within your minimum and maximum number of instances, based on the criteria that you specify.
Scaling out versus scaling in
	0.	With Amazon EC2 Auto Scaling, launching instances is referred to as scaling out, and terminating instances is referred to as scaling in.
How amazon EC2 auto scaling works
	0.	To launch EC2 instances, an Auto Scaling group uses a launch configuration, which is an instance configuration template. You can think of a launch configuration as what you are scaling. When you create a launch configuration, you specify information for the instances. The information you specify includes the ID of the Amazon Machine Image (AMI), the instance type, AWS Identity and Access Management (IAM) role, additional storage, one or more security groups, and any Amazon Elastic Block Store (Amazon EBS) volumes. See more information on the launch configuration at https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-configurations.html.
	0.	You define the minimum and maximum number of instances and desired capacity of your Auto Scaling group. Then, you launch it into a subnet within a VPC (you can think of this as where you are scaling). Amazon EC2 Auto Scaling integrates with Elastic Load Balancing to enable you to attach one or more load balancers to an existing Auto Scaling group. After you attach the load balancer, it automatically registers the instances in the group and distributes incoming traffic across the instances.
	0.	Finally, you specify when you want the scaling event to occur. You have many scaling options:
	•	  Maintain current instance levels at all times - You can configure your Auto Scaling group to maintain a specified number of running instances at all times. To maintain the current instance levels, Amazon EC2 Auto Scaling performs a periodic health check on running instances in an Auto Scaling group. When Amazon EC2 Auto Scaling finds an unhealthy instance, it terminates that instance and launches a new one.
	•	  Manual scaling - With manual scaling, you specify only the change in the maximum, minimum, or desired capacity of your Auto Scaling group. See more on manual scaling at https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-manual-scaling.html.
	•	  Scheduled scaling- With scheduled scaling, scaling actions are performed automatically as a function of date and time. This is useful for predictable workloads when you know exactly when to increase or decrease the number of instances in your group. For example, say that every week, the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. See more on scheduled scaling at https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html. You can plan your scaling actions based on the predictable traffic patterns of your web application. To implement scheduled scaling, you create a scheduled action.
	•	  - A more advanced way to scale your resources enables you to define parameters that control the scaling process. For example, you have a web application that currently runs on two instances and you want the CPU utilization of the Auto Scaling group to stay close to 50 percent when the load on the application changes. This option is useful for scaling in response to changing conditions, when you don't know when those conditions will change. Dynamic scaling gives you extra capacity to handle traffic spikes without maintaining an excessive amount of idle resources. You can configure your Auto Scaling group to scale automatically to meet this need. More on dynamic scaling at https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html. The scaling policy type determines how the scaling action is performed. You can use Amazon EC2 Auto Scaling with Amazon CloudWatch to trigger the scaling policy in response to an alarm. See policy types at https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html#as-scaling-types.
	•	  Predictive scaling - You can use Amazon EC2 Auto Scaling with AWS Auto Scaling to implement predictive scaling, where your capacity scales based on predicted demand. Predictive scaling uses data that is collected from your actual EC2 usage, and the data is further informed by billions of data points that are drawn from our own observations. AWS then uses well-trained machine learning models to predict your expected traffic (and EC2 usage), including daily and weekly patterns. The model needs at least 1 day of historical data to start making predictions. It is reevaluated every 24 hours to create a forecast for the next 48 hours. The prediction process produces a scaling plan that can drive one or more groups of automatically scaled EC2 instances.See more on predictive scaling at https://aws.amazon.com/blogs/aws/new-predictive-scaling-for-ec2-powered-by-machine-learning/.
	0.	To learn more about these options, see Scaling the Size of Your Auto Scaling Group in the AWS Documentation at https://docs.aws.amazon.com/autoscaling/ec2/userguide/scale-your-group.html.
Implementing dynamic scaling
	0.	One common configuration for implementing dynamic scaling is to create a CloudWatch alarm that is based on performance information from your EC2 instances or load balancer. When a performance threshold is breached, a CloudWatch alarm triggers an automatic scaling event that either scales out or scales in EC2 instances in the Auto Scaling group.
	0.	To understand how it works, consider this example:
	•	  You create an Amazon CloudWatch alarm to monitor CPU utilization across your fleet of EC2 instances and run automatic scaling policies if the average CPU utilization across the fleet goes above 60 percent for 5 minutes.
	•	  Amazon EC2 Auto Scaling instantiates a new EC2 instance into your Auto Scaling group based on the launch configuration that you create.
	•	  After the new instance is added, Amazon EC2 Auto Scaling makes a call to Elastic Load Balancing to register the new EC2 instance in that Auto Scaling group.
	•	  Elastic Load Balancing then performs the required health checks and starts distributing traffic to that instance. Elastic Load Balancing routes traffic between EC2 instances and feeds metrics to Amazon CloudWatch.
	0.	Amazon CloudWatch, Amazon EC2 Auto Scaling, and Elastic Load Balancing work well individually. Together, however, they become more powerful and increase the control and flexibility over how your application handles customer demand.
AWS auto scaling
	0.	So far, you learned about scaling EC2 instances with Amazon EC2 Auto Scaling. You also learned that you can use Amazon EC2 Auto Scaling with AWS Auto Scaling to perform predictive scaling.
	0.	AWS Auto Scaling is a separate service that monitors our applications. It automatically adiusts capacity to maintain steady, predictable performance at the lowest possible cost. The service provides a simple, powerful user interface that enables you to build scaling plans for resources, including:
	•	  Amazon EC2 instances and Spot Fleets
	•	  Amazon Elastic Container Service Amazon ECS) tasks
	•	  Amazon DynamoDB tables and indexes
	•	  Amazon Aurora Replicas
	0.	If you are already using Amazon EC2 Auto Scaling to dynamically scale your EC2 instances, you can now use it with AWS Auto Scaling to scale additional resources for other AWS services.
	0.	To learn more information about AWS Auto Scaling, see AWS Auto Scaling at https://aws.amazon.com/autoscaling/.
Quiz
	1.	Amazon EC2 Auto scaling and elastic load balancing help your application scale up or down based on demand.
	2.	Amazon SNS is the service you’d use to send alerts based on Amazon CloudWatch alarms.
	3.	Amazon EC2 auto scaling responds to changing conditions by adding or terminating instances, launches instances from an AMI, and enforces a minimum number of running amazon EC2 instances.
	4.	You configure the load balancer to accept incoming traffic by specifying one or more listeners
	5.	You specify the AMI, instance type, and EBS volumes when you create an auto scaling launch configuration
	6.	Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS
	7.	You can specify the minimum and maximum number of instances in each auto scaling group as well as desired capacity
	8.	AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account
	9.	When the load balancer detects an unhealthy target, it stops routing traffic to the target and sends it to a healthy target. It then resumes routing traffic to that target when it detects that the target is healthy again.
	10.	ELB offers three types of load balancers: Application Load Balancer, Network Load Balancer, and Classic Load Balancer.
